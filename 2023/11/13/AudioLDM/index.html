<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0"><title>AudioLDM | 脚踏车的日志站</title><meta name="author" content="脚踏车没有脚"><meta name="copyright" content="脚踏车没有脚"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="AudioLDM作者：Haohe Liu, Zehua Chen, Yi Yuan, Xinhao Mei, Xubo Liu, Danilo Mandic, Wenwu Wang, Mark D. Plumley机构：CVSSP, University of Surrey, Guildford, UK；Department of EEE, Imperial College London, Lon">
<meta property="og:type" content="article">
<meta property="og:title" content="AudioLDM">
<meta property="og:url" content="http://yypyyds.github.io/2023/11/13/AudioLDM/index.html">
<meta property="og:site_name" content="脚踏车的日志站">
<meta property="og:description" content="AudioLDM作者：Haohe Liu, Zehua Chen, Yi Yuan, Xinhao Mei, Xubo Liu, Danilo Mandic, Wenwu Wang, Mark D. Plumley机构：CVSSP, University of Surrey, Guildford, UK；Department of EEE, Imperial College London, Lon">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://yypyyds.github.io/img/touxiang.jpg">
<meta property="article:published_time" content="2023-11-13T02:45:19.000Z">
<meta property="article:modified_time" content="2024-02-21T02:51:13.065Z">
<meta property="article:author" content="脚踏车没有脚">
<meta property="article:tag" content="论文">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://yypyyds.github.io/img/touxiang.jpg"><link rel="shortcut icon" href="/img/favicon.ico"><link rel="canonical" href="http://yypyyds.github.io/2023/11/13/AudioLDM/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":true,"languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  }
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'AudioLDM',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-02-21 10:51:13'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
    win.getCSS = url => new Promise((resolve, reject) => {
      const link = document.createElement('link')
      link.rel = 'stylesheet'
      link.href = url
      link.onload = () => resolve()
      link.onerror = () => reject()
      document.head.appendChild(link)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 6.3.0">
<style>.github-emoji { position: relative; display: inline-block; width: 1.2em; min-height: 1.2em; overflow: hidden; vertical-align: top; color: transparent; }  .github-emoji > span { position: relative; z-index: 10; }  .github-emoji img, .github-emoji .fancybox { margin: 0 !important; padding: 0 !important; border: none !important; outline: none !important; text-decoration: none !important; user-select: none !important; cursor: auto !important; }  .github-emoji img { height: 1.2em !important; width: 1.2em !important; position: absolute !important; left: 50% !important; top: 50% !important; transform: translate(-50%, -50%) !important; user-select: none !important; cursor: auto !important; } .github-emoji-fallback { color: inherit; } .github-emoji-fallback img { opacity: 0 !important; }</style>
</head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/touxiang.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">13</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">13</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">5</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header"><nav id="nav"><span id="blog-info"><a href="/" title="脚踏车的日志站"><span class="site-name">脚踏车的日志站</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">AudioLDM</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2023-11-13T02:45:19.000Z" title="发表于 2023-11-13 10:45:19">2023-11-13</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-02-21T02:51:13.065Z" title="更新于 2024-02-21 10:51:13">2024-02-21</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/">论文笔记</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="AudioLDM"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="AudioLDM"><a href="#AudioLDM" class="headerlink" title="AudioLDM"></a>AudioLDM</h1><p>作者：Haohe Liu, Zehua Chen, Yi Yuan, Xinhao Mei, Xubo Liu, Danilo Mandic, Wenwu Wang, Mark D. Plumley<br>机构：CVSSP, University of Surrey, Guildford, UK；Department of EEE, Imperial College London, London, UK<br>发表情况： ICML 2023</p>
<h2 id="学习过程参考的相关文章"><a href="#学习过程参考的相关文章" class="headerlink" title="学习过程参考的相关文章"></a>学习过程参考的相关文章</h2><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/638442430">Diffusion Model</a></p>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>贡献：</p>
<ul>
<li>第一次将连续的隐扩散模型（LDM）应用于TTA生成，且取得了SOTA效果</li>
<li>使用了CLAP嵌入使TTA生成的训练不用依赖音频文本对</li>
<li>实验证明了在LDM训练中只使用音频数据可以得到高质量和高计算效率的TTA系统</li>
<li>展示了提出的TTA系统能够在未经过微调的情况下进行文本引导的音频风格操作，比如音频风格迁移，高分辨率生成，音频修复。</li>
</ul>
<p><img src="/2023/11/13/AudioLDM/image-20231211115457402.png" alt=""></p>
<h2 id="Text-Conditional-Audio-Generation"><a href="#Text-Conditional-Audio-Generation" class="headerlink" title="Text-Conditional Audio Generation"></a>Text-Conditional Audio Generation</h2><h3 id="Contrastive-Language-Audio-Pretraining-CLAP"><a href="#Contrastive-Language-Audio-Pretraining-CLAP" class="headerlink" title="Contrastive Language-Audio Pretraining (CLAP)"></a>Contrastive Language-Audio Pretraining (CLAP)</h3><p>参考图像文本预训练（CLIP），提出了CLAP<br>分别由一个text encoder和一个audio encoder，分别提取文本和音频的嵌入特征，两个encoder的架构分别为</p>
<ul>
<li>text: RoBERTa</li>
<li>audio: HTSAT<br>以对称交叉熵损失作为训练目标，将音频和文本特征对齐到同一个嵌入空间。</li>
</ul>
<h3 id="Conditional-Latent-Diffusion-Models"><a href="#Conditional-Latent-Diffusion-Models" class="headerlink" title="Conditional Latent Diffusion Models"></a>Conditional Latent Diffusion Models</h3><p>条件隐扩散模型<br>这部分有点绕，先翻译一下<br>TTA系统可以给定一个文本描述 $y$ 生成一个音频样本 $\hat x$ 。在有概率生成模型LDMs的情况下，我们使用模型分布 $p_{\theta}(z_0|E^y)$ 评估了真实的条件数据分布 $q(z_o|E^y)$ ，其中 $z_0 \in \mathbb{R}^{C\times \frac{T}{r} \times \frac{F}{r}}$ 是音频样本 $x$ 在由 $X \in \mathbb{R}^{T\times F}$ 梅尔谱压缩表示组成向量空间的前置，$E^y$ 是由CLAP预训练的文本encoder得到的文本嵌入。$r$ 表示压缩等级，C表示压缩表示的通道数，T 和 F 表示梅尔谱图 X 的时域频域维数。通过预训练的CLAP联合嵌入音频和文本信息，音频嵌入 $E^x$ 和文本嵌入 $E^y$ 共享一个联合的跨模态空间。这允许我们使用 $E^x$ 训练LDMs，同时将 $E^y$ 用于TTA生成。</p>
<p>这部分的重点在这两个概率分布模型  $p_{\theta}(z_0|E^y)$ 和 $q(z_o|E^y)$ ，其中 $q(z_o|E^y)$ 是真实分布，也就是我们拟合的目标，这是一个条件概率模型，条件是 $E^y$ ，也就是文本描述的嵌入，随机变量是 $z_0$ ，原文描述为</p>
<blockquote>
<p>$z_0 \in \mathbb{R}^{C\times \frac{T}{r} \times \frac{F}{r}}$ is the prior of an audio sample $x$ in the space formed from the compressed representation of the mel-spectrogram  $X \in \mathbb{R}^{T\times F}$ </p>
</blockquote>
<p>关键是 <code>the prior of an audio sample</code> 就把它当作一个用于重建音频的隐变量就好了，至于是怎么还原的之后应该会提到。<br>所以这个要拟合的分布，就是给定文本描述的情况下，符合文本描述的音频的分布</p>
<p>然后讲本文用到的扩散模型，基本和参考文章中的扩散模型一样，首先分为前向过程和反向过程，前向过程一层一层加噪声，反向过程一层一层去噪声。<br><strong>forward process</strong><br>预定义一个噪声尺度 $0 &lt; \beta_1 &lt; \cdots &lt; \beta_n &lt; \cdots &lt; \beta_N &lt;1$ ，共做 $N$ 次加噪过程，最终会将数据分布转换为一个标准高斯分布</p>
<script type="math/tex; mode=display">
\begin{aligned}
q(z_n|z_{n-1}) &= \mathcal{N}(z_n;\sqrt{1-\beta_n} \; z_{n-1},\beta_n \boldsymbol I)
\qquad &(1)
\newline
q(z_n|z_0) &= \mathcal{N}(z_n;\sqrt{\bar \alpha_n}\;z_0,(1-\bar \alpha_n)\boldsymbol \epsilon)\qquad &(2) 
\end{aligned}</script><p>这里的 $\boldsymbol \epsilon \sim \mathcal N (0,I)$ 表示输入噪声，$\alpha_n = 1-\beta_n$ 是构造参数的技巧，让式子更简洁，$\bar \alpha_n := \prod ^n_{s=1}\alpha_s$ 表示每一步的噪声水平。最后一步 $N$ ，$z_N \sim \mathcal N(0,I)$ 是一个标准正态高斯分布</p>
<p>对于模型优化，使用调整过权重的噪声评估训练目标（在参考文章中有相关证明）</p>
<script type="math/tex; mode=display">
L_n(\theta)=\mathbb E_{z_0,\epsilon,n}\Vert\epsilon-\epsilon_\theta(z_n,n,E^x)\Vert^2_2 \qquad (3)</script><p>其中，$E^x$ 是由audio encoder生成的 音频波形 $x$ 的音频嵌入。<br>在反向过程中，从高斯分布 $p(z_N)\sim \mathcal N(0,I)$ 和文本嵌入 $E^y$ 开始，以 $E^y$ 为条件的去噪过程通过以下过程逐渐产生音频先验（audio prior） $z_0$</p>
<script type="math/tex; mode=display">
\begin{aligned}
p_{\theta}(z_{0:N}|E^y) &= p(z_N)\prod^N_{t=n}p_\theta(z_{n-1}|z_n,E^y) \qquad &(4)
\newline
p_\theta(z_{n-1}|z_n,E^y) &= \mathcal N (z_{n-1};\mu_\theta(z_n,n,E^y),\sigma^2_nI)\qquad &(5)
\end{aligned}</script><p>公式（4）是对整个过程的一个描述，从高斯分布的采样 $p(z_N)$ 开始，连乘 N 个去噪的概率分布得到生成的结果，其中，每一步的去噪过程，可以定义为给定阶段 $n$ 的带噪隐变量 $z_n$ 和文本嵌入 $E^y$ ，加噪声前一阶段 $z_{n-1}$ 的概率分布。</p>
<p>反向过程在参考文章中给了很清晰的解释</p>
<blockquote>
<p>在正向过程中，我们人为设置了 $T$ 步加噪声过程。而在反向过程中，我们希望能够倒过来取消每一步加噪声操作，让一幅纯噪声图像变回数据集里的图像。这样，利用这个去噪声过程，我们就可以把任意一个从标准正态分布里采样出来的噪声图像变成一幅和训练数据长得差不多的图像，从而起到图像生成的目的。<br>现在问题来了：去噪声操作的数学形式是怎么样的？怎么让神经网络来学习它呢？数学原理表明，当 $\beta_t$ 足够小时，每一步加噪声的逆操作也满足正态分布。</p>
</blockquote>
<p>重点：<strong>直接计算所有数据的加噪声逆操作的分布是不太现实的</strong>，但是，如果给定了某个训练集输入 $x_0$，多了一个限定条件后，该分布是可以用贝叶斯公式计算的</p>
<script type="math/tex; mode=display">
\begin{aligned}
q(x_{t-1}|x_t,x_0) &= \frac{q(x_{t-1},x_t,x_0)}{q(x_t,x_0)} \newline
&=\frac{q(x_t,x_{t-1},x_0)}{q(x_0)q(x_t|x_0)} \newline
&=\frac{q(x_t|x_{t-1},x_0)q(x_{t-1},x_0)}{q(x_0)q(x_t|x_0)} \newline
&=\frac{q(x_t|x_{t-1},x_0)q(x_{t-1}|x_0)q(x_0)}{q(x_0)q(x_t|x_0)}
\newline
&=q(x_t|x_{t-1},x_0)\frac{q(x_{t-1}|x_0)}{q(x_t|x_0)}
\end{aligned}</script><p>等式右边的所有东西都是正向过程中已知的，代入可以计算得到等式左边的均值和方差，具体的推导过程见参考文章</p>
<p>回到论文，得到去噪的过程的均值和方差如下</p>
<script type="math/tex; mode=display">
\begin{aligned}
\mu_\theta(z_n,n,E^y)&=\frac{1}{\sqrt{\alpha_n}}(z_n-\frac{\beta_n}{\sqrt{1-\bar \alpha_n}}\epsilon_\theta(z_n,n,E^y)) \qquad&(6) \newline
\sigma^2_n &= \frac{1-\bar\alpha_{n-1}}{1-\bar\alpha_n}\beta_n \qquad&(7)
\end{aligned}</script><p>本文的 $\epsilon_\theta$ 和 $DDPM$ 的 $\epsilon$ 好像不是一个东西，DDPM的是从标准高斯中采样的，而这里文章中说这是预测的生成噪声，应该是原始的diffusion没有text prompt这个限制，是随机生成图片，而这里是有一个文本嵌入 $E^y$ 在限制</p>
<p>在训练阶段，学习给定音频样本 $x$ 的跨模态表示 $E^x$ 的audio prior $z_0$ 的生成。然后在TTA生成，提供文本嵌入 $E^y$ 来预测噪声 $\epsilon_\theta$<br>建立在CLAP嵌入的基础上，LDM可以不需要文本监督也能理解TTA生成</p>
<h3 id="Conditioning-Augmentation"><a href="#Conditioning-Augmentation" class="headerlink" title="Conditioning Augmentation"></a>Conditioning Augmentation</h3><p>条件作用增强<br>在图像生成中，可以捕捉到图像间微小的差别，其中一个原因是大量的language-image样本对，但是音频没有这么多样本。本文的方法在训练LDMs的时候不需要文本样本，所以数据增强只需要音频，通过以下方式对音频样本 $x_1,x_2$ 进行混频增强</p>
<script type="math/tex; mode=display">
x_{1,2}=\lambda x_1 + (1-\lambda)x_2 \qquad(8)</script><p>其中 $\lambda$ 是从beta分布 $\mathcal B(5,5)$ 中采样的0到1之间的数</p>
<h3 id="Classifier-free-Guidance"><a href="#Classifier-free-Guidance" class="headerlink" title="Classifier-free Guidance"></a>Classifier-free Guidance</h3><p>用于指导扩散模型的生成。<br>在训练过程中，使用固定的概率随机丢弃条件 $E^x$ 来同时训练条件LDMs $\epsilon_\theta(z_n,n,E^x)$ 和非条件LDMs $\epsilon_\theta(z_n,n)$ 。生成的时候，使用文本嵌入 $E^y$ 作为条件，并使用改进的噪声估计 $\hat \epsilon_\theta(z_n,n,E^y)$ 进行采样：</p>
<script type="math/tex; mode=display">
\hat \epsilon_\theta(z_n,n,E^y) = w\epsilon_\theta(z_n,n) + (1-w)\epsilon_\theta(z_n,n,E^y) \qquad (9)</script><p>其中 $w$ 决定指导尺度<br>看起来就是俩系数，分别决定条件LDMs和非条件LDMs在预测噪声中占的比重，非条件LDMs就是原始的Diffusion的那个正态采样噪声</p>
<h3 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h3><p>主要有两部分，VAE用作隐变量到梅尔频谱图的转换，HiFi-GAN用作梅尔频谱图到音频样本的重建<br>训练过程，使用VAE将梅尔频谱图 $X \in \mathbb R^{T\times F}$ 压缩进一个小的隐空间 $z\in \mathbb R^{C\times \frac R r \times \frac F r}$ ，其中 $r$ 是压缩等级。VAE的encoder和decoder采用堆叠的卷积块。<br>训练损失包括三个部分：重建损失，对抗损失，高斯约束损失（后面补充）<br>另外，和LDMs的条件增强类似，这里也使用了公式（9）的数据扩充方式</p>
<h2 id="Text-Guided-Audio-Manipulation"><a href="#Text-Guided-Audio-Manipulation" class="headerlink" title="Text-Guided Audio Manipulation"></a>Text-Guided Audio Manipulation</h2><p><strong>Style Transfer</strong><br>什么玩意儿<br>给定一个原音频样本 $x^{src}$ 我们可以使用公式（2）计算它的预定义时间步长 $n_0&lt;N$ 的噪声潜在表示 $z_{n_0}$<br>使用 $z_{n_0}$ 作为预训练AudioLDM模型逆过程的开始点，我们通过一个 <code>shallow reverse process</code> $p_\theta (z_{0:n_0}|E^y)$ 来通过文本输入 $y$ 操纵音频 $x^{src}$ </p>
<script type="math/tex; mode=display">
p_\theta (z_{0:n_0}|E^y) = p(z_{n_0})\prod^{n_0}_{n=1}p_\theta (z_{n-1}|z_n,E^y) \qquad \qquad(10)</script><p>其中 $n_0$ 控制操作结果，如果我们定义一个 $n_0\approx N$ ，源音频提供的信息将不会被保留，操作将类似于TTA生成。<br>这是个什么东西呢<br>观察公式（10）和（4），他们的差别在于隐变量 $z$ 的下标，完整的LDMs过程，是如公式（4）所描述的，<code>源音频-&gt;高斯噪声-&gt;由文本条件引导的重建音频</code>。而公式（10）的过程是不完全的LDMs过程，<code>源音频-&gt;叠加了部分噪声的隐空间表示-&gt;由文本条件引导的重建音频</code>。这种处理方式会根据 $n_0$ 的选择保留源音频包含的大部分信息，只让文本描述起到修改的作用。</p>
<p><strong>Inpainting and Super-Resolution</strong><br>chatgpt的解释：<br>Inpainting：是一种用于修复或恢复信号中缺失或损坏部分的技术。在音频生成领域，这可能涉及到填补音频信号中的缺失段落，使其在听觉上更加连贯和完整<br>Super-Resolution：是一种用于增强图像或信号分辨率的技术，通常通过提高像素级别的细节来实现。在音频领域，这可以被理解为通过算法或模型增加音频信号的采样率或提高其频率分辨率，以获得更高质量的音频。这有助于使音频更清晰，更富有细节，提高听觉上的感知质量。</p>
<p>我们通过将观察到的部分的潜在表示 $z^{ob}$ 合并到生成的潜在表示 $z$ 中来探索这些任务<br>具体来说，在反向过程中，从 $p(z_N) \sim \mathcal N(0,I)$ 开始，在由公式（5）的每次推断之后，用下面的公式修改生成的 $z_{n-1}$ </p>
<script type="math/tex; mode=display">
z_{n-1}' = (1-m) \odot z_{n-1} + m \odot z^{ob}_{n-1}\qquad \qquad (11)</script><p>  其中 $z’$ 是修改过的隐空间表示，$m\in \mathbb R^{\frac T r \times \frac F r}$ 表示一个隐空间观测mask（是什么东西），$z^{ob}_{n-1}$ 是对 $z^{ob}$ 进行前向过程加噪声得到的<br>  这个 $m$ 就是把梅尔谱图中的观测部分置为1，未观测部分置为0，然后上面的公式中，$(1-m)\odot z_{n-1}$ 就是保留反向过程中未观测到的部分，而 $z^{ob}_{n-1}$ 是未经过 $E^y$ 指导的，由原始音频正向生成的隐变量，直观来看就是在生成过程中，每去一次噪声，就把观察到的部分的预测值换为原始观测值的加噪部分，只让模型去生成未被观测到的部分。</p>
<h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p><strong>Training dataset</strong><br>AudioSet, AudioCaps, Freesound, BBC Sound Effect library<br><strong>Evaluation dataset</strong><br>AC, AS把标签串起来</p>
<p><strong>评估方法</strong><br>客观方法：</p>
<ul>
<li>frechet distance（FD）：与图像生成中的frechet起始距离类似，音频中的FD表示生成的样本与目标样本之间的相似度</li>
<li>Inception score（IS）：用于评估采样质量和多样性的指标</li>
<li>KL散度：KL在成对样本水平上测量，并作为最终结果平均。</li>
</ul>
<p>主观方法：<br>叫了6个<code>audio professionsals</code>评估两个指标</p>
<ul>
<li>overall quality(OVL)，总体质量</li>
<li>relevance to the input text(REL)，和输入文本的相关性</li>
</ul>
<p><strong>模型</strong><br>对比模型：DiffSound，AudioGen<br>训练了两个AudioLDM</p>
<ul>
<li>AudioLDM-S 181M参数</li>
<li>AudioLDM-L 739M参数</li>
</ul>
<h3 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h3><p><img src="/2023/11/13/AudioLDM/image-20231117151704510.png" alt=""></p>
<p>虽然RoBERTa和CLAP具有相同的文本编码器结构，但CLAP的优势在于它将音频-文本关系学习与生成模型训练解耦。这种分离是直观的，因为CLAP已经通过对齐音频和文本的嵌入空间来建模音频和文本之间的关系。<br>用BERT既需要学习音频文本之间的关系，又需要学习生成相关的知识，而CLAP不需要，因为它已经对其了音频文本向量空间</p>
<p>在音乐上的合成质量评估：<br><img src="/2023/11/13/AudioLDM/image-20231117155505639.png" alt=""></p>
<p><strong>Conditioning Information</strong><br>在训练LDM的时候没有用到文本特征，一个很自然的问题，如果用了文本特征效果会不会更好？然后发现不会<br><img src="/2023/11/13/AudioLDM/image-20231117155906174.png" alt=""><br>作者认为导致这个结果的主要原因是文本嵌入对生成目标的描述没有音频嵌入这么好，由于音频的复杂性和模糊性，文本描述很难做到准确和全面。下面这个图是采样质量和训练步数的函数，可以看到在整个训练过程中，音频嵌入的训练效果明显好于文本嵌入;较大的模型收敛速度较慢，但最终性能较好。<br><img src="/2023/11/13/AudioLDM/image-20231117160627718.png" alt=""></p>
<p>问题，我把前面的忘了，这个文本嵌入和音频嵌入是放在哪个地方的来着</p>
<p>压缩率：r<br><img src="/2023/11/13/AudioLDM/image-20231117161337173.png" alt=""><br>r = 1，2的时候，单张RTX3090跑不动，实验默认用r=4</p>
<p><strong>Text-Guided Audio Manipulation</strong><br>对super-resolution任务，使用了两个模型 AudioUNet 和 NVSR 作为baseline，使用log-spectral distance (LSD）作为评价指标<br>对于Inpainting 任务，使用FAD作为指标，并作为这一任务的新baseline<br><img src="/2023/11/13/AudioLDM/image-20231117163359764.png" alt=""><br>实验结果中，AudioLDM的效果比AudioUNet好，但是不如NVSR。<br>作者认为这是因为AudioLDM的训练数据里包含很多噪声，这可能会导致在超分辨率过程的输出中出现白噪声或其他非语音声音事件，从而潜在地降低性能。<br>最后给出展望，我们的贡献给了一个新的方式，以zero-shot的方式，使用TTA系统完成文本引导的音频处理任务</p>
<h3 id="消融实验"><a href="#消融实验" class="headerlink" title="消融实验"></a>消融实验</h3><p>对三个部分做了消融实验<br>简化UNet的注意力机制，系统性能显著下降，说明复杂的注意机制是首选的<br>在音频分类中广泛使用的平衡采样机制，在TTA中没有表现出改善。（我看着挺明显的？）<br>条件增强（Conditional augmentation）在主观评价中有提升，但是在客观评价中没有。作者猜测可能是因为条件增强生成的训练数据在AC中没有代表性，导致模型的输出和测试集没有很好的对齐，导致度量分数更低。<br><img src="/2023/11/13/AudioLDM/image-20231117165738362.png" alt=""></p>
<p><strong>DDIM Sampling Step</strong><br>在DDPM中，反向过程的推断步数会直接影响生成质量，通常，增加采样步数和计算量，采样质量可以得到改善。<br><img src="/2023/11/13/AudioLDM/image-20231117170501557.png" alt=""><br>100步之后提升很小。</p>
<p><strong>Guidance Scale</strong><br>是生成多样性和条件生成质量之间的权衡，就是前文的参数 $w$ ，但是这里的取值是1234，按照前文的公式应该是取0到1才对啊<br>在看代码的过程中找到了答案，代码里用的 $w=2.5$，该参数最后作用于<code>ddim.py</code>中<br></p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">e_t = e_t_uncond + unconditional_guidance_scale * (e_t - e_t_uncond)</span><br></pre></td></tr></tbody></table></figure><br>这里是用 $\epsilon_\theta(z_n,n)-\epsilon_\theta(z_n,n,E^y)$ ，再乘上 $w$ <p></p>
<h3 id="一些总结"><a href="#一些总结" class="headerlink" title="一些总结"></a>一些总结</h3><ul>
<li>参考CLIP的预训练CLAP模型，将text和audio的特征向量对齐</li>
<li>Latent Diffusion Model，将text encoder得到的文本嵌入通过扩散模型映射到隐变量空间</li>
<li>VAE，用于从隐变量生成梅尔谱图</li>
<li>HiFi-GAN，将梅尔谱图还原为音频<h3 id="附录"><a href="#附录" class="headerlink" title="附录"></a>附录</h3>看看一些细节，好多没懂的<br>首先是用到的预训练模型CLAP，最初是微软提出的，这个之后开新坑看以下，本文用到的是另一篇基于CLAP提出的流水线技术。</li>
</ul>
<p>LDM使用的是StableDiffusion的backbone，是一个叫UNet的东西，但是怎么看怎么和那个条件概率推导链没什么关系<br>回到最开始的图像LDM<br><img src="https://img-blog.csdnimg.cn/f95df368ee754d13bbccafcd5e28ff11.png" alt="图像LDM"><br>UNet参与的地方是上图中的 Denoising 过程<br>还是看看代码 8️⃣ （服务器down了，之后再看8️⃣）<br>代码看不懂，回来先翻译一下这段</p>
<blockquote>
<p>我们采用StableDiffusion的UNet backbone作为AudioLDM的LDM基本架构。如式5所示，UNet模型同时以时间步长t和CLAP嵌入E为条件（就是公式里的 n 和 $E^y$ ），我们将时间步长映射到一维嵌入中，然后将其与E链接为条件消息（conditioning information）。由于我们的条件向量只是一维的，我们没有使用StableDiffusion中的cross-attention mechanism。相反，我们直接使用特征线性调制层（feature-wise linear modulation layer）将conditioning information与UNet卷积块的特征映射合并。我们使用的UNet主干有四个编码块，一个中间块和四个解码块。使用一个basic channel number $c_u$ ，encoder块的通道维数为 $[c_u,2c_u,3c_u,5c_u]$ ，decoder块的通道数相反（对称的），中间块的通道数为 $5c_u$。我们在最后三个encoder块 和 前三个decoder块 中添加了一个注意力块，距离来说，我们添加了两个多头自注意力层和一个全连接层作为注意力块。（忘了 ，之后再看一遍开个新坑），head数为 嵌入层数/$c_u$ 。然后是参数设置，在 forward process，使用了N=1000 steps。采用$\beta_1=0.0015$到$\beta_N=0.0195$的线性噪声范围。采样过程中，使用的是DDIM（看到都说训练用DDPM，采样用DDIM，会更快而且没什么影响）采样 200 steps</p>
</blockquote>
<p>VAE的encoder和decoder都是由卷积模块组成的，每个卷积模块由多个残差神经网络块（ResNet blocks，由卷积层和残差链接构成）组成，ResNet感觉可以开新坑</p>
<p>VAE的Loss，由重建损失，对抗损失，高斯约束损失组成（前面提到过<br>对抗损失：是用来提升重建质量的，具体来说，就是用一个模型（这里用的PatchGAN）来对重建梅尔谱图分块后进行预测，判断这个块是真的还是假的，输出logits，最大化正确识别真实patch的logits，同时最小化错误识别假patch的logits。<br>高斯约束损失：使VAE学习一个连续的结构化的潜在空间，而不是一个无组织的潜在空间，按照原始VAE的说法，这里应该是一个KL散度</p>
<p>Vocoder用的HiFi-GAN，这个之后开新坑</p>
<h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><p>写了点注释<br>代码看不懂，论文又看不懂，死了得了</p>
<p>在推理过程中，$\epsilon_\theta(z_n,n),\epsilon_\theta(z_n,n,E^y)$ 两个值是一起从模型出来的，然后在第一维切开成两半<br></p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">e_t_uncond, e_t = self.model.apply_model(x_in, t_in, c_in).chunk(<span class="number">2</span>)</span><br></pre></td></tr></tbody></table></figure><p></p>
<p>完整的UNet结构在 <code>ddpm.py</code> 里进<br></p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">out = self.diffusion_model(x, t, y=cc)</span><br></pre></td></tr></tbody></table></figure><br>里面确实可以看到四个块，至于通道数我没有深究<p></p>
<p>大概看了一遍推理的过程，跟着调试走了一遍，训练的代码没看S</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="http://yypyyds.github.io">脚踏车没有脚</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://yypyyds.github.io/2023/11/13/AudioLDM/">http://yypyyds.github.io/2023/11/13/AudioLDM/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://yypyyds.github.io" target="_blank">脚踏车的日志站</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E8%AE%BA%E6%96%87/">论文</a></div><div class="post_share"><div class="social-share" data-image="/img/touxiang.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2023/11/20/CLAP/" title="CLAP"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">CLAP</div></div></a></div><div class="next-post pull-right"><a href="/2023/11/07/VQ-VAE%E7%9B%B8%E5%85%B3/" title="VQ-VAE相关"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">VQ-VAE相关</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2023/11/28/AudioLDM2/" title="AudioLDM2"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-11-28</div><div class="title">AudioLDM2</div></div></a></div><div><a href="/2023/11/20/CLAP/" title="CLAP"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-11-20</div><div class="title">CLAP</div></div></a></div><div><a href="/2023/10/11/BEATS-Audio-Pre-Training-with-Acoustic-Tokenizers/" title="BEATS-Audio Pre-Training with Acoustic Tokenizers"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-10-11</div><div class="title">BEATS-Audio Pre-Training with Acoustic Tokenizers</div></div></a></div><div><a href="/2024/04/08/%E7%94%9F%E6%88%90%E7%9B%B8%E5%85%B3/" title="生成相关"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-04-08</div><div class="title">生成相关</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/touxiang.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">脚踏车没有脚</div><div class="author-info__description">不积跬步，无以至千里</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">13</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">13</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">5</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/yypyyds"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">这是一个努力学习的笨蛋的博客</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#AudioLDM"><span class="toc-text">AudioLDM</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AD%A6%E4%B9%A0%E8%BF%87%E7%A8%8B%E5%8F%82%E8%80%83%E7%9A%84%E7%9B%B8%E5%85%B3%E6%96%87%E7%AB%A0"><span class="toc-text">学习过程参考的相关文章</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Introduction"><span class="toc-text">Introduction</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Text-Conditional-Audio-Generation"><span class="toc-text">Text-Conditional Audio Generation</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Contrastive-Language-Audio-Pretraining-CLAP"><span class="toc-text">Contrastive Language-Audio Pretraining (CLAP)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Conditional-Latent-Diffusion-Models"><span class="toc-text">Conditional Latent Diffusion Models</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Conditioning-Augmentation"><span class="toc-text">Conditioning Augmentation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Classifier-free-Guidance"><span class="toc-text">Classifier-free Guidance</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Decoder"><span class="toc-text">Decoder</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Text-Guided-Audio-Manipulation"><span class="toc-text">Text-Guided Audio Manipulation</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Experiments"><span class="toc-text">Experiments</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BB%93%E6%9E%9C"><span class="toc-text">结果</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%B6%88%E8%9E%8D%E5%AE%9E%E9%AA%8C"><span class="toc-text">消融实验</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%80%E4%BA%9B%E6%80%BB%E7%BB%93"><span class="toc-text">一些总结</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%99%84%E5%BD%95"><span class="toc-text">附录</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BB%A3%E7%A0%81"><span class="toc-text">代码</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/09/14/%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B%E4%B8%AD%E7%9A%84%E7%94%9F%E6%88%90%E6%8E%A7%E5%88%B6%E6%96%B9%E6%B3%95%E5%92%8C%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/" title="扩散模型中的生成控制方法和注意力机制">扩散模型中的生成控制方法和注意力机制</a><time datetime="2024-09-14T03:08:43.000Z" title="发表于 2024-09-14 11:08:43">2024-09-14</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/07/08/%E9%9F%B3%E4%B9%90%E7%94%9F%E6%88%90%E9%9A%90%E5%86%99/" title="音乐生成隐写">音乐生成隐写</a><time datetime="2024-07-08T08:20:58.000Z" title="发表于 2024-07-08 16:20:58">2024-07-08</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/05/15/%E5%AF%B9LDM%E4%BB%A3%E7%A0%81%E7%9A%84%E5%AD%A6%E4%B9%A0/" title="对LDM代码的学习">对LDM代码的学习</a><time datetime="2024-05-15T07:34:44.000Z" title="发表于 2024-05-15 15:34:44">2024-05-15</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/04/08/%E7%94%9F%E6%88%90%E7%9B%B8%E5%85%B3/" title="生成相关">生成相关</a><time datetime="2024-04-08T09:00:48.000Z" title="发表于 2024-04-08 17:00:48">2024-04-08</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/03/25/PyTorch%20Lighting%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0/" title="PyTorch Lighting框架学习">PyTorch Lighting框架学习</a><time datetime="2024-03-25T07:29:07.000Z" title="发表于 2024-03-25 15:29:07">2024-03-25</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2024 By 脚踏车没有脚</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div></div></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.min.js"></script><script src="/js/search/local-search.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        insertScript: [200, () => {
          document.querySelectorAll('mjx-container').forEach(node => {
            if (node.hasAttribute('display')) {
              btf.wrap(node, 'div', { class: 'mathjax-overflow' })
            } else {
              btf.wrap(node, 'span', { class: 'mathjax-overflow' })
            }
          });
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typeset()
}</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>