<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>扩散模型中的生成控制方法和注意力机制</title>
      <link href="/2024/09/14/%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B%E4%B8%AD%E7%9A%84%E7%94%9F%E6%88%90%E6%8E%A7%E5%88%B6%E6%96%B9%E6%B3%95%E5%92%8C%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/"/>
      <url>/2024/09/14/%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B%E4%B8%AD%E7%9A%84%E7%94%9F%E6%88%90%E6%8E%A7%E5%88%B6%E6%96%B9%E6%B3%95%E5%92%8C%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/</url>
      
        <content type="html"><![CDATA[<h1 id="扩散模型中的生成控制方法和注意力机制"><a href="#扩散模型中的生成控制方法和注意力机制" class="headerlink" title="扩散模型中的生成控制方法和注意力机制"></a>扩散模型中的生成控制方法和注意力机制</h1><h2 id="DDPM"><a href="#DDPM" class="headerlink" title="DDPM"></a>DDPM</h2><p><img src="/2024/09/14/%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B%E4%B8%AD%E7%9A%84%E7%94%9F%E6%88%90%E6%8E%A7%E5%88%B6%E6%96%B9%E6%B3%95%E5%92%8C%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/image-20240914111002726.png" alt=""><br>这是DDPM中的UNet<br>这里的注意力机制是可选的，在DownBlock和UpBlock里面，并且上面每一步都有一个<code>AddTimeEmbedding</code>的操作<br><img src="/2024/09/14/%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B%E4%B8%AD%E7%9A%84%E7%94%9F%E6%88%90%E6%8E%A7%E5%88%B6%E6%96%B9%E6%B3%95%E5%92%8C%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/image-20240914111912097.png" alt=""><br>TimeEmbedding层采用和Transformer一致的三角函数位置编码，将常数转变为向量。<br>Attention层则是沿着channel维度将图片拆分为Token，做完Attention后将输出组装<br>对于是DownBlock还是UpBlock，取决于<code>in_c</code>和<code>out_c</code>的大小</p><p>MiddleBlock是残差，注意力，残差的形式，和上面类似<br><img src="/2024/09/14/%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B%E4%B8%AD%E7%9A%84%E7%94%9F%E6%88%90%E6%8E%A7%E5%88%B6%E6%96%B9%E6%B3%95%E5%92%8C%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/image-20240914112516523.png" alt=""></p><p>这个attention还是分论文，有的有有的没有</p><h2 id="LDM"><a href="#LDM" class="headerlink" title="LDM"></a>LDM</h2><p>LDM相比DM，就是在图像压缩域做扩散，这是有理论支持的<br><img src="/2024/09/14/%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B%E4%B8%AD%E7%9A%84%E7%94%9F%E6%88%90%E6%8E%A7%E5%88%B6%E6%96%B9%E6%B3%95%E5%92%8C%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/image-20240914115605350.png" alt=""><br>随着比特数的增加，失真程度先降很快，然后再平缓<br>对应的意思是，在低比特数的小图上增加比特数，语义信息加的更多<br>但是在分辨率变得越来越高时，加的信息主要是感知信息<br>而扩散模型学到的主要是结构，忽略细节，所以让他建模语义信息更好（生成小图）</p><p>在LDM中的条件导入，是通过cross-attention导入的<br><img src="/2024/09/14/%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B%E4%B8%AD%E7%9A%84%E7%94%9F%E6%88%90%E6%8E%A7%E5%88%B6%E6%96%B9%E6%B3%95%E5%92%8C%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/image-20240914115954408.png" alt=""><br>这个条件编码器是一个特定于数据域的编码器，每种模态都不一样</p><h1 id="A-Survey-of-Multimodal-Controllable-Diffusion-Models"><a href="#A-Survey-of-Multimodal-Controllable-Diffusion-Models" class="headerlink" title="A Survey of Multimodal Controllable Diffusion Models"></a>A Survey of Multimodal Controllable Diffusion Models</h1><p>直接看第三节</p><h2 id="Controllable-Generation"><a href="#Controllable-Generation" class="headerlink" title="Controllable Generation"></a>Controllable Generation</h2><h3 id="Semantic-Control"><a href="#Semantic-Control" class="headerlink" title="Semantic Control"></a>Semantic Control</h3><p>控制图像的突出特征，精细的可控性允许对生成的图像进行细粒度的调整<br>主要的挑战是赋予生成模型语义理解，使它们能够表示从其他因素中解构出来的图像属性，并精确地响应语义控制。</p><h3 id="Spatial-Control"><a href="#Spatial-Control" class="headerlink" title="Spatial Control"></a>Spatial Control</h3><p>细粒度控制生成元素在图像上的区域<br>有 layout or segmentation-guided方法，在 bounding boxes或 segmentation maps上做位置条件<br>sketch, edge guided 方法，对轮廓进行控制<br>depth-guided方法可以利用图像的深度信息<br>skeleton-guided可以控制人体动作<br>将空间信息和自然语言描述相结合，比如ControlNet<br>还有FreeControl</p><h3 id="ID-Control"><a href="#ID-Control" class="headerlink" title="ID Control"></a>ID Control</h3><p>控制生成图像特定于某个对象<br>有三类控制方法：基于CLIP，基于扩散优化过程，基于解码器<br>优化过程可能会过拟合，并且时间长<br>解码器有zero shot的能力，但是可能会有复制粘贴的效果</p><h3 id="Style-Control"><a href="#Style-Control" class="headerlink" title="Style Control"></a>Style Control</h3><p>有如下方法：</p><ul><li>解耦特征和内容编码</li><li>在隐空间加权插值</li><li>在采样的每个step使用类似classifier guidance的方法，使用估计的损失梯度诱导生成</li><li>Textual Inversion, Dreambooth, LoRA，然后使用微调的模型对图像的latent code进行解码<br>但是大部分的控制机制是离散的而不是连续的<br>一个开放的研究方向是实现对颜色、纹理、光照等属性的流畅、颗粒化操作<h3 id="Controllability-Trade-Off"><a href="#Controllability-Trade-Off" class="headerlink" title="Controllability Trade-Off"></a>Controllability Trade-Off</h3>有三种类型的平衡<br>Fidelity-Diversity，Faithfulness-Realism，Speed-Fidelity</li></ul><h2 id="控制方法"><a href="#控制方法" class="headerlink" title="控制方法"></a>控制方法</h2><h3 id="Guidance"><a href="#Guidance" class="headerlink" title="Guidance"></a>Guidance</h3><p>这类方法使用frozen的扩散模型作为基础模型，在采样阶段引入修改<br>classifier-guidance<br><img src="/2024/09/14/%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B%E4%B8%AD%E7%9A%84%E7%94%9F%E6%88%90%E6%8E%A7%E5%88%B6%E6%96%B9%E6%B3%95%E5%92%8C%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/image-20240918152332177.png" alt=""><br>可以得到这样的梯度分解<br>在原始的扩散梯度上，加入了分类器对抗梯度</p><h3 id="Condition"><a href="#Condition" class="headerlink" title="Condition"></a>Condition</h3><p>classifier-free guidance<br>分别计算条件得分和无条件得分<br><img src="/2024/09/14/%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B%E4%B8%AD%E7%9A%84%E7%94%9F%E6%88%90%E6%8E%A7%E5%88%B6%E6%96%B9%E6%B3%95%E5%92%8C%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/image-20240918155459824.png" alt=""></p><h3 id="Attention-Based-Modification"><a href="#Attention-Based-Modification" class="headerlink" title="Attention-Based Modification"></a>Attention-Based Modification</h3><p>使用U-Net中的交叉注意力机制控制条件生成<br>在词特征和生成目标的的交叉注意力图中发现了重要的局部相似性<br>通过引入修改过的注意力图对原来的图做覆盖</p><h3 id="Range-Null-Space-Decomposition"><a href="#Range-Null-Space-Decomposition" class="headerlink" title="Range-Null Space Decomposition"></a>Range-Null Space Decomposition</h3><p><a href="https://zhuanlan.zhihu.com/p/588663035">https://zhuanlan.zhihu.com/p/588663035</a><br>好酷<br>在图像上做超分辨率，把原始图像分解成零域空间和值域空间，之前李博组会上也讲过</p><h3 id="Performance-Trade-Offs"><a href="#Performance-Trade-Offs" class="headerlink" title="Performance Trade-Offs"></a>Performance Trade-Offs</h3><p>Truncation：通过限制采样分布的方差来提高清晰度，降低生成多样性</p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Diffusion </tag>
            
            <tag> Attention </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>音乐生成隐写</title>
      <link href="/2024/07/08/%E9%9F%B3%E4%B9%90%E7%94%9F%E6%88%90%E9%9A%90%E5%86%99/"/>
      <url>/2024/07/08/%E9%9F%B3%E4%B9%90%E7%94%9F%E6%88%90%E9%9A%90%E5%86%99/</url>
      
        <content type="html"><![CDATA[<h1 id="音乐生成隐写"><a href="#音乐生成隐写" class="headerlink" title="音乐生成隐写"></a>音乐生成隐写</h1><p>先列一下要做的几件事<br>找近三年文生音乐的论文<br>找音频隐写，视频隐写，图像隐写的论文</p><h2 id="音乐生成"><a href="#音乐生成" class="headerlink" title="音乐生成"></a>音乐生成</h2><p>应该多找点，然后筛48k的<br>暂未录用：<br><a href="https://arxiv.org/abs/2407.03648">High Fidelity Text-Guided Music Generation and Editing via Single-Stage Flow Matching</a><br>text controlled, 48k stereo，支持文本细粒度控制，支持长时间的风格转换，没开源</p><p><a href="https://www.arxiv.org/abs/2407.02049">Accompanied Singing Voice Synthesis with Fully Text-controlled Melody</a><br>text, MIDI controlled, 伴奏合成, 24k mono, 合成人声和伴奏，听着对不太上，没开源</p><p>ICML2024: <a href="https://openreview.net/pdf?id=kOczKjmYum">MusicFlow: Cascaded Flow Matching for Text Guided Music Generation</a></p><p><a href="https://arxiv.org/pdf/2306.05284">MusicGen: Simple and Controllable Music Generation </a> <a href="https://github.com/facebookresearch/audiocraft">code</a><br><a href="https://arxiv.org/pdf/2308.01546">MusicLDM: Enhancing Novelty in Text-to-Music Generation Using Beat-Synchronous Mixup Strategies</a> <a href="https://github.com/RetroCirce/MusicLDM/">code</a> 只放了推理代码<br><a href="https://arxiv.org/pdf/2310.17162">CONTENT-BASED CONTROLS FOR MUSIC LARGE LANGUAGE MODELING</a> <a href="https://github.com/Kikyo-16/coco-mulla-repo">code</a><br>只有推理代码<br><a href="https://arxiv.org/pdf/2311.08355">Mustango: Toward Controllable Text-to-Music Generation</a> <a href="https://github.com/AMAAI-Lab/mustango">code</a><br>多条件文本可控，UNet为Music-Domain-Knowledge-Informed UNet</p><p><a href="https://arxiv.org/abs/2401.04577">Masked Audio Generation using a Single Non-Autoregressive Transformer</a> <a href="https://github.com/facebookresearch/audiocraft/blob/main/docs/MAGNET.md">code</a><br>自回归生成音频，也包括音乐</p><p><a href="https://arxiv.org/pdf/2402.06178">MusicMagus: Zero-Shot Text-to-Music Editing via Diffusion Models</a> <a href="https://github.com/ldzhangyx/MusicMagus">code</a><br>修改文本生成音乐的部分音乐元素</p><p><a href="https://arxiv.org/pdf/2402.04825">Fast Timing-Conditioned Latent Audio Diffusion</a> <a href="https://github.com/Stability-AI/stable-audio-tools">code</a><br>长时立体声生成<br><a href="https://arxiv.org/abs/2404.10301">Long-form music generation with latent diffusion</a> <a href="https://github.com/Stability-AI/stable-audio-tools/">code</a><br>这两篇工作都是SD他们的，然后代码也是一个仓库，不知道什么情况，然后这个二代更长了</p><p><a href="https://arxiv.org/abs/2405.18386">Instruct-MusicGen: Unlocking Text-to-Music Editing for Music Language Models via Instruction Tuning</a> <a href="https://github.com/ldzhangyx/instruct-musicgen">code</a><br>文本诱导音乐编辑</p><p><a href="https://arxiv.org/pdf/2405.18503">SoundCTM: Uniting Score-based and Consistency Models for Text-to-Sound Generation</a> <a href="https://github.com/sony/soundctm">code</a><br>快速生成音乐，在低采样步的情况下尽量保持生成质量</p><p><a href="https://arxiv.org/abs/2406.08384">Diff-A-Riff: Musical Accompaniment Co-creation via Latent Diffusion Models</a><br>没开源，做的伴奏生成</p><p>首先明确一下目标，这么多工作，需要能生成wav文件的<br>风格转换，歌声转换也可以看看？</p><h2 id="生成式隐写"><a href="#生成式隐写" class="headerlink" title="生成式隐写"></a>生成式隐写</h2><p><a href="https://ieeexplore.ieee.org/document/10418202">Provably Secure Public-Key Steganography Based on Elliptic Curve Cryptography</a><br>ECC+图像生成模型<br><a href="https://ieeexplore.ieee.org/document/10418193">Toward Secure and Robust Steganography for Black-Box Generated Images</a><br>针对黑盒系统生成的图片的隐写，什么稳健系数<br><a href="https://ieeexplore.ieee.org/document/10306313">Provably Secure Robust Image Steganography</a><br>GAN隐变量映射密文隐写<br><a href="https://ieeexplore.ieee.org/document/10179287">Discop: Provably Secure Steganography in Practice Based on “Distribution Copies”</a><br>在扩散模型的分布采样中设置分布副本作为隐写信息源<br><a href="https://ieeexplore.ieee.org/document/9931443">Cover Reproducible Steganography via Deep Generative Models</a><br>没太看懂<br><a href="https://ieeexplore.ieee.org/document/9477049">Distribution-Preserving Steganography Based on Text-to-Speech Generative Models</a><br>使用生成模型保护隐写分布<br><a href="https://ieeexplore.ieee.org/abstract/document/10354433">Steganography With Generated Images: Leveraging Volatility to Enhance Security</a><br>根据生成模型生成图像的分布掩盖隐写修改<br><a href="https://www.mdpi.com/2227-7390/12/4/615">Image Steganography and Style Transformation Based on Generative Adversarial Network</a><br>通过风格转换这个变换通道隐藏信息，使用GAN训练降低隐写分析准确率</p><h2 id="生成式水印"><a href="#生成式水印" class="headerlink" title="生成式水印"></a>生成式水印</h2><p><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Yang_Gaussian_Shading_Provable_Performance-Lossless_Image_Watermarking_for_Diffusion_Models_CVPR_2024_paper.html">Gaussian Shading: Provable Performance-Lossless Image Watermarking for Diffusion Models</a>无需训练，没有性能损失的水印方案<br><a href="https://dl.acm.org/doi/abs/10.1145/3581783.3612448">Flexible and Secure Watermarking for Latent Diffusion Model</a><br>在前向传播过程中融合信息矩阵和中间输出，将水印信息嵌入，无需再次训练LDM</p><p>7.12小组会<br>STC 嵌入代价</p><h1 id="生成式隐写研究综述"><a href="#生成式隐写研究综述" class="headerlink" title="生成式隐写研究综述"></a>生成式隐写研究综述</h1><p><img src="/2024/07/08/%E9%9F%B3%E4%B9%90%E7%94%9F%E6%88%90%E9%9A%90%E5%86%99/image-20240709111739805.png" alt=""></p><p><img src="/2024/07/08/%E9%9F%B3%E4%B9%90%E7%94%9F%E6%88%90%E9%9A%90%E5%86%99/image-20240709112306614.png" alt=""><br>都是老方法，我想找新的，基于扩散模型的生成方法</p><h1 id="CRoSS-Diffusion-Model-Makes-Controllable-Robust-and-Secure-Image-Steganography"><a href="#CRoSS-Diffusion-Model-Makes-Controllable-Robust-and-Secure-Image-Steganography" class="headerlink" title="CRoSS: Diffusion Model Makes Controllable, Robust and Secure Image Steganography"></a>CRoSS: Diffusion Model Makes Controllable, Robust and Secure Image Steganography</h1><p><img src="/2024/07/08/%E9%9F%B3%E4%B9%90%E7%94%9F%E6%88%90%E9%9A%90%E5%86%99/image-20240712113759215.png" alt=""><br>现有方法无论有没有cover，都不能在安全性、可控性和鲁棒性方面达到很好的统一</p><p>使用可控的扩散模型，可以天然的适配这些问题<br>安全性：利用DDIM可逆技术进行基于扩散的图像翻译，保证了翻译过程的可逆性。这种可逆的翻译过程实现了无cover的隐写框架，确保了隐藏图像的安全性。<br>可控性：条件扩散模型强大的控制能力使容器图像具有高度可控性，扩散模型的生成先验保证了容器图像的视觉质量;<br>鲁棒性：扩散模型本质上是高斯去噪器，对噪声和扰动具有天然的鲁棒性。即使在传输过程中容器图像被加噪，我们仍然可以揭示秘密图像的主要内容。</p><p>很重要，无需任何额外的训练！围绕SD社区，提出了基于lora，controlnet，prompts的变体</p><h2 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h2><h3 id="Definition-of-Image-Steganography"><a href="#Definition-of-Image-Steganography" class="headerlink" title="Definition of Image Steganography"></a>Definition of Image Steganography</h3><p><img src="/2024/07/08/%E9%9F%B3%E4%B9%90%E7%94%9F%E6%88%90%E9%9A%90%E5%86%99/image-20240709161312159.png" alt=""><br>$X_{sec}$ 是需要隐写的秘密图像，通过 <code>Hide Process</code> 嵌入容器图像 $X_{cont}$ ，在网络传播过程中，容器图像可能会遭遇损失，导致质量下降，变为 $X’_{cont}$ ，但是经过 <code>Reveal Process</code> 之后，可以得到语义一致的结果。</p><h3 id="Invertible-Image-Translation-using-Diffusion-Model"><a href="#Invertible-Image-Translation-using-Diffusion-Model" class="headerlink" title="Invertible Image Translation using Diffusion Model"></a>Invertible Image Translation using Diffusion Model</h3><p>本方法使用Diffusion达成可提取性的方法是，把扩散采样DDIM过程中的噪声去掉，变成一个固定的常微分方程求解<br><strong>Diffusion Model Defined by DDIM</strong><br>原始的DDIM去噪过程可表示为</p><script type="math/tex; mode=display">x_s=\sqrt{\bar \alpha_s} \boldsymbol{f}_{\theta}(x_t,t)+\sqrt{1-\bar \alpha_s - \sigma^2_s}\; \epsilon_{\theta}(x_t,t) + \sigma_s \epsilon</script><p>当公式中的 $\sigma_s$ 设为 0 时，DDIM采样过程变为确定性。这时采样结果仅仅由 $x_t$ 确定。<br>采样过程可以等同于解决一个常微分方程</p><script type="math/tex; mode=display">x_0 = ODESolver(x_T;\epsilon_{\theta},T,0)</script><p>使用扩散模型进行图像转换<br><img src="/2024/07/08/%E9%9F%B3%E4%B9%90%E7%94%9F%E6%88%90%E9%9A%90%E5%86%99/image-20240712101558808.png" alt=""><br>加上控制条件</p><script type="math/tex; mode=display">x_0=ODESolver(x_T;\epsilon_{\theta},c,T,0)</script><p>图像转换有两个重要的点：两张图片的结构一致性和转换过程是否可逆<br><img src="/2024/07/08/%E9%9F%B3%E4%B9%90%E7%94%9F%E6%88%90%E9%9A%90%E5%86%99/image-20240712103110712.png" alt=""><br><img src="/2024/07/08/%E9%9F%B3%E4%B9%90%E7%94%9F%E6%88%90%E9%9A%90%E5%86%99/image-20240712103118929.png" alt=""><br><img src="/2024/07/08/%E9%9F%B3%E4%B9%90%E7%94%9F%E6%88%90%E9%9A%90%E5%86%99/image-20240712105202624.png" alt=""></p><p>使用私钥作为控制条件，为秘密图像加噪，使用公钥作为控制条件，从噪声生成秘密信息载体。<br>接收方使用公钥加噪，得到噪声，再使用私钥作为控制条件，从噪声生成秘密信息。<br>为了实现图像的可逆转换，采用确定性DDIM倒转(Inversion)过程。</p><p>这里很有意思，有空要详细研究下DDIM的过程<br>这里提到上面的DDIM去噪过程，在 $s<t$ 时，进行的是反向过程；在="" $s="">t$ 时，进行的是正向过程，这两个过程的操作是相似的，输入和输出的图片也是相似的，这个中间噪声 $x_T$ 可以被视作倒转过程的隐变量。</t$></p><script type="math/tex; mode=display">x_T = ODESolve(x_0;\epsilon_{\theta},c,0,T)\qquad x'_0=ODESolve(x_T;\epsilon_{\theta},c,T,0)</script><h3 id="The-Coverless-Image-Steganography-Framework-CRoSS"><a href="#The-Coverless-Image-Steganography-Framework-CRoSS" class="headerlink" title="The Coverless Image Steganography Framework CRoSS"></a>The Coverless Image Steganography Framework CRoSS</h3><p>和上面讲的差不多，就是用私钥掩盖秘密图像，公钥生成掩盖图像<br><strong>安全性</strong><br>使用不同prompt从噪声中生成的图像质量都很高，故攻击者很难从穷举的私钥中判断哪副生成的图片是真实的秘密图像。<br>载体不包含秘密信息。<br>Keys不但可以包含prompt，还可以选择ControlNet或LoRA<br>等于是把控制diffusion生成的方法都加进来了</p><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>隐写分析<br><img src="/2024/07/08/%E9%9F%B3%E4%B9%90%E7%94%9F%E6%88%90%E9%9A%90%E5%86%99/image-20240712112552121.png" alt=""><br>使用Size-Independent-Detector 的分析结果<br><img src="/2024/07/08/%E9%9F%B3%E4%B9%90%E7%94%9F%E6%88%90%E9%9A%90%E5%86%99/image-20240712113043543.png" alt=""><br>NIQE是一个（no-reference image quality assessment）模型，用来评估自然程度和视觉安全性<br>右边三个是三种隐写分析模型<br><strong>抗压缩和噪声</strong><br><img src="/2024/07/08/%E9%9F%B3%E4%B9%90%E7%94%9F%E6%88%90%E9%9A%90%E5%86%99/image-20240712113259972.png" alt=""><br>信道损失<br><img src="/2024/07/08/%E9%9F%B3%E4%B9%90%E7%94%9F%E6%88%90%E9%9A%90%E5%86%99/image-20240712113517670.png" alt=""></p><p>总结<br>这也能发，纯纯风格转换拿来说是隐写</p><h1 id="Distribution-Preserving-Steganography-Based-on-Text-to-Speech-Generative-Models"><a href="#Distribution-Preserving-Steganography-Based-on-Text-to-Speech-Generative-Models" class="headerlink" title="Distribution-Preserving Steganography Based on Text-to-Speech Generative Models"></a>Distribution-Preserving Steganography Based on Text-to-Speech Generative Models</h1><p><img src="/2024/07/08/%E9%9F%B3%E4%B9%90%E7%94%9F%E6%88%90%E9%9A%90%E5%86%99/image-20240712153402896.png" alt=""><br>隐写安全性可以用cover和stego的相对熵量化<br>$D(P_c\Vert P_s)=0$ 意味着绝对的安全，攻击者无法区分cover和stego<br>设计了两种分布保持隐写方法<br>引入了一种 reversible flow-based generative model 作为采样器<br>将加密的数据映射到latent code，该空间符合高斯分布然后将 latent code送入生成模型，得到生成数据。<br>一旦生成模型得到良好的训练和固定，生成模型就可以生成遵循相同分布的数据，从而满足采样器的要求。生成模型可以从公共站点下载，例如github。接收方与发送方共享相同的生成模型，生成模型的可逆性保证了从生成的数据中正确提取消息。<br>本文以文本-语音生成模型WaveGlow为例验证其实际安全性。<br>对于基于压缩的隐写系统，采用自回归模型，因为他们生成数据的良好分布<br>看不懂<br>总之就是针对两个场景 基于采样的分布保持，基于压缩的分布保持 提出了两个隐写方案。<br><img src="/2024/07/08/%E9%9F%B3%E4%B9%90%E7%94%9F%E6%88%90%E9%9A%90%E5%86%99/image-20240712165457292.png" alt=""><br>通信模型<br>隐写安全性在信息论中，用cover和stego的分布的KL散度衡量<br><img src="/2024/07/08/%E9%9F%B3%E4%B9%90%E7%94%9F%E6%88%90%E9%9A%90%E5%86%99/image-20240712165703042.png" alt=""></p><h3 id="Existing-Distribution-Preserving-Methods"><a href="#Existing-Distribution-Preserving-Methods" class="headerlink" title="Existing Distribution-Preserving Methods"></a>Existing Distribution-Preserving Methods</h3><p>基于采样器的分布保持方法可以这样表示<br><img src="/2024/07/08/%E9%9F%B3%E4%B9%90%E7%94%9F%E6%88%90%E9%9A%90%E5%86%99/image-20240715113040335.png" alt=""><br>给定一个映射函数 $f_k(\cdot):K\times C \rightarrow R$，其中 $K,C,R$ 分别是 密钥空间，载体空间，和信息空间<br>使用密钥 $k$ 和 e 比特的标志 $b=\{0,1\}^e\in R$ ，嵌入隐写系统的消息是基于拒绝采样算法的<br>就是依托这个 $O^C$ 采样一个隐写载体 $s$ ，可以从里面提取出 b 来</p><p>另一类保持分布的隐写是基于压缩的隐写<br>基于压缩的方案需要知道确切的cover分布，这在数字媒体对象的分布中很难捕获。对于基于采样器的系统，很难获得完美的采样器，现有方案的容量也很低。为此，我们将生成模型引入到分布保持隐写术中。</p><h3 id="TTS-Models"><a href="#TTS-Models" class="headerlink" title="TTS Models"></a>TTS Models</h3><p>使用WaveGlow和WaveRNN<br>看不懂，先往后看吧</p><h2 id="Distribution-preserving-steganography-based-on-an-implicit-generative-model"><a href="#Distribution-preserving-steganography-based-on-an-implicit-generative-model" class="headerlink" title="Distribution-preserving steganography based on an implicit generative model"></a>Distribution-preserving steganography based on an implicit generative model</h2><p><img src="/2024/07/08/%E9%9F%B3%E4%B9%90%E7%94%9F%E6%88%90%E9%9A%90%E5%86%99/image-20240723104356397.png" alt=""><br>这个图上看，Waveglow生成wav需要一个高斯向量，消息嵌入就是通过对高斯向量的映射实现的</p><h3 id="Message-Mapping"><a href="#Message-Mapping" class="headerlink" title="Message Mapping"></a>Message Mapping</h3><p>经典的将均匀分布映射到高斯分布<br><img src="/2024/07/08/%E9%9F%B3%E4%B9%90%E7%94%9F%E6%88%90%E9%9A%90%E5%86%99/image-20240723110127401.png" alt=""><br>首先对长为 n 的秘密消息 $m’$ ，要生成latent code，然后用这个latent送到waveglow里做TTS<br>这个latent code是符合高斯分布的<br>将高斯分布的分布函数划分为 $2^p$ 个区间，然后得到在 $(-\infty, \infty)$ 上对应的 x 值的点<br>将秘密信息按照每组 p 比特划分<br>然后就是拒绝采样算法得到样本 s</p><h3 id="Message-Embedding-and-Extraction"><a href="#Message-Embedding-and-Extraction" class="headerlink" title="Message Embedding and Extraction"></a>Message Embedding and Extraction</h3><p>得到latent code之后，给定一段文本和一个说话人ID，SPN（spectrogram generation model）将他们转换为梅尔谱图 $F_{mel}$ ，然后使用一堆Flow模型生成wav<br><img src="/2024/07/08/%E9%9F%B3%E4%B9%90%E7%94%9F%E6%88%90%E9%9A%90%E5%86%99/image-20240723113238926.png" alt=""></p><p>提取就是Flow的逆过程<br><img src="/2024/07/08/%E9%9F%B3%E4%B9%90%E7%94%9F%E6%88%90%E9%9A%90%E5%86%99/image-20240723113948579.png" alt=""><br>消息通过消息映射的逆得到</p><h2 id="Distribution-preserving-steganography-based-on-an-explicit-generative-model"><a href="#Distribution-preserving-steganography-based-on-an-explicit-generative-model" class="headerlink" title="Distribution-preserving steganography based on an explicit generative model"></a>Distribution-preserving steganography based on an explicit generative model</h2><p>上面的是基于隐式生成模型的，这个是基于显示生成模型<br><img src="/2024/07/08/%E9%9F%B3%E4%B9%90%E7%94%9F%E6%88%90%E9%9A%90%E5%86%99/image-20240723145234299.png" alt=""><br>说这个数据隐藏和数据压缩是差不多的过程<br>说是自然媒体的分布很难获得，但是生成模型可以拟合自然数据的分布，可以解决上述问题</p><p>为了解决这个问题，我们设计了基于文本到语音自回归模型WaveRNN的隐写系统，该系统可以生成具有丰富语义的音频。此外，语义自然地嵌入到音频中，这意味着接收方可以获得与发送方相同的文本，这样它们就可以使用相同的WaveRNN模型重新实现相同的生成过程。<br>方案流程图<br><img src="/2024/07/08/%E9%9F%B3%E4%B9%90%E7%94%9F%E6%88%90%E9%9A%90%E5%86%99/image-20240723150917044.png" alt=""><br>这个WaveRNN都什么年代的论文了，18年的老东西</p><h3 id="Message-Embedding-and-Extraction-1"><a href="#Message-Embedding-and-Extraction-1" class="headerlink" title="Message Embedding and Extraction"></a>Message Embedding and Extraction</h3><p>这里涉及到一个自适应算术（adaptive arithmetic）<br>嵌入过程和解码相关，提取过程和编码相关<br>看不懂啊<br>$A=\{a_1,a_2,\cdots , a_m\}$ 是生成的音频样本值按一定顺序排列的字母表，什么意思<br>其中元素的分布概率为 $P=\{P(a_1),P(a_2),\cdots,P(a_m)\}$<br>分布函数为</p><script type="math/tex; mode=display">F(a_i) = \sum^i_{k=1}P(a_k)</script><p><strong>Message embedding</strong><br>给定加密信息 $m’=[m_1m_2m_3\cdots m_L]$ ，可以被解释为一个小数 $q$ </p><script type="math/tex; mode=display">m_1m_2m_3\cdots m_L \rightarrow q=0.m_1m_2m_3\cdots m_L = \sum^L_{i=1}m_i \cdot 2^{-i}</script><p>采用自适应算术解码算法，从区间 $[0,1)$ 开始，根据符号的概率 $P$ 将其细分为子区间 $[l,h)$<br>然后将和这个子区间相关的符号 $a_j$ ，其中 $q$ 在这个子区间内，并入隐写信息</p><script type="math/tex; mode=display">y=y::a_j</script><p>其中 $::$ 表示将后面的符号附加到前面的向量中<br>符号表的概率 $P$ 定期更新<br>这个更新是按照设定更新的，预设好了一堆离散概率分布</p><p>然后按照下式根据更新的概率 $P$ 重新计算子区间</p><script type="math/tex; mode=display">\begin{aligned}h_k &= h_{k-1}+(h_{k-1}-l_{k-1}) *F(a_j)\newlinel_k &= l_{k-1}+(h_{k-1}-l_{k-1}) *F(a_{j-1})\end{aligned}</script><p>其中 $h_k$ 和 $l_k$ 是第k步<strong>子区间</strong>的上下界<br>重复这个过程，直到小数 $q$ 满足如下约束</p><script type="math/tex; mode=display">\begin{aligned}a+(0.5)^L &\notin[l_k,h_k)\newlineq-(0.5)^L &\notin[l_k,h_k)\end{aligned}</script><p>这个约束保证了二元分数 $q$ 是在区间 $[l_k,h_k)$ 中的唯一的长为 $L$ 的分数<br>这样可以正确的提取消息，信息长度 $L$ 和符号的概率 $P$ 与接收方共享<br><strong>Message extraction</strong><br>在接收端，区间 $[l,h)$ 从 $[0,1)$ 开始划分到和符号概率正比长度的子区间<br>如果第 k 个元素 $y_k$ 和符号 $a_j$ 相关，按照如下更新子区间</p><script type="math/tex; mode=display">\begin{aligned}h_k &= h_{k-1}+(h_{k-1}-l_{k-1}) *F(a_j)\newlinel_k &= l_{k-1}+(h_{k-1}-l_{k-1}) *F(a_{j-1})\end{aligned}</script><p>和上面一样<br>循环这个过程，直到次数达到和 $y$ 的长度一致<br>最后，找到满足 $l_L\leq q \leq h_L$ 的小数 $q=\sum^L_{i=1}m_i 2^{-i}$<br>其中 $m_i \in \{0,1\}$ 是消息比特，L 是消息长度</p><p>所以这些东西和基于压缩的隐写有什么关系呢？</p><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><h3 id="Evaluation-Metric"><a href="#Evaluation-Metric" class="headerlink" title="Evaluation Metric"></a>Evaluation Metric</h3><p>主观指标：让人听听是否自然</p><p>这个主观评估，好复杂，看不懂一点<br>上文介绍到分布保持的定义是基于KL散度，因为KL散度会输出不稳定的结果，这里使用<code>energy distance</code>和<code>Wasserstein distance</code>来测量一阶分布（一阶分布是什么）<br>好像就是一阶矩，看公式<br>分布$u,v$的<code>energy distance</code>为<br><img src="/2024/07/08/%E9%9F%B3%E4%B9%90%E7%94%9F%E6%88%90%E9%9A%90%E5%86%99/image-20240730103519226.png" alt=""><br>其中$X,Y,X’,Y’$为独立随机变量<br>看不懂，什么<code>length of vector</code><br><code>Wasserstein distance</code>如下，也看不懂<br><img src="/2024/07/08/%E9%9F%B3%E4%B9%90%E7%94%9F%E6%88%90%E9%9A%90%E5%86%99/image-20240730104747567.png" alt=""><br>$\Gamma(u,v)$ 是边缘分布为 $u,v$ 的二元分布</p><p>高阶分布的表现是通过隐写分析判断的<br>选取 <code>combined feature of time-Markov and mel-frequency(CTM)</code>特征组合来评估安全性能，是SoTA特征，不同嵌入算法和载荷训练不同的分类器</p><p>最后的安全性使用平均错误率度量</p><p>为了说明隐写分析算法的有效性，引入基于修改的隐写算法作为比较，包括LSBM和DFR</p><h3 id="Sampler-Based-Stegosystem"><a href="#Sampler-Based-Stegosystem" class="headerlink" title="Sampler-Based Stegosystem"></a>Sampler-Based Stegosystem</h3><p>SPN使用的是Flowtron，然后用WaveGlow生成wav<br>几个距离如下，文章说这个数值很小，可以认为两个分布没有区别<br><img src="/2024/07/08/%E9%9F%B3%E4%B9%90%E7%94%9F%E6%88%90%E9%9A%90%E5%86%99/image-20240730191525538.png" alt=""><br>在LJSpeech数据集上，嵌入比特数和隐写分析错误率的图<br><img src="/2024/07/08/%E9%9F%B3%E4%B9%90%E7%94%9F%E6%88%90%E9%9A%90%E5%86%99/image-20240730191628855.png" alt=""></p><p><img src="/2024/07/08/%E9%9F%B3%E4%B9%90%E7%94%9F%E6%88%90%E9%9A%90%E5%86%99/image-20240730191641323.png" alt=""><br>嵌入比特数对性能的影响</p><h3 id="Compression-Based-Stegosystem"><a href="#Compression-Based-Stegosystem" class="headerlink" title="Compression-Based Stegosystem"></a>Compression-Based Stegosystem</h3><p>Flowtron+WaveRNN<br>两个数据集上的两个分布距离<br><img src="/2024/07/08/%E9%9F%B3%E4%B9%90%E7%94%9F%E6%88%90%E9%9A%90%E5%86%99/image-20240730192053680.png" alt=""><br><img src="/2024/07/08/%E9%9F%B3%E4%B9%90%E7%94%9F%E6%88%90%E9%9A%90%E5%86%99/image-20240730192059649.png" alt=""></p><p><img src="/2024/07/08/%E9%9F%B3%E4%B9%90%E7%94%9F%E6%88%90%E9%9A%90%E5%86%99/image-20240730192203504.png" alt=""></p><h1 id="Fast-Timing-Conditioned-Latent-Audio-Diffusion"><a href="#Fast-Timing-Conditioned-Latent-Audio-Diffusion" class="headerlink" title="Fast Timing-Conditioned Latent Audio Diffusion"></a>Fast Timing-Conditioned Latent Audio Diffusion</h1><p>Stable Diffusion团队，ICML2024<br>大多数之前的工作都没有解决音乐和音效在持续时间上自然变化的问题。（开局给我干蒙了<br>研究重点是使用text prompt生成长时间，可变长度的44.1kHZ立体声音乐和的音频。</p><p>之前的工作要么推理速度很慢（自回归模型），要么不能生成可变长度的长时间音频，该工作依赖于latent diffusion，能干这些事<br>提问，如何生成可变长度的长时间音频</p><h2 id="Related-works"><a href="#Related-works" class="headerlink" title="Related works"></a>Related works</h2><p><strong>Timming condition</strong><br>是第一个在扩散模型上使用可变时间控制的<br><strong>Evaluation metrics</strong><br>常用的定量音频指标是在评估短时单通道16kHz音频生成的，<br>提出了新的定量指标来评估长时全带立体声生成，还有音乐性，立体声正确性和音乐结构的定量指标<br><strong>Multitask generative modeling</strong><br>能够生成音乐和音效</p><h2 id="Architecture"><a href="#Architecture" class="headerlink" title="Architecture"></a>Architecture</h2><p><img src="/2024/07/08/%E9%9F%B3%E4%B9%90%E7%94%9F%E6%88%90%E9%9A%90%E5%86%99/image-20240718150315046.png" alt=""><br>分为三个部分，VAE，conditioning signal，diffusion model</p><h3 id="VAE"><a href="#VAE" class="headerlink" title="VAE"></a>VAE</h3><p>为了能够给任意长度的音频编解码，使用了一个<code>fully-convolutional</code>结构，来自Descript Audio Codec<br>压缩率为32，将 $2\times L$ 的输入压缩为 $64 \times L / 1024$ 长度的隐变量</p><h3 id="Conditioning"><a href="#Conditioning" class="headerlink" title="Conditioning"></a>Conditioning</h3><p><strong>Text encoder</strong><br>自己从头训练了一个CLAP，发现要比预训练的效果好<br>并且在SD的实验中，text encoder的输出中，倒数第二层的文本特征可以提供比最终输出更好的控制信号，这些文本特征通过交叉注意力层提供给UNet</p><p><strong>Timing embeddings</strong><br><img src="/2024/07/08/%E9%9F%B3%E4%B9%90%E7%94%9F%E6%88%90%E9%9A%90%E5%86%99/image-20240718153433837.png" alt=""><br>在搜集每一个音频块（从一段完整的音频上切出来的95 sec音频片段）的时候，计算两个属性，音频块的起始时间<code>seconds_start</code>，和原始音频的总时间<code>seconds_total</code>，对于比训练窗口段的音频样本，填充静音片段</p><h3 id="Diffuison-model"><a href="#Diffuison-model" class="headerlink" title="Diffuison model"></a>Diffuison model</h3><p>好大的Diffuison，907M参数<br>结构好像也有创新，但是看他的描述和这里引用的Schneider的论文不一样<br>4层对称的上下采样块+skip connection<br>通道数分别为 1024,1024,1024,1280<br>每一块在卷积后还有一系列自注意力和交叉注意力层<br>prompt和时间条件由cross-attention传入</p><h3 id="Inference"><a href="#Inference" class="headerlink" title="Inference"></a>Inference</h3><p>使用基于DPMSolver++的采样策略 <a href="https://zhuanlan.zhihu.com/p/583367414#:~:text=%E6%B8%85%E5%8D%8E%E5%A4%A7%E5%AD%A6%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%B3%BB%E6%9C%B1%E5%86%9B">链接</a><br>可变长度是通过生成带静音片段的音频，然后修剪静音片段达到的</p><h2 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h2><h3 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h3><p>好多数据，806,284条audio共19500小时，使用来自AudioSparx的文本元数据</p><div class="table-container"><table><thead><tr><th>class</th><th>number of files</th><th>GBs of content</th></tr></thead><tbody><tr><td>music</td><td>66%</td><td>94%</td></tr><tr><td>sound effects</td><td>25%</td><td>5%</td></tr><tr><td>instrument</td><td>9%</td><td>1%</td></tr></tbody></table></div><h3 id="VAE-1"><a href="#VAE-1" class="headerlink" title="VAE"></a>VAE</h3><p>编解码器差分训练的，先共同训练，再单独继续训练解码器<br>为了保证立体声重建的一致性，专门为立体声信号设计了 <code>multi-resolution sum, difference STFT loss</code><br>为此，我们在STFT之前应用A-weighting，并使用2048、1024、512、256、128、64和32的窗口长度。我们还使用修改后的多尺度STFT鉴别器来接受立体声音频<br>判别器使用复数域的STFT表示和一个patch-based判别损失作为关键损失<br>这个STFT representation of the real and reconstructed audio不知道是干嘛的</p><h3 id="Prompt-presentation"><a href="#Prompt-presentation" class="headerlink" title="Prompt presentation"></a>Prompt presentation</h3><p>这个有说法的<br>对每个音频文件都有对应的自然语言描述，以及对于音乐的BPM，流派，情绪和音轨的乐器。<br>在text encoder和diffusion的训练过程中，通过将metadata的随机子串连接成字符串作为text prompt<br>这允许在推理过程中指定特定的属性，而不要求这些属性始终存在<br>数据分成两半，一般用结构表示 <code>Instruments:Gutar|Moods:Energetic</code>，一般用逗号隔开</p><h2 id="Methodology"><a href="#Methodology" class="headerlink" title="Methodology"></a>Methodology</h2><h3 id="Quantitative-metrics"><a href="#Quantitative-metrics" class="headerlink" title="Quantitative metrics"></a>Quantitative metrics</h3><p><strong>FD(Frechet Distance)</strong><br>不同于之前文章中使用VGGish计算的，这里用的Openl3模型，因为他接收的是48kHz的信号<br>将所有评估音频重新采样到44.1kHz，对于立体声，将左右声道独立的投影到Openl3的特征空间，然后拼接得到立体声特征，单通道的就复制一份</p><p><strong>KL</strong><br>使用PaSST来计算生成音频和参考音频标签的KL散度<br>对KL散度做了修改，以适应不同长度和更长的音频，包括将音频切割成重叠的分析窗口（10s，主要是要和PaSST训练的长度对齐，5s的重叠区域）<br>计算不同窗口同类别的均值，做softmax之后计算KL</p><p><strong>CLAP score</strong><br>使用CLAP LAION计算<br>从生成的长音频的，前，中，后，随机取三段，拼接起来重采样到48k</p><h3 id="Qualitative-metrics"><a href="#Qualitative-metrics" class="headerlink" title="Qualitative metrics"></a>Qualitative metrics</h3><p>Audio quality, Text alignment, Musicality Stereo correctness, Musical structure</p><h3 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h3><p>在Musiccaps上的结果<br><img src="/2024/07/08/%E9%9F%B3%E4%B9%90%E7%94%9F%E6%88%90%E9%9A%90%E5%86%99/image-20240721203615281.png" alt=""><br>分别是自编码器对音频保真度的影响<br>文本编码器的消融实验<br>和其他音频生成模型的对比<br><img src="/2024/07/08/%E9%9F%B3%E4%B9%90%E7%94%9F%E6%88%90%E9%9A%90%E5%86%99/image-20240721204754471.png" alt=""><br>时序控制</p><h3 id="思考"><a href="#思考" class="headerlink" title="思考"></a>思考</h3><p>音乐隐写求的是立体声高采样率？<br>生成式隐写和传统的音乐隐写，好就好在他可以控制生成<br>要同时利用音乐载体的高采样率带来的高隐写容量和生成式隐写对生成数据的分布可控性<br>那就是从带载体的分布保持相关的工作入手？</p><p>思考一个方法，如果我知道VAE的latent分布空间，通过加入嵌入比特，训练一个维护分布的模型，在输入嵌入比特后，输出对应的控制信息，让生成的数据保持分布</p><p>看看代码<br>先弄明白这几点<br>端到端的VAE是怎么运作的<br>控制信息是怎样传入的，没看到classifier-free guidance之类的</p><p>这个VAE啊，全是卷积，而且也没有采样过程，纯纯的ae<br>对于立体声，就是最后输出是双通道</p><p>每个控制条件（prompt，seconds_start，seconds_total）都有一个mask，暂时不知道这个mask是干嘛的<br></p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> key <span class="keyword">in</span> self.cross_attn_cond_ids:</span><br><span class="line">cross_attn_in, cross_attn_mask = conditioning_tensors[key]</span><br></pre></td></tr></tbody></table></figure><p></p><p>然后在sequence dimension把这几个控制tensor连接起来<br><code>[128,768]+[1,768]+[1,768]=[130,768]</code></p><h1 id="Message-Driven-Generative-Music-Steganography-Using-MIDI-GAN"><a href="#Message-Driven-Generative-Music-Steganography-Using-MIDI-GAN" class="headerlink" title="Message-Driven Generative Music Steganography Using MIDI-GAN"></a>Message-Driven Generative Music Steganography Using MIDI-GAN</h1><p><img src="/2024/07/08/%E9%9F%B3%E4%B9%90%E7%94%9F%E6%88%90%E9%9A%90%E5%86%99/image-20240804151307698.png" alt=""><br>聚焦于MIDI生成的隐写</p><ul><li>我们提出了一种结合纠错码(error correction code, ECC)的消息映射机制，将二进制秘密消息转换为浮点数组来合成MIDI。特别是，所提出的机制可以确保相同的 bit/bits 每次映射到完全不同的和弦，这有利于抵御隐写分析。</li><li>我们开发了一种基于GAN的消息驱动生成式MIDI隐写方案，以逃避基于深度学习的隐写分析器的检测。在我们的方法中，称为MIDI-GAN，生成器将秘密信息转换为人工隐写MIDI文件，该文件体积小，旋律优美，并且无法被隐写分析器检测到。特别是，隐写MIDI还可以以和弦序列的形式传输，使其与现有的生成图像/音频隐写术不同，并且没有理由怀疑。此外，鉴别器使stego MIDI文件尽可能与真实的MIDI文件相似。提取器从隐写MIDI或和弦序列中恢复秘密消息。</li><li>在我们的MIDI-GAN中，我们考虑了生成器和鉴别器之间的对抗性学习，以及生成器和提取器之间的对抗性学习。这种增强使生成器、鉴别器和提取器相互竞争，共同进步，提高了隐蔽性、不可检测性和鲁棒性。<br><img src="/2024/07/08/%E9%9F%B3%E4%B9%90%E7%94%9F%E6%88%90%E9%9A%90%E5%86%99/image-20240804155154884.png" alt=""></li></ul><h2 id="proposed-scheme"><a href="#proposed-scheme" class="headerlink" title="proposed scheme"></a>proposed scheme</h2><p>信息载体的前提是载体的广泛使用，确保载体的存在或出现不会引起怀疑<br>基本框架如图2所示。首先，用纠错码(error correction code, ECC)对秘密信息进行编码，将其映射到浮点数组中，并在消息驱动生成隐写步骤中通过MIDI-GAN生成隐写MIDI文件。接下来，对于消息恢复，在MIDI-GAN架构中起关键作用的基于CNN的提取器，从隐音MIDI文件或和弦数序列中提取浮点数组。然后通过逆映射机制检索秘密消息，并由ECC进行校正。在随后的小节中，我们将进一步详细介绍MIDI-GAN方法。</p><h3 id="MIDI-File-Format-and-Chord-Dataset"><a href="#MIDI-File-Format-and-Chord-Dataset" class="headerlink" title="MIDI File Format and Chord Dataset"></a>MIDI File Format and Chord Dataset</h3><p>MIDI(Musical Instrucent Digital Interface)<br>乐器数字接口文件<br>MIDI文件的内容主要包括所演奏的音符，音符的持续时间以及每个音符所需的响度<br>一般MIDI文件中的音符编号范围从1到128，每个数字代表一个标准化的声音，可以用来在MIDI作曲中创建旋律。对于本研究中的MIDI合成，我们使用了一个开放的Pokemon MIDI数据集，其中包含来自的307个真实MIDI文件。然后使用music21工具包分析该数据集并生成一个和弦数据集，表示为Π。</p><script type="math/tex; mode=display">\Pi =\{(1,N{t_1}),\dots,(n,Nt_n)\}</script><p>其中，i表示和弦的索引，n是总的和弦数，$Nt_i$ 表示和弦i中并发音符的集合<br>为了更方便的合成，每个和弦的持续时间统一设置为t秒</p><p>将这个索引和和弦之间的映射关系作为密钥，只要映射关系没泄露，隐写系统就是安全的<br><img src="/2024/07/08/%E9%9F%B3%E4%B9%90%E7%94%9F%E6%88%90%E9%9A%90%E5%86%99/image-20240804212401013.png" alt=""></p><h3 id="纠错码"><a href="#纠错码" class="headerlink" title="纠错码"></a>纠错码</h3><p>为了提高恢复的准确性，我们实现了特定的(3,1)重复码来对密电进行编码和纠错。这种ECC原理包括在传输前将每个数据位增加三倍，使提取过程中出现的错误可以通过复制的数据来纠正。</p><h3 id="消息映射和反映射"><a href="#消息映射和反映射" class="headerlink" title="消息映射和反映射"></a>消息映射和反映射</h3><p>感觉公式写错了<br>看了原论文，就是按照嵌入的比特数$\sigma$，将区间 $[-1,1]$ 划分为 $2^{\sigma -1}$ 个小区间，然后小区间有间隔以免产生误差<br><img src="/2024/07/08/%E9%9F%B3%E4%B9%90%E7%94%9F%E6%88%90%E9%9A%90%E5%86%99/image-20240804200024451.png" alt=""><br>就是均匀分布到高斯分布的那个东西</p><h3 id="MIDI-GAN"><a href="#MIDI-GAN" class="headerlink" title="MIDI-GAN"></a>MIDI-GAN</h3><p><img src="/2024/07/08/%E9%9F%B3%E4%B9%90%E7%94%9F%E6%88%90%E9%9A%90%E5%86%99/image-20240804200550007.png" alt=""><br>生成器是一个CNN网络<br><img src="/2024/07/08/%E9%9F%B3%E4%B9%90%E7%94%9F%E6%88%90%E9%9A%90%E5%86%99/image-20240804210744927.png" alt=""><br>把嵌入映射$F_1$，映射到一个新的$F_2$<br>将$F_2$归一化到整数区间<br><img src="/2024/07/08/%E9%9F%B3%E4%B9%90%E7%94%9F%E6%88%90%E9%9A%90%E5%86%99/image-20240804211021678.png" alt=""><br>提取的时候，用类似的方法将整数转换为浮点数</p><p>损失函数：<br>提取器的MSE损失<br>判别器为损失为输出的期望的差值加上线性组合的输出的梯度<br><img src="/2024/07/08/%E9%9F%B3%E4%B9%90%E7%94%9F%E6%88%90%E9%9A%90%E5%86%99/image-20240804213050475.png" alt=""><br>梯度保持策略？</p><h2 id="实验-1"><a href="#实验-1" class="headerlink" title="实验"></a>实验</h2><h3 id="指标"><a href="#指标" class="headerlink" title="指标"></a>指标</h3><p>MOS：平均主观得分<br>BER：比特错误率，衡量鲁棒性<br>PSNR：峰值信噪比<br>能量距离和Wasserstein距离，上一篇分布保持的论文里讲过，衡量分布距离的<br>召回率用于衡量抗检测<br><img src="/2024/07/08/%E9%9F%B3%E4%B9%90%E7%94%9F%E6%88%90%E9%9A%90%E5%86%99/image-20240804213750657.png" alt=""><br>主观得分，重复，声调跨度<br>重复衡量的是曲子的结构是否合理<br><img src="/2024/07/08/%E9%9F%B3%E4%B9%90%E7%94%9F%E6%88%90%E9%9A%90%E5%86%99/image-20240804214416238.png" alt=""><br>对几个损失的消融实验</p><p>分布保持的性能<br><img src="/2024/07/08/%E9%9F%B3%E4%B9%90%E7%94%9F%E6%88%90%E9%9A%90%E5%86%99/image-20240804214545475.png" alt=""><br><img src="/2024/07/08/%E9%9F%B3%E4%B9%90%E7%94%9F%E6%88%90%E9%9A%90%E5%86%99/image-20240804214609505.png" alt=""><br>使用三种不同的神经网络隐写分析器的平均召回率</p><h1 id="High-Fidelity-Text-Guided-Music-Generation-and-Editing-via-Single-Stage-Flow-Matching"><a href="#High-Fidelity-Text-Guided-Music-Generation-and-Editing-via-Single-Stage-Flow-Matching" class="headerlink" title="High Fidelity Text-Guided Music Generation and Editing via Single-Stage Flow Matching"></a>High Fidelity Text-Guided Music Generation and Editing via Single-Stage Flow Matching</h1><p><img src="/2024/07/08/%E9%9F%B3%E4%B9%90%E7%94%9F%E6%88%90%E9%9A%90%E5%86%99/image-20240812091107379.png" alt=""><br>是一个简单高效的可控高保真音乐生成与编辑模型</p><p>贡献：</p><ul><li>高效的单阶段文本到音乐的Flow Matching model，在音频潜在特征表示和生成模型上都有增强</li><li>提出了一种新的FM反向过程，能够完成zero-shot test-time text-guided editing</li></ul><h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><p>MelodyFlow is a non-causal transformer-based flow matching model conditioned on T5 text representations.<br>Non-Causal：非因果（non-causal）指的是模型在处理数据时不考虑时间顺序，也就是说，模型可以同时访问序列中的所有位置，而不必按照时间步骤顺序逐个处理。</p><h3 id="latent-audio-representation"><a href="#latent-audio-representation" class="headerlink" title="latent audio representation"></a>latent audio representation</h3><p>latent 是通过卷积自编码器提取的，这里用的是词是latent bottleneck representations，意思应该是在训练过程中引入了信息瓶颈相关的损失约束特征的熵</p><h3 id="Conditional-flow-matching-model"><a href="#Conditional-flow-matching-model" class="headerlink" title="Conditional flow matching model"></a>Conditional flow matching model</h3><p>给定音频 $a\in R^{D\times f_s}$ ，从中提取一个潜在表示 $x\in R^{L\times d}$<br>Flow matching模型通过一个线性变换，将序列 $\epsilon \in R^{L\times d}$ 映射到 x，</p><script type="math/tex; mode=display">z_t = tx +(1-t)\epsilon,t\in[0,1]</script><p>这是训练过程，t是从 这个区间中随机采样的，等于0时，$z_0=\epsilon$ ，就是初始状态的随机噪声，$z_1=x$ 就是最后要生成的样本<br>看起来很扩散模型<br>生成过程就是扩散步骤的ODE求解过程</p><h3 id="Text-guided-editing-through-latent-inversion"><a href="#Text-guided-editing-through-latent-inversion" class="headerlink" title="Text-guided editing through latent inversion"></a>Text-guided editing through latent inversion</h3><p>就是执行t步扩散，然后用新的prompt去噪进行风格转换</p><h1 id="MuseFlow-music-accompaniment-generation-based-on-flow"><a href="#MuseFlow-music-accompaniment-generation-based-on-flow" class="headerlink" title="MuseFlow: music accompaniment generation based on flow"></a>MuseFlow: music accompaniment generation based on flow</h1><p>这是真的flow模型<br>根据输入的钢琴旋律生成包括鼓，吉他，贝斯和弦乐在内的音乐伴奏</p><h2 id="proposed-model"><a href="#proposed-model" class="headerlink" title="proposed model"></a>proposed model</h2><h3 id="data-representation"><a href="#data-representation" class="headerlink" title="data representation"></a>data representation</h3><p>矩阵表示，横坐标表示事件，纵坐标表示音高，数值表示速度<br>使用8比特标识每个音符，并定义了额外三种状态<br><img src="/2024/07/08/%E9%9F%B3%E4%B9%90%E7%94%9F%E6%88%90%E9%9A%90%E5%86%99/image-20240812151949212.png" alt=""><br>描述的很模糊</p><h3 id="MuseFlow"><a href="#MuseFlow" class="headerlink" title="MuseFlow"></a>MuseFlow</h3><p><img src="/2024/07/08/%E9%9F%B3%E4%B9%90%E7%94%9F%E6%88%90%E9%9A%90%E5%86%99/image-20240812152144192.png" alt=""><br>前向训练过程将各个音轨的信息映射到多维高斯分布空间中，然后反向解码过程就是正向的逆过程</p><h4 id="Encoders"><a href="#Encoders" class="headerlink" title="Encoders"></a>Encoders</h4><p>使用多个相同结构不同参数的encoder编码不同音轨的特征<br>如图，五种乐器，五个编码器<br>钢琴轨道作为其他音轨的引导旋律，伴奏轨道转换到高斯分布中<br>编码器的输出Z，得到所有的五个轨道的信息<br>相较于Flow，有两个提升</p><ol><li>通过去除不同encoder之间常量交换的需求来简化结构</li><li>MuseFlow通过同时处理所有四个伴奏轨道来实现多个通道的并行映射</li></ol><p>forward process<br><img src="/2024/07/08/%E9%9F%B3%E4%B9%90%E7%94%9F%E6%88%90%E9%9A%90%E5%86%99/image-20240812153919661.png" alt=""></p><h4 id="Generators"><a href="#Generators" class="headerlink" title="Generators"></a>Generators</h4><p>输入钢琴音轨 $x_{piano}$ 和从高斯分布中采样的其他音轨的随机向量 $z_{n,other}$ ，和要生成的音轨大小相同。<br>先将 $x_{piano}$ 使用Bi-GRU生成引导用的特征向量 $An(x_{piano})$<br><img src="/2024/07/08/%E9%9F%B3%E4%B9%90%E7%94%9F%E6%88%90%E9%9A%90%E5%86%99/image-20240812155236969.png" alt=""><br>类似扩散逆过程的条件，直接拼接</p><h4 id="Sliding-generation"><a href="#Sliding-generation" class="headerlink" title="Sliding generation"></a>Sliding generation</h4><p>将滑动窗口的大小设置为8个小节，步长为4个小节<br>使用Bi-GRU作为模型主干，没记错的话有个部分是专门处理前序特征的叠加特征，遗忘比较慢<br><img src="/2024/07/08/%E9%9F%B3%E4%B9%90%E7%94%9F%E6%88%90%E9%9A%90%E5%86%99/image-20240812160717638.png" alt=""></p><h1 id="Gaussian-Shading-Provable-Performance-Lossless-Image-Watermarking-for-Diffusion-Models"><a href="#Gaussian-Shading-Provable-Performance-Lossless-Image-Watermarking-for-Diffusion-Models" class="headerlink" title="Gaussian Shading: Provable Performance-Lossless Image Watermarking for Diffusion Models"></a>Gaussian Shading: Provable Performance-Lossless Image Watermarking for Diffusion Models</h1><p><img src="/2024/07/08/%E9%9F%B3%E4%B9%90%E7%94%9F%E6%88%90%E9%9A%90%E5%86%99/image-20240923102443931.png" alt=""><br>CVPR2024</p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>现有扩散模型上的水印，主要有以下几类<br><img src="/2024/07/08/%E9%9F%B3%E4%B9%90%E7%94%9F%E6%88%90%E9%9A%90%E5%86%99/image-20240923102945873.png" alt=""><br>很直观，a, b两种方法都会影响图像的质量（一个在cover修改，一个在VAE decoder修改）<br>c方法会限制随机高斯变量的采样随机性</p><p>本文提出的方法叫 Gaussian Shading，包括三个基本要素：watermark diffuse, randomization, and distribution-preserving sampling。<br>就是把均匀分布到高斯分布的那个搬到这里来了。</p><h2 id="Methods"><a href="#Methods" class="headerlink" title="Methods"></a>Methods</h2><p><img src="/2024/07/08/%E9%9F%B3%E4%B9%90%E7%94%9F%E6%88%90%E9%9A%90%E5%86%99/image-20240923105353775.png" alt=""></p><h3 id="Embedding"><a href="#Embedding" class="headerlink" title="Embedding"></a>Embedding</h3><p>潜在表示$Z$ 的大小为 $c\times h \times w$ 每个维度可以表示 $l$ 比特的水印<br>将水印从整个隐空间大小中缩放，然后重复，以保证鲁棒性<br><strong>watermark diffuse</strong><br>就是这个缩放重复的过程，取 $\frac{1}{f_{hw}}$ 的潜在表示的高度和宽度，$\frac{1}{f_c}$ 的通道数，用于水印，然后复制$f_c \cdot f^2_{hw}$ 次<br>（问题：那这个随机潜在表示就不符合高斯分布了）<br>（答：用随机密钥加密了）</p><p><strong>watermark randomization</strong><br>使用随机二进制流密码对上面得到水印信息异或，得到随机比特流 $m$</p><p><strong>Distribution-preserving sampling driven by randomized watermark</strong><br>均匀分布映射到高斯分布<br>上面的randomization得到的是均匀分布的水印信息，现在需要将其映射到高斯潜在表示<br>潜在表示的每个维度表示 $l$ 比特的水印信息<br>就把高斯分布分割成 $2^l -1$ 个小区间，然后在 $l$ 表示的整数对应的区间中进行拒绝采样<br>这个区间是根据分位点计算 $[ppf(\frac{i}{2^l}),ppf(\frac{i+1}{2^l})]$，具体原理可以看StegoDDPM，写的更清晰的<br>但是拒绝采样耗时长，采用累积概率密度计算<br><img src="/2024/07/08/%E9%9F%B3%E4%B9%90%E7%94%9F%E6%88%90%E9%9A%90%E5%86%99/image-20240923115408908.png" alt=""><br>这个分布是0到1的均匀分布$u$，采样一个点然后移项：</p><script type="math/tex; mode=display">\begin{aligned}cdf(z^s_T) &= \frac{u+i}{2^l} \newlinez^s_T &= ppf(cdf(z^s_T)) \end{aligned}</script><p>上面这俩公式就是对一部分水印比特对应的整数 $i$，采样得到的隐空间表示<br>提取过程为</p><script type="math/tex; mode=display">i=\lfloor2^l \cdot cdf(z^s_T) \rfloor</script><p><strong>Image generation</strong><br>和正常过程没有区别，去噪的几个过程，DPMSolver，DDIM，ODESolver都可以，最后得到 $z^s_0$<br>然后进decoder得到最后的图片 $X^s = D(z^s_0)$</p><h3 id="Extraction"><a href="#Extraction" class="headerlink" title="Extraction"></a>Extraction</h3><p>使用 SD 的encoder $E$ ，从有损图片$X’^s$ 中得到隐空间表示 $z’^s_0 = E(X’^s)$ 然后引入DDIM反向过程，来估计添加的噪声，得到 $z’_T$<br>并且这个反向过程可以应用于其他的连续时间采样器</p><p>然后根据上面的公式进行提取，对于复制了 $f_c \cdot f_{hw}^2$ 次的水印，对于每个比特计算0，1出现次数，取出现最多的</p><h1 id="On-Exact-Inversion-of-DPM-Solvers"><a href="#On-Exact-Inversion-of-DPM-Solvers" class="headerlink" title="On Exact Inversion of DPM-Solvers"></a>On Exact Inversion of DPM-Solvers</h1><p>CVPR2024<br><img src="/2024/07/08/%E9%9F%B3%E4%B9%90%E7%94%9F%E6%88%90%E9%9A%90%E5%86%99/image-20241021190826606.png" alt=""><br>DPM求解器已经显著降低了延迟并提高了质量，但在找到确切的逆方面存在困难。<br>本文对dpm求解器的每个显示去噪step，使用隐式方法（如梯度下降或forward step）来确保<code>large classifier-free guidance</code>的鲁棒性，这和以前使用定点迭代的方法不同</p><h2 id="Introduction-1"><a href="#Introduction-1" class="headerlink" title="Introduction"></a>Introduction</h2><p>高阶DPM求解器可以使用更少的步数去完成去噪过程，但是追踪反向过程很难。<br>DDIM Inversion能行，是因为假设：<code>预测的噪声在时间步t和t+dt上是几乎相同的</code><br>但是对于更少去噪步骤的dpm求解器，此假设不成立。</p><p>本文研究了一阶DPM求解器DDIM和更快的高阶DPM求解器的准确反向。<br>对于使用前向欧拉方法的标准DDIM，使用后向欧拉方法作为精准inversion<br>对于使用线性多步法的高阶DPM求解器，提出使用近似高阶项的后向欧拉法</p><h2 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h2><p>只能说之前积累少了，这里用到了扩散模型的统一SDE和ODE<br>牛逼，完全看不懂</p><h2 id="Proposed-Method"><a href="#Proposed-Method" class="headerlink" title="Proposed Method"></a>Proposed Method</h2>]]></content>
      
      
      <categories>
          
          <category> 论文笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 隐写 </tag>
            
            <tag> 生成 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>对LDM代码的学习</title>
      <link href="/2024/05/15/%E5%AF%B9LDM%E4%BB%A3%E7%A0%81%E7%9A%84%E5%AD%A6%E4%B9%A0/"/>
      <url>/2024/05/15/%E5%AF%B9LDM%E4%BB%A3%E7%A0%81%E7%9A%84%E5%AD%A6%E4%B9%A0/</url>
      
        <content type="html"><![CDATA[<h1 id="对LDM代码的学习"><a href="#对LDM代码的学习" class="headerlink" title="对LDM代码的学习"></a>对LDM代码的学习</h1><p>看了下ControlNet，感觉要完全掌握这个技术，需要对扩散模型的代码十分熟悉，所以打算再集中研究一下Make-an-Audio的训练过程，然后看看如何在这个的基础上去进行下一步工作。</p><h2 id="Make-an-audio代码"><a href="#Make-an-audio代码" class="headerlink" title="Make an audio代码"></a>Make an audio代码</h2><p>开源代码中把VAE也训练了，就调试一下Diffusion的训练过程<br>模型的生成训练目标是<code>ldm.models.diffusion.ddpm_audio.LatentDiffusion_audio</code><br>该文件下还有其他两个类 <code>LatentFinetuneDiffusion, LatentInpaintDiffusion</code><br>注释里写道，前者是<code>Basis for different finetunes, such as inpainting or depth2image</code><br>先不管，反正是重建的做Inpainting的工作，先把正经生成搞懂</p><p>这篇文章真的是神中神 <a href="https://blog.csdn.net/yusijinfs/article/details/134684608">链接</a></p><h3 id="LatentDiffusion-audio类"><a href="#LatentDiffusion-audio类" class="headerlink" title="LatentDiffusion_audio类"></a>LatentDiffusion_audio类</h3><p>这里的<code>on_train_batch_start</code><br>用到了一个<code>std-rescaling</code><br>看的卵痛<br>于是回来复习Diffusion的数学知识了</p><p>准确来说，加噪声并不是给上一时刻的图像加上噪声值，而是从一个均值与上一时刻图像相关的正态分布里采样出一幅新图像。<br>然后对于分布采样的问题，考虑简单正态分布，有下面的推导</p><script type="math/tex; mode=display">\begin{aligned}x &\sim \mathcal{N}(\mu,\sigma^2) \newlinex_T &\sim \mathcal{N}(0,1) \newlinex &= \mu \cdot x_T + \sigma\end{aligned}</script><p>推广到扩散模型的加噪过程，首先有一个噪声程度的schedule，$\beta_t$ ，对于$x_t$，可以从以下分布中采样</p><script type="math/tex; mode=display">x_t\sim \mathcal{N} (\sqrt{1-\beta_t}x_{t-1},\beta_t I)</script><p>按照上面的推导，可以将$x_t$写成下面的形式</p><script type="math/tex; mode=display">x_t=\sqrt{1-\beta_t}x_{t-1}+\sqrt{\beta_t}\epsilon_{t-1};\epsilon_{t-1}\sim \mathcal{N}(0,I)</script><p><img src="/2024/05/15/%E5%AF%B9LDM%E4%BB%A3%E7%A0%81%E7%9A%84%E5%AD%A6%E4%B9%A0/image-20240517160949141.png" alt=""></p><h2 id="ControlNet代码"><a href="#ControlNet代码" class="headerlink" title="ControlNet代码"></a>ControlNet代码</h2><p>按照文档，跑通了fill50k的训练<br>先看配置文件<code>cldm_v15.yaml</code><br>和普通的VAE+LDM配置文件不同<br>模型有下面这种包含关系<br></p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">|--&gt; ControlNet</span><br><span class="line">ControlLDM--+</span><br><span class="line">|--&gt; ControlledUnetModel</span><br></pre></td></tr></tbody></table></figure><br>然后还有VAE和CLIP就不放进来了<br>就是多了一个<code>control_stage_config</code>键<p></p><p>其中<code>ControlLDM</code>是继承<code>LatentDiffusion</code>的，和Make-an-Audio中的<code>LatentDiffusion_audio</code>类似</p><h3 id="ControlLDM"><a href="#ControlLDM" class="headerlink" title="ControlLDM"></a>ControlLDM</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, control_stage_config, control_key, only_mid_control, *args, **kwargs</span>):</span><br><span class="line"><span class="built_in">super</span>().__init__(*args, **kwargs)</span><br><span class="line">self.control_model = instantiate_from_config(control_stage_config)</span><br><span class="line">self.control_key = control_key</span><br><span class="line">self.only_mid_control = only_mid_control</span><br><span class="line">self.control_scales = [<span class="number">1.0</span>] * <span class="number">13</span></span><br></pre></td></tr></tbody></table></figure><p>沟槽的图像，LDM可以直接用，感觉初始化好简洁<br>这几行就是处理添加的ControlNet</p><p>对于可学习mask的思考<br>在controlnet的条件信息部分，加上一个什么AudioMAE特征，看看是否是帧级别的，然后和帧级01mask乘一下</p><h1 id="AudioControlNet"><a href="#AudioControlNet" class="headerlink" title="AudioControlNet"></a>AudioControlNet</h1><p>先列个文件结构<br></p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">-data</span><br><span class="line">-cldm</span><br><span class="line">cldm.py</span><br><span class="line">ddim_hacked.py</span><br><span class="line">-ldm</span><br><span class="line">-models</span><br><span class="line">config.yaml</span><br><span class="line">pretrained.ckpt</span><br><span class="line">control.ckpt</span><br><span class="line">train.py</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure><br>预训练模型有了<br>写完还得加上controlnet的参数<p></p><h1 id="Have-Done"><a href="#Have-Done" class="headerlink" title="Have Done"></a>Have Done</h1><p>给预训练模型添加了ControlNet部分的参数<br>数据预处理</p><h1 id="TODO-list"><a href="#TODO-list" class="headerlink" title="TODO list"></a>TODO list</h1><ul><li>[ ] 训练时注意scheduler，MAA里自己写了一个，评估一下要不要用</li><li>[x] 简单main函数先测试是否能运行</li><li>[x] scale_factor是什么看看</li><li>[x] 先简易实现cal_control</li><li>[x] 把logger的每个环节的采样数量保持一致</li><li>[x] 在control信息保存的图片中加入control的起始和终止时间窗口</li><li>[ ] 把control的图片和sample出来的mel拼在一起</li></ul><h1 id="Important-Information"><a href="#Important-Information" class="headerlink" title="Important Information"></a>Important Information</h1><p>对于优化参数的处理，在 <code>ControlLDM.configure_optimizers</code> 里<br>该函数只将要优化的参数加入优化器</p><p>LDM同时接收两个控制信息，分别是text和control<br>text在Unet中通过cross_attn的方式传入<br>control通过concat的方式传入</p><h1 id="Problem"><a href="#Problem" class="headerlink" title="Problem"></a>Problem</h1><p>scale_factor变了<br>register_buffer会携带在模型参数文件中，导入之后就变了<br>已解决</p><p>关于cal_control的实现<br>首先是起止时间，由于输入音频都是 10s ，可以直接代换到梅尔谱大小的mask，这是01mask的思路，缺点是没有标签语义</p><p>ControlNet中的<code>input_hint_block</code>用不了，得自己重新写，因为要和UNet Encoder第一层的输出相加，所以得对上形状<br>h.shape=10,78<br>已解决</p><p>如何将标签语义导入？<br>需要找一个办法，使文本特征转换到和梅尔谱特征具有空间相关性的形状</p><p>没有工作是做声音事件时长预测的<br>那就先不考虑时长预测，毕竟感觉确实做不了，比如说话声，搞不了我具体说多长<br>那问题就变为，预测给定事件和时间，声音事件在这段事件内的发展规律<br>首先要得到文本的特征表示，CLAP的特征好长<br>对于不同事件的表示，如何有效组合（写个简易的self-attention<br>懂了<br>搞个帧级的音频特征提取器，作为这个特征的训练目标</p><p>想到了AudioMAE，但是和文本没有关联。<br>在AudioLDM2中，是训练GPT2生成的特征和AudioMAE特征相关联。</p><p>采样也有问题，每次训练需要写一个log，记录当前的信息<br>callback里， hook <code>on_train_batch_end</code> ，<br>看看MAA里的几个logger咋搞的</p><p>首先要实现ControlLDM中的log_image函数，加入control信息去采样<br>这个好了</p><p>把logger的每个环节的采样数量保持一致：<br>涉及到的函数<br><code>ImageLogger.on_train_batch_end</code><br>-&gt;<br><code>ImageLogger.log_img</code> 有一个 <code>self.max_images</code> 在控制<br>-&gt;<br><code>AudioLogger.log_local</code><br>-&gt;<br><code>AudioLogger._log_rec_audio</code><br>我想方便调试，就想每轮开始就采样一下，于是把模型换成上次训练的模型，并修改log_first_step为True<br>另外修改了log_step，见配置文件和logger.py文件中注释了debug的部分</p><p>发现实验结果可能是一个巨大的巧合<br>看看lr_schedule</p><p>无语，发现没把压缩部分的插件加入训练过程</p><p>对于ControlNet，Control信息对于原来的模型来说是和原图片相似的<br>12000step就有很好的效果</p><p>不是我判断的问题，mask是那个mask，音频也是那个音频<br>就是训练不一样了，而且生成的音频质量也变的好差</p><p>Music ControlNet从原音乐中提取特征，而且是音乐独有的特征，韵律等<br>这个melody可以在频谱图中反应出来，符合图像中的controlnet的合理性<br>但是rhythm和dynamic呢</p><blockquote><p>rhythm is the word we use to describe when a sound should be played and how long it should last. It’s the basic building block of all music.<br>Dynamics refer to how loudly or softly a piece of music is played. Words such as “piano” (soft) and “forte” (loud) are an indication of dynamics.</p></blockquote><p>韵律可以在梅尔谱图的出现时间模式上，比如每个beat的出现时间，小拍大拍，力度等<br>动态体现在声音的强弱变化上，比如波西米亚狂想曲妈妈咪呀那里，就是渐弱</p><p>对当前结果的猜测</p><ul><li>数据集的问题，好多强标签的持续时间只有零点几秒，要么就是覆盖整个音频段</li><li><del>对于零卷积层，参数可能并没有正确初始化</del></li><li>和图像控制相比，所谓的CLAP特征复制并没有体现出在梅尔谱图上的空间特征</li></ul><p>一个一个排查，首先是零卷积层问题<br>把原来制作的模型的参数打印出来看看<br>这个没问题</p><p>再来排除数据集的问题</p><p>先什么再什么，CLAP，</p><p>SEQ2seq-&gt; 文本到任意特征序列</p><p>找找别的强标签数据集<br>DESED: Real validation, Synthetic training, Real public evaluation,<br></p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># public evaluation</span></span><br><span class="line">https://zenodo.org/api/records/<span class="number">3588172</span>/files-archive</span><br><span class="line"><span class="comment"># Synthetic</span></span><br><span class="line">https://zenodo.org/api/records/<span class="number">4307908</span>/files-archive</span><br></pre></td></tr></tbody></table></figure><br>real validation是Audioset的子集<p></p><p>MAESTRO<br>还有这个task的数据集<br><a href="https://dcase.community/challenge2024/task-audio-and-audiovisual-sound-event-localization-and-detection-with-source-distance-estimation">https://dcase.community/challenge2024/task-audio-and-audiovisual-sound-event-localization-and-detection-with-source-distance-estimation</a></p><p>使用清洗过的精简的数据集也没办法实现控制，沟槽的<br>感觉是不是因为损失的问题，这也没有损失来约束controlnet部分的参数<br>沟槽的主包漏掉了一个很重要的东西<br>随机丢弃文本prompt<br>现在加上这个去训练，如果没用的话尝试不用单条标注的control，而是直接加入一整个样本，并且完全标注<br>没用喵😭</p><p>换desed的数据集，并且组装一下一个样本的所有tag</p><p>感觉也需要预训练微调，在带caption的数据上训练几轮，然后放到无caption的纯强标签数据集上训练</p><p>牛魔，组装tag的写完发现dataloader不认，要全部数据涉及到的list都等长<br>然后最长的的tag数为 22， 我还想说加点空的把所有都对齐来着<br>现在的想法是只传路径和yid之类的，然后根据yid搞一个字典，字典哈希表查着快，把tag放在这个表里</p><p>log_txt_as_img也要改<br>把每个小标签都列出来，就是tag是帧级的，乘窗长采样率转换为时间再标出来</p><p>等这个写完了，加上对未标记部分的原数据的mask，感觉这样会有点说法</p><p>切时间的算法没写好，对于覆盖整段音频的标记，没有加入重叠部分<br>用以下方法重写：</p><ul><li>所有事件处理为(起始帧，终止帧，事件标签)的形式</li><li>遍历625帧，判断事件窗口是否包含，这里单独写一个list保存未标记帧，方便之后写原样本mask</li><li>对每一帧单独计算一个latent？（ 时间复杂度太高了，感觉不太行，要处理一下</li></ul><p>思路：<br>对于每个文件，会有一个事件组，每个事件有一个序号<br>维护一个当前事件序号，在事件序号发生变化时，记录一组数据 <code>(起始帧，终止帧，包含事件的标签组合)</code><br>上面这个写好了</p><p>搞另外两个数据集<br>/data1/yyp/tmpfile/DESED_synthetic/metadata/train/synthetic20</p><p>train_folder 是很多jams的<code>/data1/yyp/tmpfile/DESED_synthetic/audio/train/synthetic20/soundscapes</code></p><p>在看FoleyCrafter的时候想到一个东西<br>把所有强标签样本聚合起来，对于有caption的部分，对于原音频就不做mask，对于没有caption，只有强标签的部分，就把强标签没有覆盖到的时间段的原音频mask掉</p><p>又想到一个东西，既然我已经text only了，有没有一种可能，输入一段有声书，输出带环境音的版本<br>细想一下，好像没什么可做的</p><p>还是看看立体声吧，做出来了一定要蹭一下空间音频的热度</p>]]></content>
      
      
      
        <tags>
            
            <tag> Diffusion </tag>
            
            <tag> 深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>生成相关</title>
      <link href="/2024/04/08/%E7%94%9F%E6%88%90%E7%9B%B8%E5%85%B3/"/>
      <url>/2024/04/08/%E7%94%9F%E6%88%90%E7%9B%B8%E5%85%B3/</url>
      
        <content type="html"><![CDATA[<h1 id="生成相关"><a href="#生成相关" class="headerlink" title="生成相关"></a>生成相关</h1><h2 id="Audiogen"><a href="#Audiogen" class="headerlink" title="Audiogen"></a>Audiogen</h2><p>ICLR 2023<br>Meta AI<br>包括两个主要的阶段<br>第一阶段将原始音频编码成离散的token序列，通过一个压缩模型进行<br>该模型以端到端的方式进行训练，使用压缩表示重建输入音频，并以一组鉴别器的形式添加感知损失。<br>第二阶段使用一个自回归的Transformer-decoder language-model，在文本条件的基础上重建音频序列</p><p>主要贡献：</p><ol><li>sota方法</li><li>提高TTA生成性能的两个方法：classifier free guidance，动态文本和音频混合来提高组合性</li><li>可以做条件和非条件的音频延续</li><li>探索了音频保真度和采样时间之间的关系<h3 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h3><img src="/2024/04/08/%E7%94%9F%E6%88%90%E7%9B%B8%E5%85%B3/image-20240411103419341.png" alt=""><h4 id="Audio-representation"><a href="#Audio-representation" class="headerlink" title="Audio representation"></a>Audio representation</h4>一个时长为d的音频信号可以表示为一个序列 $x\in[-1,1]^{C_a \times T}$ ，$C_a$ 是通道数，$T=d\cdot f_{sr}$ 是采样点数，至于为什么是 $[-1,1]$ ，是因为py库读取wav文件会自动归一化<br>Audio representation model 包含三个部分</li></ol><ul><li>encoder network E：将音频片段作为输入，输出一个latent representation $z$ </li><li>quantization layer Q：使用Vector Quantizaiton将 $z$ 压缩为 $z_q$</li><li>decoder net work G：从压缩表示 $z_q$ 中重建时域信号 $\hat x$<br>整个系统端到端进行训练，损失包括时域和频域上的重建损失，以及不同时间分辨率的感知损失，这一部分通过多个鉴别器来实现。</li></ul><p>训练目标：<br>时域重建损失：$\ell_t(x,\hat x)=\Vert x-\hat x\Vert_1$<br>频域重建损失：</p><script type="math/tex; mode=display">\ell_f(x,\hat x)=\frac{1}{|\alpha|\cdot|s|}\sum_{\alpha_i\in \alpha}\sum_{i\in e}\Vert S_i(x)-S_i(\hat x) \Vert_1 +\alpha_i\Vert S_i(x)-S_i(\hat x) \Vert_2</script><p>其中 $S_i$ 是根据 $i$ 改变的不同窗长和步长的64-bins梅尔谱图，系数 $\alpha_i$ 用于平衡两项损失，但是这里设置的是1<br>感知损失：<br>使用一个multi-scale STFT-based discriminator来捕获声音信号中的不同结构。<br>判别器的结构是一样的，但是时间分辨率不同，对抗损失如下</p><script type="math/tex; mode=display">\ell_g(\hat x)=\frac{1}{K}\sum_k \max(0,1-D_k(\hat x))</script><p>根据公式，这个 $D_k(\hat x)$ 输出的应该是为真实样本的概率<br>还有特征匹配损失（这个在VITS中也出现了），L是判别器的层数</p><script type="math/tex; mode=display">\ell_{feat}(x,\hat x)=\frac{1}{KL}\sum^K_{k=1}\sum^L_{l=1}\Vert D^l_k(x)-D^l_k(\hat x) \Vert_1</script><p>判别器的总损失还加了一个部分，总的如下</p><script type="math/tex; mode=display">L_d(x, \hat x)=\frac{1}{K}\sum^K_{k=1}\max(0,1-D_k(x))+\max(0,1+D_k(\hat x))</script><p>对抗损失的两部分，对真样本 $x$ 输出要大，假的输出要小，因为是损失，所以二者都取负，得到的就是上面这种形式。<br><strong>Audio language modeling</strong><br>看不懂，什么卵<br>给定一个文本输入 $C$ ，Audio Language Model（ALM）组件输出一个音频token序列 $\hat z_q$ 然后在其上面做音频重建<br>给定如下：<br>一个文本编码器F，将原始文本输出投射成一个语义稠密表示（semantic dense representation）$F(c)=u$<br>一个查阅表（Look-Up-Table, LUT）将音频token $\hat z_q$ 嵌入一个连续的空间，$LUT(\hat z_q)=v$<br>然后将 $u,v$ 连接，得到 $Z=u_1,\dots,u_{T_u},v_1,\dots,v_{T_v}$<br>然后使用上面这个表示 $Z$ ，训练一个Transformer-decoder language-model，使用如下损失函数</p><script type="math/tex; mode=display">L_{LM}=-\sum^N_{i=1}\sum^{T_v}_{j=1}\log p_{\theta}(v^i_j|u_1^1,\dots,u^i_{T_u},v_1^1,\dots,v^i_{j-1})</script><p>两个连乘，N是样本数吗？然后下标是音频嵌入连续空间序列<br>突然懂了<br>首先这是一个Transformer序列预测器，目标是给定文本，得到音频token序列，上面损失函数是一个条件熵的形式，对于一条样本，最开始的条件是文本表示 $u$ ，然后依次预测下一个音频token，每次预测计算一次条件熵。<br><strong>长序列预测问题</strong><br>本方法对原始音频下采样32倍，让每个audiotoken关联2ms，这会导致很长的序列，因为每秒就需要500个token。<br>使用Multi-stream audio inputs的方法缓解这个问题<br>看不懂，看代码的时候再研究研究</p><h4 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h4><p>一些评估指标<br>Frechet Audio Distance：FAD，是一种与人类感知密切相关的无参考评估指标<br>还评估了KL散度和主观的标准<br>做了音频续写的实验，使用的条件包括音频片段，音频相关文本，音频无关文本</p><h4 id="Limitations"><a href="#Limitations" class="headerlink" title="Limitations"></a>Limitations</h4><p>缺乏理解时间顺序的能力，经常生成莫名其妙的语音</p><h2 id="Diffsound"><a href="#Diffsound" class="headerlink" title="Diffsound"></a>Diffsound</h2><p>TASLP2023<br>北大, Tencent AI lab<br>Dongchao Yang, Jianwei Yu<br>传统的自回归token解码器，有两个问题</p><ul><li>梅尔谱token总是按顺序预测，这会限制模型的生成能力，因为有些声音的事件位置可能来自文本的两端</li><li>预测阶段，来自之前的错误的预测token会导致累计的预测错误<br>主要贡献：</li><li>第一次在音频生成任务中使用Diffusion</li><li>提出了一种基于掩码的文本生成策略（mask-based text generation strategy, MBTG），有助于在AudioSet数据集上构建大规模文本-音频数据集</li><li>提出了三个客观的评价指标 <h3 id="Proposed-text-to-sound-framework"><a href="#Proposed-text-to-sound-framework" class="headerlink" title="Proposed text-to-sound framework"></a>Proposed text-to-sound framework</h3><img src="/2024/04/08/%E7%94%9F%E6%88%90%E7%9B%B8%E5%85%B3/image-20240412150427637.png" alt=""><br><strong>Text Encoder</strong><br>有点搞，用的是BERT和CLIP的text encoder，但是效果更好，说明和图片的对比学习会让textencoder携带更多的语义信息<br><strong>Learning Discrete Latent Space of Mel-Spectrograms via VQ-VAE</strong><br><img src="/2024/04/08/%E7%94%9F%E6%88%90%E7%9B%B8%E5%85%B3/image-20240412152710502.png" alt=""><br>和TTS任务不同，TTA任务生成的音频与文本没有直接的对应关系<br>使用VQ-VAE来生成梅尔谱，问题就被转换成生成token序列<br>这里就是普通的VQ-VAE，不再详细记录<br><strong>Token-Decoder</strong><br>用于将文本特征变换到离散的梅尔谱图token序列，本文首先提出采用自回归的token-decoder<br>啥意思，前面说不用自回归，这里又用的自回归<br>说后面给了一个不同自回归的方法，等下看看<br>这里是用自回归预测前面VQ-VAE的encoder生成的梅尔token序列<br>在训练的时候，因为累计错误问题，使用<code>"teacher-forcing" strategy</code>，使用ground truth作为预测第 $i$ 个token时前 $i-1$ 个token条件<br><strong>Vocoder</strong><br>用的MelGAN，不懂<br>这还是自己训练的，在AS上训练的<h3 id="Diffusion-based-decoder"><a href="#Diffusion-based-decoder" class="headerlink" title="Diffusion-based decoder"></a>Diffusion-based decoder</h3>就是经典Diffusion，参考<a href="../_posts/AudioLDM.md">AudioLDM</a>中关于Diffusion的部分<br>这里给了一个好高大上的loss，叫负对数似然变分上界（variational upper bound on the negative log-likelihood）<script type="math/tex; mode=display">\mathcal{L}_{vb}=\mathbb{E}_{q(\boldsymbol{x}_0)}\left[D_{KL}[q(\boldsymbol{x}_t|\boldsymbol{x}_0)\Vert p(\boldsymbol{x}_t)] + \sum^T_{t=1}\mathbb{E}_{q(\boldsymbol{x}_t|\boldsymbol{x}_0)}\left[D_{KL}[q(\boldsymbol{x}_{t-1}|\boldsymbol{x}_t,\boldsymbol{x}_0)\Vertp_{\theta}(\boldsymbol{x}_{t-1}|\boldsymbol{x}_t)]\right]\right]</script>这一大坨狗屎是什么<br>首先，$q$ 是前向过程的分布，是加噪过程，$p$ 是不变的高斯分布，$p_\theta$ 是反向过程去噪的分布<br>第一个KL散度是加噪过程的损失，第二个是T步去噪过程的损失，这下懂了</li></ul><p><strong>Discrete Diffuison Model</strong><br>因为送入Decoder的是离散的整数，不能在这上面加噪声，引入一个转移概率矩阵，用于指导前向过程中 $\boldsymbol{x}_0$ 如何一步一步转换为 $\boldsymbol{x}_t$<br>定义为：</p><script type="math/tex; mode=display">[\boldsymbol{Q}_t]_{ij}=q(x_t=i|x_{t-1}=j) \in \mathbb{R}^{P\times P}</script><p>对整个序列的前向过程可以写为：</p><script type="math/tex; mode=display">q(X_t|X_{t-1})=\boldsymbol{c}^T(x_t)\boldsymbol{Q}_t \boldsymbol{c}(x_{t-1})</script><p>其中 $\boldsymbol{c}(\cdot)$ 是一个将标量元素转换成one-hot向量的函数<br>沟槽的后面好多，这里暂时解释不清，继续往后看<br>然后说按照马尔可夫链和贝叶斯公式，可以计算出 $q(x_t|x_0)$ 和 $q(x_{t-1}|x_t,x_0)$<br><strong>Non-Autoregressive Mel-Spectrograms Generation via Diffsound</strong><br>作者希望预测的结果可以同时获得，并利用扩散模型T步的迭代优化预测结果<br>但是这里的特征是离散的，这里提到，离散扩散模型训练的关键是设计一种合适的策略来预定义马尔可夫转移矩阵 $Q_t$<br>注意到上面用的是离散标签在这搞扩散，我直接一个问号<br>这是将离散标签one-hot之后类似一个分类过程？<br>发现one-hot之后就是一个单位向量啊，然后左乘右乘就是选择 $Q_t$ 中的一个元素嘛<br>定义了三种转移矩阵</p><ul><li>Uniform transition matrix</li><li>mask transition matrix</li><li>mask and uniform transition matrix<br>这个就是传统diffusion的扩展，不想看了<br><strong>Pre-Training Diffsound on AudioSet Dataset</strong><br>对于文本描述，采用的做法是在标签两边随机插入一到两个mask标记，以使模型主要关注于声音事件而不是文本（感觉不太靠谱<br>采用渐进式训练，从单标签开始训练，然后再多标签<h3 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h3>相比之前的文章，引入了一个Audio Caption Loss，是将生成的音频送入Audio caption模型，执行caption任务</li></ul><h2 id="Make-an-Audio"><a href="#Make-an-Audio" class="headerlink" title="Make an Audio"></a>Make an Audio</h2><p>ICML2023<br>浙大，北大，字节跳动<br>Rongjie Huang, Jiawei Huang, Dongchao Yang<br>引入了一种数据增强方法，解决音频文本对数据稀缺的问题<br>使用频谱自编码器预测自监督音频表示而不是波形<br>支持多模态输入生成音频<br><img src="/2024/04/08/%E7%94%9F%E6%88%90%E7%9B%B8%E5%85%B3/image-20240419092228304.png" alt=""></p><h3 id="Pseudo-Prompt-Enhancement-Distill-then-Reprogram"><a href="#Pseudo-Prompt-Enhancement-Distill-then-Reprogram" class="headerlink" title="Pseudo Prompt Enhancement: Distill-then-Reprogram"></a>Pseudo Prompt Enhancement: Distill-then-Reprogram</h3><p>这个方法包含两个阶段，一个expert distillation approach，使用audio caption方法，从无标签音频中得到文本描述，一个dynamic reprogramming procedure来构建不同的音频文本组合<br><img src="/2024/04/08/%E7%94%9F%E6%88%90%E7%9B%B8%E5%85%B3/image-20240419092220640.png" alt=""></p><p><strong>Expert Distillation</strong><br>使用两个预训练模型来做这个事情，分别是audio caption的工作和audio-text retrieval的工作，我记得两个都是DCASE的task<br>没看懂，这个audio-text retrieval的模型是拿来干嘛的？<br><strong>Dynamic reprogramming</strong><br>根据几个模板，把 $N\in \{0,1,2\}$ 个音频文本对使用预设的几个方法连接</p><h3 id="Textual-Representation"><a href="#Textual-Representation" class="headerlink" title="Textual Representation"></a>Textual Representation</h3><p>就说用了CLAP和T5-Large(一个大语言模型)，实验表明二者的结果相近，但是CLAP更高效，因为不需要offline computation</p><h3 id="Audio-Representation"><a href="#Audio-Representation" class="headerlink" title="Audio Representation"></a>Audio Representation</h3><p>经典的梅尔自编码器带判别器提高重建质量<br>这个除了重建损失和对抗损失还有一个KL散度惩罚损失</p><p>后面还有经典的介绍Diffusion和CLassifier-Free Guidance</p><h3 id="X-To-Audio-No-Modality-Left-Behind"><a href="#X-To-Audio-No-Modality-Left-Behind" class="headerlink" title="X-To-Audio: No Modality Left Behind"></a>X-To-Audio: No Modality Left Behind</h3><p>包括三个部分，text to audio, audio inpainting visual to audio(video or image)<br><img src="/2024/04/08/%E7%94%9F%E6%88%90%E7%9B%B8%E5%85%B3/image-20240419092200720.png" alt=""></p><h4 id="Personalized-Text-To-Audio-Generation"><a href="#Personalized-Text-To-Audio-Generation" class="headerlink" title="Personalized Text-To-Audio Generation"></a>Personalized Text-To-Audio Generation</h4><p>没搞懂他想干嘛<br>这里给了个例子，在给定打雷的声音时，让模型生成babycrying的音频，就会生成<code>a baby crying in the thunder day</code><br>大概就是通过文本描述修改音频，添加背景声，或者插入说话的主体之类的<br>做法是对给定的音频片段，选取一个特定的时间段$t_0$然后对其进行Diffuision的加噪去噪过程<br>这里提到音频的文本对其度和真实度之间随着T的变大有一个trade-off</p><h4 id="Audio-Inpainting"><a href="#Audio-Inpainting" class="headerlink" title="Audio Inpainting"></a>Audio Inpainting</h4><p>通过mask来实现Inpainting的训练，采用了两种mask方法<br>一个是<a href="Suvorov, R., Logacheva, E., Mashikhin, A., Remizova, A., Ashukha, A., Silvestrov, A., Kong, N., Goka, H., Park, K., and Lempitsky, V. Resolution-robust large mask inpainting with fourier convolutions. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pp. 2149–2159, 2022">LaMa</a>中的什么polygonal chains和任意宽高比的矩形mask（听起来像是在mel谱上做mask，因为原来的工作是图像上的<br><img src="/2024/04/08/%E7%94%9F%E6%88%90%E7%9B%B8%E5%85%B3/image-20240419092131181.png" alt=""><br>大概长这样<br>另一个是在语音相关文献中常用的帧级掩蔽策略</p><h4 id="Visual-To-Audio-Generation"><a href="#Visual-To-Audio-Generation" class="headerlink" title="Visual-To-Audio Generation"></a>Visual-To-Audio Generation</h4><p>用CLIP来抽取图像特征，用文本表示来弥合视觉和音频世界之间的模态差距，用一个或多个Flow模型将CLIP的特征向量空间映射到CLAP的特征向量空间<br>对于视频，抽取固定的4帧，池化得到4帧的平均表示，退化为图片转音频</p><h3 id="Training-and-Evaluation"><a href="#Training-and-Evaluation" class="headerlink" title="Training and Evaluation"></a>Training and Evaluation</h3><p>笑死用了18块V100<br>在常用的评估方法FID和KL散度之上，添加了CLAP分数来评估audio-text alignment<br>对比了Diffsound，AudioLDM和AudioGen<br><img src="/2024/04/08/%E7%94%9F%E6%88%90%E7%9B%B8%E5%85%B3/image-20240419170509432.png" alt=""></p><p>IS: inception score，在AudioLDM中，对他的解释是</p><blockquote><p>IS is effective in evaluating both sample quality and diversity.</p></blockquote><p>网上找的都说什么是用谷歌一个分类模型计算的，对于音频，需要专门的分类器 $C$<br>可以自己训练，计算公式如下</p><script type="math/tex; mode=display">IS(G)=\exp(\mathbb{E}_{z\sim P_g} D_{KL}(p(y|x)\Vert p(y)))</script><p>在AudioLDM中有讲，是用的PANNs分类器做的评估</p><p>消融实验中提到，对于文本特征提取，CLAP和T5表现差不多，但是CLAP性能高</p><h3 id="与AudioLDM2的对比"><a href="#与AudioLDM2的对比" class="headerlink" title="与AudioLDM2的对比"></a>与AudioLDM2的对比</h3><p>二者同为多模态内容生成音频的工作，从结构、特征工程、结果三个角度来看<br><strong>结构</strong><br>AudioLDM2</p><ul><li>使用AudioMAE编码音频特征</li><li>使用GPT-2生成其他模态特征</li><li>使用含Transformer块的U-net<br>Make-an-Audio</li><li>使用自己训练的梅尔自编码器结构网络编码音频特征</li><li>使用CLIP和CLAP编码其他模态特征</li><li>使用的普通的卷积U-net</li></ul><p><strong>特征工程</strong><br>AudioLDM2</p><ul><li>使用各模态的专家模型抽取模态特征 </li><li>使用GPT2生成音频大一统特征<br>Make-an-Audio</li><li>也是用的专家模型抽取模态特征</li><li>使用Flow映射分布到自编码器特征空间</li></ul><p><strong>结果</strong><br><img src="file:///C:/Users/89492/Desktop/%E5%91%A8%E6%8A%A5/2024-4-22%E9%84%A2%E6%B9%A7%E6%A3%9A%E5%91%A8%E6%8A%A5/image-20240422100319446.png?lastModify=1713926396" alt="image-20240422100319446"><br>AudioLDM使用的训练数据少很多，主观评价相对也更好<br>OVL：Overall quality<br>REL：relevance t the input text</p><h3 id="复现"><a href="#复现" class="headerlink" title="复现"></a>复现</h3><p>在Audiocaps数据集上进行了训练<br>总的训练时间大约为两天<br>VAE训练了35 epochs<br>Diffusion 训练了40 epochs<br><strong>VAE</strong><br>原始输入<br><img src="/2024/04/08/%E7%94%9F%E6%88%90%E7%9B%B8%E5%85%B3/image-20240426163758549.png" alt=""><br>VAE编解码重建，质量很高<br><img src="/2024/04/08/%E7%94%9F%E6%88%90%E7%9B%B8%E5%85%B3/image-20240426163837870.png" alt=""></p><p><strong>diffusion</strong><br>inputs: batch中的音频<br>reconstruction<br>diffusion_row: 随机噪声，经过Diffusion每200 step记录一次<br><img src="/2024/04/08/%E7%94%9F%E6%88%90%E7%9B%B8%E5%85%B3/image-20240426182907899.png" alt=""></p><p>sample<br><img src="/2024/04/08/%E7%94%9F%E6%88%90%E7%9B%B8%E5%85%B3/image-20240426182848923.png" alt=""><br><strong>指标</strong></p><div class="table-container"><table><thead><tr><th>模型</th><th>FD</th><th>FAD</th><th>IS</th></tr></thead><tbody><tr><td>作者给的</td><td>26.6</td><td>2.39</td><td>7.83</td></tr><tr><td>我训练的</td><td>38.9</td><td>6.43</td><td>5.32</td></tr></tbody></table></div><p>FD: 曲线相似性度量<br>FAD：人类感知指标<br>IS：评估采样质量和多样性</p><h2 id="Retrieval-Augmented-text-to-audio-generation"><a href="#Retrieval-Augmented-text-to-audio-generation" class="headerlink" title="Retrieval-Augmented text to audio generation"></a>Retrieval-Augmented text to audio generation</h2><p>ICASSP 2024<br>University of Surrey<br>Yi Yuan, Haohe Liu<br>提出的问题：由于TTA模型训练数据的类别不平衡问题，其生成性能存在偏差，这些模型可以为常见的声音事件生成真实的音频片段，但是当遇到不太频繁或看不见的声音事件时，他们可能会生成不正确或不相关的音频。作者把这个问题定义为 <code>long-tailed text-to-audio generation problem</code><br>本文提出了一个训练框架，用在作者之前AudioLDM的工作上，使用了Audio retrieval来增强训练时的样本丰富度。称之为 <code>Re-AudioLDM</code></p><blockquote><p>具体来说，我们首先使用输入文本提示从数据集(例如AudioCaps)中检索相关引用（比如音频文本对），然后使用预训练的音频模型和语言模型分别提取声学和文本特征。然后将这些提取的特征进一步交给LDM的cross attention模块，以指导生成过程。检索到的音频文本对作为补充信息，有助于改进训练阶段低频发生的音频事件的建模。在推理阶段，检索增强策略还提供了与文本提示相关的参考，确保了更加准确和忠实的音频生成结果。</p></blockquote><p>注意这几点：检索的做法，cross attention是在哪里，怎么指导的，和原来的训练方法有什么不同，推理时是怎么提供参考的<br>检索的做法：在Audiocaps中选k邻居<br>cross attention: 在Unet的注意力层<br>下面几个没讲，沟槽的</p><h3 id="Method-1"><a href="#Method-1" class="headerlink" title="Method"></a>Method</h3><p><strong>Text and Retrieval Embedding Encoder</strong><br>作为一个级联结构模型，使用两个平行的输入<br>一个文本输入 $c_t$ 用于提供低级语义信息<br>一系列音频文本 $c_r$ 用于提供高级语义音频信息<br>音频文本对：$c_r = [<text_1,audio_1>,<text_2,audio_2>,\dots,<text_k,audio_k>]$ 是通过目标的caption 的embedding与检索数据库进行相似度比较选择的top-k邻居，最后将这些音频文本对特征连接：</text_k,audio_k></text_2,audio_2></text_1,audio_1></p><script type="math/tex; mode=display">\begin{aligned}E^{ra} &= CAT[f_{mae}(audio_1),\dots,f_{mae}(audio_k)]\newlineE^{rt} &= CAT[f_{t5}(text_1),\dots,f_{t5}(text_k)]\end{aligned}</script><p><strong>Retrieval-Augmented Diffusion Generator</strong><br>上面两个特征表示是在Unet的每个cross attention中引入的</p><blockquote><p>将检索到的文本和音频信息与其余层中的所有交叉注意力块共享，采用 re-weighted training objective，训练目标如下：</p></blockquote><p><img src="/2024/04/08/%E7%94%9F%E6%88%90%E7%9B%B8%E5%85%B3/image-20240425155735124.png" alt=""><br>看公式根本看不出啥啊，一会儿看看什么是re-weighted training objective</p><h3 id="Experiments-1"><a href="#Experiments-1" class="headerlink" title="Experiments"></a>Experiments</h3><p><img src="/2024/04/08/%E7%94%9F%E6%88%90%E7%9B%B8%E5%85%B3/image-20240425162911590.png" alt=""><br>相较之前的工作，各个指标都提升了很多<br>在Retrieval info中，选择音频文本对作为检索目标，会比纯音频好<br>对于top-k的k的选择，综合效果和开销，文章说选取3-5最好<br><img src="/2024/04/08/%E7%94%9F%E6%88%90%E7%9B%B8%E5%85%B3/image-20240425163127181.png" alt=""><br>对于少样本事件类别的CLAP得分<br><img src="/2024/04/08/%E7%94%9F%E6%88%90%E7%9B%B8%E5%85%B3/image-20240425163204049.png" alt=""><br>zero-shot性能高了很多<br>未来这个组会探索大数据集下的效果和下游任务中的效果</p><h2 id="VoiceBox"><a href="#VoiceBox" class="headerlink" title="VoiceBox"></a>VoiceBox</h2><p>这个好像是TTS，暂时先不看</p><h2 id="TANGO"><a href="#TANGO" class="headerlink" title="TANGO"></a>TANGO</h2><p>本文认为更换TTA中的文本编码器将提高文本理解和整体音频生成<br>替换为一个指令微调大语言模型（instruction-tuned large language model），而且无需任何微调，因为LLM的梯度下降模仿特性(gradient-descent mimicking property)<br>提到现有的样本增强方法没有考虑音频的声压级<br><img src="/2024/04/08/%E7%94%9F%E6%88%90%E7%9B%B8%E5%85%B3/image-20240428113108732.png" alt=""><br>贡献：</p><ol><li>没有使用模态对比训练</li><li>不需要微调文本编码器</li><li>使用<code>pressure levels</code>的样本增强<h3 id="Method-2"><a href="#Method-2" class="headerlink" title="Method"></a>Method</h3><strong>Textual-Prompt Encoder</strong><br>和AudioLDM的区别就是，把CLAP换成了LLM（FLAN-T5）<br>这个做法的根据来自：<blockquote><p>Due to the pre-training of FLAN-T5 models on a large-scale chain-of-thought- (CoT) and instruction-based dataset, Dai et al [4] posit that they are able to learn a new task very well from the in-context information by mimicking gradient descent through attention weights.<br>由于FLAN-T5模型在大规模思维链(CoT)和基于指令的数据集上进行了预训练，Dai等人[4]假设他们能够通过注意力权重模拟梯度下降，从上下文信息中很好地学习新任务。</p></blockquote></li></ol><p><strong>Latent Diffusion Model for Text-Guided Generation</strong><br>不懂<br>作者说这个能力在其他工作用的文本编码器上是没有的<br>还说和音频联合微调文本编码器会降低其上下文学习的能力，所以冻结了文本编码器</p><p>训练过程也不一样<br>在AudioLDM中，扩散模型的训练过程中，Guidance使用的是音频特征，然后在推理时使用文本的CLAP特征。<br>这里训练和推理都使用文本特征（？）<br>感觉还挺合理，毕竟AudioLDM中使用CLAP时提到，可以不需要音频文本对数据。</p><p><strong>Augmentation</strong><br>这一小节讲的就是按照平衡声压级的方式做样本增强<br>首先按照声压级计算混合权重系数</p><script type="math/tex; mode=display">p=\left(1+10^{\frac{G_1-G_2}{20}}\right)^{-1}</script><p>其中$G_1,G_2$是两个样本的声压级<br>混合样本如下：</p><script type="math/tex; mode=display">mix(x_1,x_2)=\frac{px_1+(1-p)x_2}{\sqrt{p^2+(1-p)^2}}</script><p>纯纯的物理学，不看了</p><h3 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h3><p>数据集也是用的Audiocaps<br><img src="/2024/04/08/%E7%94%9F%E6%88%90%E7%9B%B8%E5%85%B3/image-20240428170716835.png" alt=""></p><p>这有个新看到的实验，是把Audiocaps的caption分成两类，一种是包含多个事件的，一种是单个事件，分开比较</p><h2 id="中途小记"><a href="#中途小记" class="headerlink" title="中途小记"></a>中途小记</h2><p>看了这几篇，感觉收获不大，有点浮于表面，中午和武洋聊天，好多都不知道，或者是他和我讲了，然后我左耳进右耳出了。所以现在把脑子里有的都记下来</p><p>首先，他说听别人说，目前Audio generation的大方法差不多这样了，应该往精细化的方向去走，比如控制每个事件在音频中的持续时间，对于音频中的前景声和背景声的合成控制，对于一些精细语义的控制，比如狗叫了多少下，文本中事件的先后顺序。<br>我之前一拍脑门想到的那个控制时长的方法，不是科学研究的正确做法，想是要想，更多的应该是去找有没有类似的做法。这个方向已经有很多人在做了，只不过还没迁移到音频上来。比如图像上的control net，应该意识到这一点，然后去图像上找相应的方法。<br>然后是那个生成引导域适应的工作，目前他想到两个做法，一个是给数据集，一个是给方法，都是用来提高分类模型在域迁移问题下的分类性能。</p><p>对于我现在看的这些，刚才看了下周报，发现也没有看几周这个内容，目前的几个问题是，对生成的各个环节，没有了解的很透彻，比如评价生成模型的几个指标，叫什么，怎么计算的，表明什么特征。生成的各个环节，具体的工作原理，有没有深入代码，让我修改一下要从哪里入手，有没有拿生成模型干过事情。现在还仅仅停留在运行代码的层面，还远远不够。</p><p>现阶段要做的事情：</p><ul><li>详细看看音频生成的几个指标是怎么计算的，表示什么</li><li>调研一下控制音频生成的工作</li><li>调研在图像上控制精细生成的工作</li><li>尝试将图像上的工作搬到音频</li></ul><h2 id="音频生成指标"><a href="#音频生成指标" class="headerlink" title="音频生成指标"></a>音频生成指标</h2><p>看到个人写了点东西<br><a href="https://blog.csdn.net/Blackoutdragon/article/details/132687747">https://blog.csdn.net/Blackoutdragon/article/details/132687747</a></p><h3 id="主观指标"><a href="#主观指标" class="headerlink" title="主观指标"></a>主观指标</h3><h4 id="Overall-quality-OVL"><a href="#Overall-quality-OVL" class="headerlink" title="Overall quality, OVL"></a>Overall quality, OVL</h4><p>评分者被要求在1到100之间的范围内对所提供样本的感知质量进行评分</p><h4 id="relevance-to-the-text-input"><a href="#relevance-to-the-text-input" class="headerlink" title="relevance to the text input"></a>relevance to the text input</h4><p>评分者被要求在1到100的范围内对音频和文本的匹配程度进行评分</p><h4 id="Fidelity"><a href="#Fidelity" class="headerlink" title="Fidelity"></a>Fidelity</h4><p>Diffsound中使用，保真度，也是衡量音频质量的指标，1-5分</p><h4 id="Intelligibility"><a href="#Intelligibility" class="headerlink" title="Intelligibility"></a>Intelligibility</h4><p>Diffsound中使用，可理解度，应该是度量和文本的照应程度的，是1-5分</p><h4 id="mean-opinion-score-MOS"><a href="#mean-opinion-score-MOS" class="headerlink" title="mean opinion score, MOS"></a>mean opinion score, MOS</h4><p>在make an audio中，将这个指标分成了两部分，生成的质量（是否自然, MOS-Q），是否与自然语言描述相符(MOS-F)<br>和上面的OVL和relevance相照应<br><img src="/2024/04/08/%E7%94%9F%E6%88%90%E7%9B%B8%E5%85%B3/image-20240508162318997.png" alt=""><br>给了一些阶段，然后找人评价</p><h3 id="客观指标"><a href="#客观指标" class="headerlink" title="客观指标"></a>客观指标</h3><h4 id="Frechet-Audio-Distance-FAD"><a href="#Frechet-Audio-Distance-FAD" class="headerlink" title="Frechet Audio Distance, FAD"></a>Frechet Audio Distance, FAD</h4><p>是谷歌提出的一个评估指标，最初是用于评估<code>music enhancement</code>算法<br>谷歌还专门有篇论文讲这个，看了下这篇论文<br>是因为之前的那些指标，在衡量音乐增强的工作时，基于信号的指标通常与人对增强音乐的主观评估不一致，为此提出了FAD：用于衡量给定音频片段与干净的录音室录制的音乐的比较<br><img src="/2024/04/08/%E7%94%9F%E6%88%90%E7%9B%B8%E5%85%B3/image-20240509102604162.png" alt=""><br>不使用单个音频片段来计算得分，而是将整个需要评估的数据集上生成的<a href="https://zhuanlan.zhihu.com/p/111996722">VGGish</a>的嵌入的统计数据，与大型干净音乐数据集（比如训练集，这里其他也行，我看AudioLDM用的是PANNs）<br>评估集和背景干净数据集上得到的嵌入的高斯统计特征分别为$\mathcal N_e(\mu_e,\Sigma_e), \mathcal N_b(\mu_b,\Sigma_b)$<br>如下计算得到FAD：</p><script type="math/tex; mode=display">\boldsymbol{F}(\mathcal{N_b,N_e})=\Vert \mu_b-\mu_e \Vert^2 + tr(\Sigma_b + \Sigma_e - 2\sqrt{\Sigma_b \Sigma_e})</script><h4 id="Frechet-Inception-Distance-FID"><a href="#Frechet-Inception-Distance-FID" class="headerlink" title="Frechet Inception Distance, FID"></a>Frechet Inception Distance, FID</h4><p>这个是图像上的，但是不知道为什么Diffsound要用这个评估，不同FAD<br>不过Diffsound用的是相似架构的InceptionV3模型，然后自己在AudioSet上训练了，用自己训练的模型计算FID</p><h4 id="sota-分类模型对生成样本和原始样本的KL散度"><a href="#sota-分类模型对生成样本和原始样本的KL散度" class="headerlink" title="sota 分类模型对生成样本和原始样本的KL散度"></a>sota 分类模型对生成样本和原始样本的KL散度</h4><p>问题：怎么选取这个原始样本和生成的样本<br>就说计算生成样本和真实样本经过分类模型的输出，然后输出的两个概率分布之间的KL散度</p><h4 id="SPICE，CIDEr"><a href="#SPICE，CIDEr" class="headerlink" title="SPICE，CIDEr"></a>SPICE，CIDEr</h4><p>这个是Audio Caption损失，仅在Diffsound中用过，计算的是生成的音频经过Caption之后，得到的描述和用于生成的描述之间的相似度</p><h4 id="Frechet-Distance-FD"><a href="#Frechet-Distance-FD" class="headerlink" title="Frechet Distance, FD"></a>Frechet Distance, FD</h4><p>就是衡量两条曲线的相似性，在音频中应该是算音频信号？还是那个经典的问题，怎么选取两条波形信号，一个是生成的，另一个呢<br>但是在audioldm_eval中，又涉及到了一个分类模型<br>后面两个在audioldm_eval中都有<br>看了下代码，就是FAD的计算过程啊，就是用的是PANNs而不是FAD</p><h4 id="Inception-Score-IS"><a href="#Inception-Score-IS" class="headerlink" title="Inception Score, IS"></a>Inception Score, IS</h4><p>这也是从图像上搞过来的<br>从两个方面评价GAN生成的图片的质量：</p><ul><li>清晰度：对于清晰的图片，属于分类器某一类的概率应该非常大，$p(y|x)$的熵应该很小</li><li>多样性：从所有的图片的角度考虑，在生成的一堆图片中，如果这些图片是多样性的，那么每个类别的数目应该是差不多的，也就是说$p(y)$的熵应该很大<br>所以这两个分布的距离要大，用KL散度衡量<script type="math/tex; mode=display">\boldsymbol{IS}(G)=\exp \left( \mathbb{E}_{x\sim p_g} D_{KL}(p(y|x)\Vert p(y)) \right)</script>在audioldm_eval中，使用的是多个样本，然后取一个样本窗口，在窗口内计算输出概率的平均值，和没求平均的概率算KL散度。<h4 id="CLAP-score"><a href="#CLAP-score" class="headerlink" title="CLAP score"></a>CLAP score</h4>算的是CLAP特征的余弦相似度</li></ul><h1 id="控制图像生成的工作"><a href="#控制图像生成的工作" class="headerlink" title="控制图像生成的工作"></a>控制图像生成的工作</h1><p>牛魔，一搜一堆<br>这几个先看看<br><a href="https://zhuanlan.zhihu.com/p/626346656">https://zhuanlan.zhihu.com/p/626346656</a><br><a href="https://zhuanlan.zhihu.com/p/687862680">https://zhuanlan.zhihu.com/p/687862680</a><br><a href="https://zhuanlan.zhihu.com/p/613909875">https://zhuanlan.zhihu.com/p/613909875</a><br><a href="https://cloud.tencent.com/developer/article/2379807">https://cloud.tencent.com/developer/article/2379807</a></p><h2 id="Adding-Conditional-Control-to-Text-to-Image-Diffusion-Models"><a href="#Adding-Conditional-Control-to-Text-to-Image-Diffusion-Models" class="headerlink" title="Adding Conditional Control to Text-to-Image Diffusion Models"></a>Adding Conditional Control to Text-to-Image Diffusion Models</h2><p>Lvmin Zhang, Anyi Rao<br>Stanford University</p><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>仅仅通过文本提示来精确表达复杂的布局、姿势、形状和形式是很困难的。<br>直接对大型预训练模型训练或微调会导致过拟合和遗忘<br>ControlNet通过锁定参数保持大模型的质量和功能，并将encoding layer复制一份可训练部分<br>可训练部分和原始锁定的模型使用 <code>zero convolution layers</code>连接，就是0初始化的卷积层<br>这样做的目的是保证在训练开始时，大扩散模型的深层特征中不会加入有害噪声，并保证了可训练副本中主干网络不被这些噪声破坏<br>实验表明，可以用多种控制条件控制Stable Diffusion</p><h3 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h3><p>其他微调神经网络的方法：HyperNetwork，Adapter，Additive Learning, LoRA<br>控制图像扩散模型的方法：<br>文本引导控制方法侧重于调整提示、操纵CLIP特征和修改交叉注意力<br>巴拉巴拉讲一堆，看不懂的专业词汇，留在这之后再一篇一篇具体看</p><h3 id="Method-3"><a href="#Method-3" class="headerlink" title="Method"></a>Method</h3><h4 id="ControlNet"><a href="#ControlNet" class="headerlink" title="ControlNet"></a>ControlNet</h4><p><img src="/2024/04/08/%E7%94%9F%E6%88%90%E7%9B%B8%E5%85%B3/image-20240514191522795.png" alt=""><br>ControlNet将额外的条件信息注入神经网络块中，如上图<br>x,y 是两个特征图，ControlNet通过拷贝一个副本，在输入输出加上一个1x1的0初始化卷积层，将条件和原输入加入副本，输出加入原输出</p><h4 id="ControlNet-for-Text-to-Image-Diffusion"><a href="#ControlNet-for-Text-to-Image-Diffusion" class="headerlink" title="ControlNet for Text-to-Image Diffusion"></a>ControlNet for Text-to-Image Diffusion</h4><p><img src="/2024/04/08/%E7%94%9F%E6%88%90%E7%9B%B8%E5%85%B3/image-20240514194010649.png" alt=""><br>上图为Stable Diffusion的Unet结构，和上面对每个小块的处理类似，在Unet的Encoder部分，只在最前面用了一个零卷积，然后在之后decoder部分，每一小块都有一个0卷积层（因为Diffusion的decoder每一块还要和encoder的每一块做cross-attention）<br>SD是在latent space做的扩散，所以最先的Condition $C_f$ 也要映射到相同形状，作者这里用了4个卷积层加ReLU。</p><h4 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h4><p>优化目标和普通的Diffusion一样，也是预测每一步的噪声<br>在训练过程中，随机将50%的文本描述换成空字符串，这个方法会提高ControlNet直接识别输入语义条件的能力。<br>而且在实验中，模型是突然学会依照condition去生成图片的</p><h4 id="Inference"><a href="#Inference" class="headerlink" title="Inference"></a>Inference</h4><p><strong>Classifier free guidance</strong><br>这里对CFG的系数做了一些工作，按照Unet的块的大小，分别计算 $w_i = 64/h_i$<br><strong>Composing multiple ControlNets</strong><br>使用多个ControlNets conditions时，比如姿势和线稿，我们可以直接将对应condition相加输入到SD，不需要额外的工作</p><h3 id="Experiments-2"><a href="#Experiments-2" class="headerlink" title="Experiments"></a>Experiments</h3><p>消融实验证明了0卷积层和可训练副本的有效性<br>人类感知实验证明了ControlNet生成的条件依赖性和质量，并且和SDv2-D2I的生成结果几乎难以分辨</p><p>不会改变Stable Diffusion的网络结构，所以可以直接应用于SD社区中的其他模型</p><h1 id="控制音频生成的工作"><a href="#控制音频生成的工作" class="headerlink" title="控制音频生成的工作"></a>控制音频生成的工作</h1><p>也是有的，不过没有图像那么多</p><h2 id="Simple-and-Controllable-Music-Generation"><a href="#Simple-and-Controllable-Music-Generation" class="headerlink" title="Simple and Controllable Music Generation"></a>Simple and Controllable Music Generation</h2><p>发表情况：NeurIPS 2023<br>作者：Jade Copet, Felix Kreuk<br>机构：Meta AI<br>这还是我看的第一篇关于音乐生成的文章，所以仔细看看<br>音乐生成的几个问题：长序列，需要全频段，采样率要求也更高<br>要控制旋律和和声，使结构很复杂<br>还需要用不同的方法控制生成过程，比如音阶，乐器，旋律，流派<br>为了使音频建模更容易处理，最近的研究提出将音频信号表示为表示相同信号的多个离散token stream？不是很懂<br>大概就是，要对一条音频流进行建模，采用离散token的方法，但是有多个token序列，比如 Kharitonov对语音流的做法是每个流有不同的偏移，Agostinelli的做法是使用不同粒度的离散token表示音乐片段，使用自回归模型的层次结构对他们进行建模。</p><p>本工作提出了一个简单可控的音乐生成模型 MUSICGEN，能够给定文本描述，生成高质量的音乐<br>看方法，带着这几个问题去看</p><ul><li>音乐生成的大框架是怎样的</li><li>用什么方法来控制生成的音乐和文本旋律之间的关系<h3 id="Method-4"><a href="#Method-4" class="headerlink" title="Method"></a>Method</h3>这前面的部分完全看不懂在讲什么<h4 id="Audio-tokenization"><a href="#Audio-tokenization" class="headerlink" title="Audio tokenization"></a>Audio tokenization</h4><a href="https://zhuanlan.zhihu.com/p/627515982">RVQ相关</a><br>使用的是 EnCodec 是个别人的工作<br>给定一个音频随机变量 $X\in \mathbb{R}^{d\cdot f_s}$ ，EnCodec将其编码成一个连续的tensor，帧率 $f_r \ll f_s$<br>然后被RVQ量化为 $Q\in \{1,\dots,M\}^{d\cdot f_r \times K}$ ，其中$K$是码本数，M是码本大小<br>然后他这个码本，是按照RVQ层数算的，有几层就有几个码本<br>因为在RVQ中，每个量化器都是对前一个量化器留下的量化误差进行编码，所以每个码本也不是独立的，第一个码本是最重要的</li></ul><h4 id="Codebook-interleaving-patterns"><a href="#Codebook-interleaving-patterns" class="headerlink" title="Codebook interleaving patterns"></a>Codebook interleaving patterns</h4><p><img src="/2024/04/08/%E7%94%9F%E6%88%90%E7%9B%B8%E5%85%B3/image-20240510144419328.png" alt=""><br><strong>Exact flattened autoregressive decomposition</strong><br>这一部分是说，对于tokenizer提取的几个码本的序列，因为有四个量化器，所以对于一条音频，就有四个token序列。在使用自回归建模这些序列的时候，有两种方式，一是把这些序列展平，这样会让序列变的很长，二是把这些序列叠起来，就是本文的做法<br><strong>Inexact autoregressive decomposition</strong><br>这两对数学公式有什么区别，我能问问嘛？<br>把这个序列叠起来预测的问题是，随着t的增加，两个分布会越离越远，但是可以加快训练和推理，特别是对于长序列<br>然后将任意码本交错模式，就是图里的</p><h4 id="Model-conditioning"><a href="#Model-conditioning" class="headerlink" title="Model conditioning"></a>Model conditioning</h4><p>分为两个部分，一个是文本条件，另一个是旋律条件，也没有细说，沟槽的<br>文本条件还是那几种做法，T5，FLAN-T5，CLAP<br>旋律条件：用的是色谱图（chromagram），只随便看了看，<a href="https://www.zhihu.com/question/23631477">链接</a></p><p>沟槽的论文，怎么连个系统结构都不给。<br>对双声道的处理如下<br><img src="/2024/04/08/%E7%94%9F%E6%88%90%E7%9B%B8%E5%85%B3/image-20240510161643681.png" alt=""><br>就是在token的堆叠上加入了两个声道<br>吗的好气，一笔带过了没讲清楚<br>但是这篇论文和他们的AudioGEN是同一时期的东西，而且二者的结构都是类似的</p><h2 id="Music-ControlNet"><a href="#Music-ControlNet" class="headerlink" title="Music ControlNet"></a>Music ControlNet</h2><p>TASLP<br>2024.5.22<br>Shih-Lun Wu, Chris Donahue, Shinji watanabe<br>CMU<br>真的是快人一步<br><img src="/2024/04/08/%E7%94%9F%E6%88%90%E7%9B%B8%E5%85%B3/image-20240527154322264.png" alt=""></p><p>一篇在text to music中，使用controlnet加入时序，音符控制的工作<br>由于训练数据的原因，文本基本只能控制生成音乐的风格，情绪这种全局的信息<br>贡献：</p><ul><li>一个能够使用组合精准时变条件控制音频生成的框架</li><li>可以在推理时加入灵活的时变控制条件</li><li>可以控制旋律动力和节奏<h3 id="问题规约"><a href="#问题规约" class="headerlink" title="问题规约"></a>问题规约</h3>这里提到和图像上像素级别的控制相比，在频谱图上的控制不同。图像上前两维都是空间信息，而音频上一个是时间一个是频率<br>对于创作者来说，一个时变控制与时间的相关性会比频率更大。<br>所以将控制信号放宽定义为：<script type="math/tex; mode=display">\begin{aligned}C:=\left\{ c^{(n)} \in \mathbb{R}^{Tf_k\times B_n \times D_n} \right\}^N_{n=1}\end{aligned}</script>$B_n$ 是特定于每种控制信号的类别数，D是通道数（单声道双声道）<br>按照这种规约，每一个 $c^{(n)}_t\in \mathbb{R}^{1\times 1}$ 都是一个帧级约束<h3 id="Method-5"><a href="#Method-5" class="headerlink" title="Method"></a>Method</h3>把Music ControlNet问题规约为<script type="math/tex; mode=display">\tilde f^{(l)}\left( x^{(m,l-1)},m,c_{text},C \right):=Z_{out}\left( f^{(l)}(x^{(m,l-1)}+Z_{in}(M^{(n)}(c^{(n)})),m,c_{text})\right)</script>其中 $Z_{in},Z_{out}$ 是ControlNet的零卷积层，$M^{(n)}$ 是一个额外添加的单层MLP，将n种控制信号$B_n$映射到和梅尔bins$B$ 的形状相同。<br>对于多种控制信号，每种控制信号有一个单独的MLP<h4 id="Masking-Strategy-to-Enable-Partially-Specified-Controls"><a href="#Masking-Strategy-to-Enable-Partially-Specified-Controls" class="headerlink" title="Masking Strategy to Enable Partially-Specified Controls"></a>Masking Strategy to Enable Partially-Specified Controls</h4>为了使创作者可以自由使用控制类型的任何组合，使用CFG-like训练策略。令控制信号的索引集合为$I=\{1,\dots,N\}$，在每个训练step，选择一个子集$I’\in I$，其中的训练类型会被设为0或丢弃<br>$$c^{(n)}:=<br>\left\{  <pre><code>       \begin{array}{**lr**}        0_{Tf_k\times B_n \times D_n}&amp;\forall n\in I' \newline      c^{(n)} &amp;\forall n \in I\setminus I'       \end{array}  </code></pre>\right.  </li></ul><script type="math/tex; mode=display">这种训练策略可以让模型学习任何控制子集之间的相关性。这个地方就和武洋说的多个事件组合的问题，如何让模型学会不同事件组合之间的相关性。然后为了使这个control能在时域上部分指定，除了上面的全mask，还加了一个部分mask![](生成相关/image-20240528102726846.png)就是在启用的控制条件中，随机mask一个时间段。#### Musical Control Signals三种控制信号：melody, dynamics, rhythm两种得到控制信号的方法：extracted controls, created controlsMelody：$c_{mel}\in \mathbb{R}^{Tf_k \times 12\times 1}$ 对线性谱的每一帧计算chromagram，做argmax操作，将最突出的音阶作为该帧的的one-hot特征还会过滤低音部分，以使生成的音阶为旋律音阶而不是低音部分的音阶动态是音乐中声音强度随时间变化的表征节奏曲线#### Evaluation MetricsMelody accuracyDynamics correlationRhythm F1CLAP scoreFAD## FoleyCrafter: Bring Silent Videos to Life with Lifelike and Synchronized Sounds用于自动生成与视频同步的高质量音效，包括两个关键组件：用于语义对齐的语义适配器（semantic adapter）和用于精确音视频同步的时间控制器（temporal controller）。可以与预先训练T2A模型集成，并通过音频监督进行优化时间控制器包括一个开始检测器（onset detector）和一个基于时间戳的适配器（timestamp-based adapter）这个适配器的作用是，根据开始检测器预测的时间戳和音频生成对齐重点关注这个适配器控制LDM的生成有两种方式，一个是多层感知机MLP，扩散过程的时间戳和text embedding拼接起来作为条件信息，然后和Unet的特征图通过MLP层融合另一个是交叉注意力机制，作用于Unet的每一块。本文采用交叉注意力价值整合text和vision条件。还提到了Aufusion那个工作，已经AudioLDM2，都是在Unet中间块中加入了Attension### Approach#### FoleyCrafter![](生成相关/image-20240703145720202.png)接下来详细讲每个模块S.A.， Semantic Adapter![](生成相关/image-20240703150424814.png)使用预训练的visual encoder，直接提取视频特征，然后这个线性层和归一化层，算是特征压缩吧，搞成和原来的text embedding相同的形状，然后算cross attention加入unet每个模块关于[cross attention](https://blog.csdn.net/MengYa_Dream/article/details/126688503)无语，这哪是cross attention下文说，是两个不同的attention，然后用一个权重参数λ组合起来训练的优化目标还是diffusion的噪声损失为了有效的捕获视觉线索并将它们与文本到音频生成器的条件空间对齐，以90%的概率随机丢弃文本条件Temporal Controller![](生成相关/image-20240703154303748.png)这不ControlNet吗Timestamp Detector将视频帧作为输入，预测二元时间掩码，指示目标音频中的声音效果是否存在Temporal Adapter，将时域信息编码，注入到Unet的decoder和我做的事情一模一样问题，怎么把这个时间戳和对应的事件语义联系起来呢，我能问问吗回到semantic adapter部分这里的visual encoder干的事情</script><p>V_{emb} = MLP(AvgPooling(\tau_{vis}(v)))</p><script type="math/tex; mode=display">其中 $\tau_{vis}$ 代表CLIP image encoder，$AvgPooling$ 指对提取的CLIP特征的**跨帧特征**的平均池化就是说，这里得到的visual embedding，已经是包含事件发生顺序的了。# Seeing and Hearing: Open-domain Visual-Audio Generation with Diffusion Latent AlignersCVPR 2024![](生成相关/image-20240807093050411.png)提出了一个 ImageBind ，多模态隐空间对齐器，有着和 classifier guidance相同的核心在预测时引导扩散模型的生成- 利用了单模态生成模型的优秀性能- 我们的目标是探索如何利用ImageBind作为有效连接和集成各种模式的桥梁我们已经注意到，预训练模型ImageBind[17]在共享语义空间内不同数据模式之间建立有效连接方面具有卓越的能力。### Diffusion Latent Aligner# Conditional sound generation using neural discrete time-frequency representation learning21年 MLSP workshop![](生成相关/image-20240813095804786.png)这篇文章追溯到17年的SampleRNN，但是SampleRNN无法捕获长序列依赖提出一种通过`neural discrete time-frequency representation learning`通过声音标签控制生成的工作用VQ-VAE做生成我们的方法可以模拟声音的长期依赖关系，同时减少了在T-F域中具有高时间分辨率的声音建模的计算成本使用VQ-VAE学习离散的TF representation（DTFR）然后使用一个自回归模型，用DTFR作为输入，在事件类别的条件下生成音频此外，我们提出了VQ-VAE编码器的多尺度卷积方案，以捕获不同尺度声音的声学信息## Approach### Discrete time-frequency representation learning就是普通的VQ-VAE的训练过程但是这里每个频谱图样本 $x$ 生成的隐变量表示 $z\in R^{H/2^m \times W/2^m \times D}$ ，m为压缩系数每个隐变量 $z$ 包含 N 个元素 $z_n \in R^{1\times 1\times D}$ ，其中 $N=H/2^m \times W/2^m$ 然后和码本 $C=\{c_k\}^K_{k=1}$ 计算距离，取最近的码本向量，得到 $DTFR=r=\{r_n\}^N_{n=1}$  然后是重建，因为有个量化，所以有stop gradient啥的操作上文提到的多尺度卷积方案<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">blocks_1 = [</span><br><span class="line">nn.Conv2d(in_channel, channel // <span class="number">2</span>, <span class="number">4</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>),</span><br><span class="line">nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">nn.Conv2d(channel // <span class="number">2</span>, channel, <span class="number">4</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>),</span><br><span class="line">nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">nn.Conv2d(channel, channel, <span class="number">3</span>, padding=<span class="number">1</span>),</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">blocks_2 = [</span><br><span class="line">nn.Conv2d(in_channel, channel // <span class="number">2</span>, <span class="number">2</span>, stride=<span class="number">2</span>, padding=<span class="number">0</span>),</span><br><span class="line">nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">nn.Conv2d(channel // <span class="number">2</span>, channel, <span class="number">2</span>, stride=<span class="number">2</span>, padding=<span class="number">0</span>),</span><br><span class="line">nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">nn.Conv2d(channel, channel, <span class="number">3</span>, padding=<span class="number">1</span>),</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">blocks_3 = [</span><br><span class="line">nn.Conv2d(in_channel, channel // <span class="number">2</span>, <span class="number">6</span>, stride=<span class="number">2</span>, padding=<span class="number">2</span>),</span><br><span class="line">nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">nn.Conv2d(channel // <span class="number">2</span>, channel, <span class="number">6</span>, stride=<span class="number">2</span>, padding=<span class="number">2</span>),</span><br><span class="line">nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">nn.Conv2d(channel, channel, <span class="number">3</span>, padding=<span class="number">1</span>),</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">blocks_4 = [</span><br><span class="line">nn.Conv2d(in_channel, channel // <span class="number">2</span>, <span class="number">8</span>, stride=<span class="number">2</span>, padding=<span class="number">3</span>),</span><br><span class="line">nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">nn.Conv2d(channel // <span class="number">2</span>, channel, <span class="number">8</span>, stride=<span class="number">2</span>, padding=<span class="number">3</span>),</span><br><span class="line">nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">nn.Conv2d(channel, channel, <span class="number">3</span>, padding=<span class="number">1</span>),</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, <span class="built_in">input</span></span>):</span><br><span class="line"><span class="keyword">return</span> self.blocks_1(<span class="built_in">input</span>) + self.blocks_2(<span class="built_in">input</span>) + self.blocks_3(<span class="built_in">input</span>) + self.blocks_4(<span class="built_in">input</span>)</span><br></pre></td></tr></table></figure>就是这样的多尺度卷积核的大小不一样，但是最后特征的大小是一样的### Conditional sound generation其实和LDM的思想类似，用一个生成模型去生成latent，然后从latent解码出原来模态的数据最终要生成的是码本序列，作为一个自回归模型，要得到序列的联合分布，可以拆解为：</script><p>p(y)=p(y_1,\dots,y_n)=\prod^n_{i=1}p(y_i|y_1,\dots,y_{i-1})</p><script type="math/tex; mode=display">加上类别条件的one-hot 编码向量 h</script><p>p(y|h)=p(y_1,\dots,y_n|h)=\prod^n_{i=1}p(y_i|y_1,\dots,y_{i-1}|h)</p><script type="math/tex; mode=display">使用PixelSNAIL来建模这个分布这个东西见[链接](https://blog.csdn.net/Blackoutdragon/article/details/132306463)最后HiFi-GAN从梅尔生成wav也没啥对比方法了，就一个SampleRNN# Acoustic scene generation with conditional sampleRNN2019 ICASSP![](生成相关/image-20240813154535583.png)conditional SampleRNN 在输入类别的条件下生成声音场景举了DCASE 2016的例子，说生成模型生成样本的分类准确率有65.5%啥啥啥的## IntroductionSampleRNN基于多个RNN，对应不同时间分辨率Conditional SampleRNN被用于语音识别然后讲评估方法不太行，以前使用的是生成样本对待评估数据的似然来评价生成质量，但是似然和生成质量有时不是正相关这篇文章就提到了inception score，已经用来评估生成样本的质量，但是，inception score之评估类别之间的多样性，没有考虑一个类别内生成的多样性。本文中，使用`Conditional SampleRNN`模型生成声学场景，还提出了一个衡量生成样本质量的指标## Audio generation model![](生成相关/image-20240814101520693.png)生成模型，诸如WaveNet和SampleRNN，生成样本X的过程可以建模为T个采样点的联合分布</script><p>p(X)=p(x_1,\dots x_T)=\prod^{T-1}_{t=0}p(x_{t+1}|x_1,\dots,x_t)</p><script type="math/tex; mode=display">SampleRNN首先将采样点分帧，然后送入RNN学习长依赖下层的卷积学习的是采样点中的短时依赖，然后加在一起本文使用的`conditional SampleRNN`，使用one-hot类别信息编码 z然后将z复制到每一帧作为附加信息整个过程可以规约为：![](生成相关/image-20240814104118275.png)$f_R$ 指代第二层的循环连接W和V是嵌入映射矩阵，分别用于输入和条件类别## EvaluationIS使用两个分类器，一个在真实数据上训练，在生成数据上分类，用来评估生成质量，$f_{real}$一个在生成数据上训练，在真实数据上分类，用来评估生成多样性，$f_{gen}$好像还挺有道理的第二个的解释是，如果生成数据多样性差，在生成数据上训练的分类器在真实数据上就覆盖不到那么多类别，分类准确率就低## Experiments![](生成相关/image-20240814143538612.png)$f_{real}$ 的结果![](生成相关/image-20240814144435986.png)真实数据上的混淆矩阵后面还有个生成数据上的混淆矩阵![](生成相关/image-20240814144743552.png)生成样本数量和分类准确率# UniAudio: An Audio Foundation Model Toward Universal Audio GenerationICML 2024![](生成相关/image-20240814163644934.png)首先tokenize所有类别的音频和相应的条件模态，然后使用LLM预测下一个token想要实现 universal audio generation使用一个 `universal neural codec model` 来建模所有类别的音频，对于其他模态的条件，也有对应的tokenizer为了解决token序列过长的问题，使用一个多尺度的Transformer结构来减少计算复杂度，包括一个全局一个局部1B参数量贡献点：- 11个音频生成任务的统一解决方案- 提供了新的方法论，音频和其他输入模态的序列表征，制定了统一化的基于LLM的音频生成任务，专门为音频生成设计的高效模型架构## UniAudio### Tokenization这些tokenizer都是固定参数的，不参与UniAudio的优化过程Audio模态的tokenizer是RVQ转换成序列，每帧音频的RVQ离散序列数量是3，然后按照顺序展平为 $z$因为可以从 $z$ 还原出音频，所以只用预测序列 $z$ 就行了也是LDM的做法**其他模态**音素：纯文本的话，用一个text-to-phoneme映射，语音的话用DNN-HMMMIDI：使用duration information展平F0 序列，得到帧级F0序列Text：使用一个预训练的LLM，T5 modelSemantic Token：音频自监督模型的输出，将这些连续表征做K-means聚类，得到序列，这个是做VC用的然后会有一些标签来提示大模型任务### Multi-Scale TransformerTransformer对于输入特征长度的计算复杂度是二次的使用多尺度Transformer，用于建模离散音频序列全局transformer考虑的是patch（比如连续的$n_q$个token），局部考虑的是patch内的内容，都是causal的这里学到一个词，causal，在序列预测模型中，指的是模型只能从当前输入和之前输入获取信息用于预测，不能从之后的输入中预测当前的输出对于非音频的特征（其他条件特征）因为每个token都有独立的语义，所以每个都是一个patch啥意思，连续的文本嵌入也重复 $n_q$ 次，并且嵌入过程由线性变换取代，啥啥啥的## Experiment![](生成相关/image-20240815152026862.png)和AudioLDM比的还拿的人家最差的模型比，他自己的也不高# AudioLM: a Language Modeling Approach to Audio Generationgoogle research也是通过特征离散化，将音频生成任务当作语言建模任务提出了一种混合标记化方案来实现这两个目标能够做多任务生成提问，好像主要做speech？难怪我没看这有个翻译 [链接](https://blog.csdn.net/eggplant323/article/details/134386710)笑死，全是中文字，看起来比英文还费劲确实没太关注声音事件生成，和VALL-E类似# ENVIRONMENTAL SOUND SYNTHESIS FROM VOCAL IMITATIONS AND SOUND EVENT LABELS![](生成相关/image-20240823232154019.png)ICASSP24使用声音模仿表达环境声音的特征VQ encoder and Tacotron2 decoder声音模仿可以控制合成声音的音高和节奏，这是只有声音事件标签无法控制的各种音频合成的控制信息![](生成相关/image-20240823232611623.png)本文提出的方法，使用声音模仿和声音事件类别标签合成声音事件## Proposed Method本模型 $Synthesizer(\cdot )$  从口头模仿和one-hot 音频标签中合成声音事件</script><p>\hat y = Synthesizer(x,c)</p><p>$$<br><img src="/2024/04/08/%E7%94%9F%E6%88%90%E7%9B%B8%E5%85%B3/image-20240823235323604.png" alt=""></p><p>条件离散表示 $V’$ 如下计算<br><img src="/2024/04/08/%E7%94%9F%E6%88%90%E7%9B%B8%E5%85%B3/image-20240824001441349.png" alt=""><br>式中，V和Linear(·)分别表示编码器和线性变换得到的t长度的d维离散表示。量化特征控制合成声音的音高和节奏，声音事件标签控制整体印象。Tacotron2[24]论文后面的解码器输出mel谱图。对线性层和解码器进行训练，使真实值和预测梅尔谱图之间的均方误差最小。用神经声码器从预测的梅尔谱图合成声音波形</p><h2 id="Experiments-3"><a href="#Experiments-3" class="headerlink" title="Experiments"></a>Experiments</h2><p>按照其他使用矢量量化方法的音频合成任务的设置，将k-means的k设置成200<br><img src="/2024/04/08/%E7%94%9F%E6%88%90%E7%9B%B8%E5%85%B3/image-20240824223649554.png" alt=""><br>所提出的方法在平均类别MOS上不如仅使用标签的方法<br><img src="/2024/04/08/%E7%94%9F%E6%88%90%E7%9B%B8%E5%85%B3/image-20240824223736333.png" alt=""><br>可以看到，使用声音模仿生成环境音只在音高和韵律两个属性上更好，人类模仿的声音可能会干扰生成模型生成音频的自然度<br>后面两个表的结果也是这样</p>]]></content>
      
      
      
        <tags>
            
            <tag> 论文 </tag>
            
            <tag> 深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>PyTorch Lighting框架学习</title>
      <link href="/2024/03/25/PyTorch%20Lighting%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0/"/>
      <url>/2024/03/25/PyTorch%20Lighting%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0/</url>
      
        <content type="html"><![CDATA[<h1 id="PyTorch-Lighting框架学习"><a href="#PyTorch-Lighting框架学习" class="headerlink" title="PyTorch Lighting框架学习"></a>PyTorch Lighting框架学习</h1><p>最近在搞一个挑战赛，但是感觉自己写的代码好难看，很混乱，而且大部分还是复用别人的东西，所以打算系统学习一下这个简洁的框架。</p><h2 id="parser"><a href="#parser" class="headerlink" title="parser"></a>parser</h2><p>先把这个学了，然后去看那个参数转实例的<br><a href="https://blog.csdn.net/MengYa_Dream/article/details/124451852">参考链接</a></p><h2 id="dataset"><a href="#dataset" class="headerlink" title="dataset"></a>dataset</h2><p>看一个知乎的<a href="https://zhuanlan.zhihu.com/p/353985363">帖子</a>，讲到的一种项目组织方法：<br></p><figure class="highlight text"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">root-</span><br><span class="line">    |-data</span><br><span class="line">        |-__init__.py</span><br><span class="line">        |-data_interface.py</span><br><span class="line">        |-xxxdataset1.py</span><br><span class="line">        |-xxxdataset2.py</span><br><span class="line">        |-...</span><br><span class="line">    |-model</span><br><span class="line">        |-__init__.py</span><br><span class="line">        |-model_interface.py</span><br><span class="line">        |-xxxmodel1.py</span><br><span class="line">        |-xxxmodel2.py</span><br><span class="line">        |-...</span><br><span class="line">    |-main.py</span><br></pre></td></tr></tbody></table></figure><br>我在root下还加了个config.py，上面这种方法，在data_interface和model_interface中，分别写数据集和模型的wrapper，然后在main中用trainer去训练<p></p><p>在mixmatch半监督方法中，会用到标记和未标记两部分数据集，框架支持定义dataloader时返回一个列表或数组：<br></p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pytorch_lightning.trainer.supporters <span class="keyword">import</span> CombinedLoader</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train_dataloader</span>(<span class="params">self</span>):</span><br><span class="line">    loader_a = DataLoader()</span><br><span class="line">    loader_b = DataLoader()</span><br><span class="line">    loaders = {<span class="string">"a"</span>: loader_a, <span class="string">"b"</span>: loader_b}</span><br><span class="line">    combined_loader = CombinedLoader(loaders, mode=<span class="string">"max_size_cycle"</span>)</span><br><span class="line">    <span class="keyword">return</span> combined_loader</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">training_step</span>(<span class="params">self, batch, batch_idx</span>):</span><br><span class="line">    batch_a = batch[<span class="string">"a"</span>]</span><br><span class="line">    batch_b = batch[<span class="string">"b"</span>]</span><br></pre></td></tr></tbody></table></figure><p></p><p>在我这个任务中会涉及到好几个数据集，lightning框架的<code>LightningDataModule</code>提供了一个setup的hook接口，所有的数据集定义，条件选择都在这里做<br></p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">setup</span>(<span class="params">self, stage=<span class="literal">None</span></span>):</span><br><span class="line"><span class="comment"># Assign train/val datasets for use in dataloaders</span></span><br><span class="line"><span class="keyword">if</span> self.dataset == <span class="string">'TAU'</span>:</span><br><span class="line"><span class="keyword">if</span> stage == <span class="string">'fit'</span>:</span><br><span class="line">self.trainset = DeltaDataset(self.train_csv, self.fea_path)</span><br><span class="line">self.valset = DeltaDataset(self.val_csv, self.fea_path)</span><br><span class="line"><span class="keyword">if</span> stage == <span class="string">'test'</span>:</span><br><span class="line">self.testset = DeltaDataset(self.test_csv, self.fea_path)</span><br><span class="line"><span class="keyword">if</span> self.dataset == <span class="string">'CAS'</span>:</span><br><span class="line"><span class="keyword">if</span> stage == <span class="string">'fit'</span>:</span><br><span class="line">self.trainset = CASDeltaDataset(self.train_csv, self.fea_path)</span><br><span class="line">self.valset = valdataset(self.val_csv, self.fea_path)</span><br><span class="line">self.unlabelset = unlabeled_CASDeltaDataset(self.unlabel_csv,                                             self.fea_path)</span><br><span class="line">self.iteration = self.unlabelset.__len__()//self.batch_size</span><br><span class="line"><span class="keyword">if</span> stage == <span class="string">'test'</span>:</span><br><span class="line">self.testset = valdataset(self.test_csv, self.fea_path)</span><br></pre></td></tr></tbody></table></figure><p></p><h2 id="model"><a href="#model" class="headerlink" title="model"></a>model</h2><p>model_interface需要做的不仅是包装模型，训练和验证的过程也包含在内，作为<code>LightningModule</code>的子类<code>MInterface</code>的类函数。<br>该类中的几个hook函数如下（不全）<br></p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MInterface</span>(pl.LightningModule):</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, model_name:<span class="built_in">str</span>, lr:<span class="built_in">float</span>, mode:<span class="built_in">str</span>, **kargs</span>):</span><br><span class="line"><span class="built_in">super</span>().__init__()</span><br><span class="line"><span class="comment"># 这个可以保存每次训练的超参数到yaml文件，很好用</span></span><br><span class="line">self.save_hyperparameters()</span><br><span class="line">self.init_model()</span><br><span class="line">self.configure_loss()</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">init_model</span>(<span class="params">self</span>):</span><br><span class="line"><span class="comment"># 初始化模型，看参数然后给self.model一个模型实例</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line"><span class="keyword">return</span> self.model(x)[<span class="string">'logits'</span>]</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">configure_loss</span>(<span class="params">self</span>):</span><br><span class="line"><span class="comment"># 配置损失函数，因为训练用到了很多个损失，就搞了个字典存</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">configure_optimizers</span>(<span class="params">self</span>):</span><br><span class="line"><span class="comment"># 看上面的链接，这里可以返回一个优化器，或者优化器和scheduler的列表</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">training_step</span>(<span class="params">self, batch, batch_idx</span>):</span><br><span class="line"><span class="comment"># mixmatch和pretrain的过程的混合，用ifelse区分</span></span><br><span class="line"><span class="comment"># 其实我觉得分别再写两个类函数区分两种训练过程</span></span><br><span class="line"><span class="comment"># 然后在这里调用两个类函数会更好</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">validation_step</span>(<span class="params">self, batch, batch_idx</span>):</span><br><span class="line"><span class="comment"># 验证过程</span></span><br></pre></td></tr></tbody></table></figure><p></p><p>对于训练过程中需要记录的loss等信息，可以使用自带的log，会调用定义Trainer时传入的logger，有两种方法，单值log和字典log<br></p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">self.log_dict(log_info, on_step=<span class="literal">True</span>, prog_bar=<span class="literal">True</span>, on_epoch=<span class="literal">True</span>, logger=<span class="literal">True</span>)</span><br><span class="line">self.log(<span class="string">"pretrain_loss"</span>, loss, on_step=<span class="literal">True</span>, on_epoch=<span class="literal">True</span>, prog_bar=<span class="literal">True</span>, logger=<span class="literal">True</span>)</span><br></pre></td></tr></tbody></table></figure><br>当参数 <code>prog_bar=True</code>时，会把这些信息打印到进度条后面，但是有一说一挺丑的。<p></p><h2 id="main"><a href="#main" class="headerlink" title="main"></a>main</h2><p>定义好两个interface时候，在main函数只需要先根据参数定义两个interface的实例，然后：<br></p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">data_loder = ...</span><br><span class="line">model = MInterface(model_name=args.model, **<span class="built_in">vars</span>(args))</span><br><span class="line">logger = CSVLogger(<span class="string">'./'</span>, <span class="string">'logs'</span>)</span><br><span class="line">trainer = Trainer(accelerator=<span class="string">'cuda'</span>, devices=[<span class="number">1</span>], fast_dev_run=<span class="literal">False</span>, max_epochs=max_epochs, logger=logger)</span><br><span class="line">trainer.fit(model, data_loader)</span><br></pre></td></tr></tbody></table></figure><p></p><p>有几个地方要注意</p><ul><li>Trainer的参数中，accelerator就是训练用的device，后面的devices有如下几种情况<ul><li>使用k个设备训练<code>devices=k</code>(<font color="red">很重要，是大坑</font>)</li><li>使用第k个设备训练<code>devices=[k]</code>，也可以在列表中定义多个设备</li></ul></li><li>Trainer中的<code>fast_dev_run</code>很好用，设为<code>True</code>后会把训练验证测试先按照<code>batch=1</code>跑一轮，验证程序准确性，防止跑完训练验证代码出问题这种很傻逼的情况。</li></ul>]]></content>
      
      
      <categories>
          
          <category> 技术问题记录 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> pytorch_lighting </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>StegaDDPM</title>
      <link href="/2023/11/30/StegaDDPM/"/>
      <url>/2023/11/30/StegaDDPM/</url>
      
        <content type="html"><![CDATA[<h1 id="StegaDDPM"><a href="#StegaDDPM" class="headerlink" title="StegaDDPM"></a>StegaDDPM</h1><blockquote><p>In addition, it can securely conceal and accurately extract secret messages up to 9 bits per pixel.</p></blockquote><p>汗流浃背了，一个像素的信息量也没有9比特吧</p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>隐写有基于嵌入的隐写和无需嵌入的隐写</p><h2 id="Proposed-Approach"><a href="#Proposed-Approach" class="headerlink" title="Proposed Approach"></a>Proposed Approach</h2><p><img src="/2023/11/30/StegaDDPM/image-20231205101028145.png" alt=""></p><h3 id="基于DDPM的隐写网络"><a href="#基于DDPM的隐写网络" class="headerlink" title="基于DDPM的隐写网络"></a>基于DDPM的隐写网络</h3><p>又讲了一遍Diffusion<br>StegaDDPM分析了DDPM的逆生成过程来实现生成式图像隐写，并推导出适合于隐写的两个属性。详情如下</p><ul><li>消噪扩散概率模型定义了扩散步骤的马尔可夫链。</li><li>Z的分量服从高斯分布，其维数与生成的图像的维数相等。<br>算法：<br><img src="/2023/11/30/StegaDDPM/image-20231206145636034.png" alt=""><br>和传统DDPM模型生成图像的不同：初始采样噪声（即 $t=T$ 时）使用约定的 $Seed_1$ 生成，中间的去噪过程使用 $Seed_2$ 作为Diffusion去噪条件（类比条件诱导生成中的条件特征向量？），在最后一步使用一个残差 $R_1$ 作为嵌入信息，得到隐写图片 $X_0^S$</li></ul><h3 id="信息隐藏和提取"><a href="#信息隐藏和提取" class="headerlink" title="信息隐藏和提取"></a>信息隐藏和提取</h3><p><strong>隐藏过程</strong><br>秘密信息 $m$ ，长 $L$， 使用 $K$ 先加密得到 $m^e$<br>分成 c 组，表示为 $m^e=\{s_1,s_2,\cdots s_c\}$ 每个组长 $l$<br><img src="/2023/11/30/StegaDDPM/image-20231205114833057.png" alt=""><br>嵌入思路和之前李博讲的一样，和diffusion没有什么关系<br>将残差向量 $R_1$ 的每个分量 $r_i$ 的概率密度函数的横坐标划分为 $2^l$ 个区间，每个区间的概率积分为 $\frac{1}{2^l}$<br>对于每个 $s_i=[b_l,b_{l-1}\cdots b_1]$ ，将其看成小数，小数点在最前面（比如 101 就是 $0.5+0+0.125$），记作 $s_i^d$<br>最后，每个残差分量由如下公式得到：</p><script type="math/tex; mode=display">r_i=F^{-1}(s^d_i + \frac{rand(0,1)}{2^l})</script><p>其中 $F^{-1}(\cdot)$ 是高斯分布的概率分布函数的逆函数，图中最右边</p><p><strong>提取过程</strong><br>按照约定号的随机数种子 $Seed_1,Seed_2$ 使用相同的模型执行DDPM逆过程，得到 $X_1$<br>$R_1 = X_1 - X_0^S$<br>对于每个分量 $r_i$ 令 $s_i^{d’}=F(r_i)$ ，取前 $l$ 比特就是嵌入信息 $s_i$ </p><h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><ul><li>RGB图片每个通道的像素值不是 0~255 的8字节值吗，上面提到的都是小数（和最前面的提问一样</li><li>需要交换随机数种子，相当于固定住了diffusion的噪声</li></ul>]]></content>
      
      
      <categories>
          
          <category> 论文笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 隐写 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>AudioLDM2</title>
      <link href="/2023/11/28/AudioLDM2/"/>
      <url>/2023/11/28/AudioLDM2/</url>
      
        <content type="html"><![CDATA[<h1 id="AudioLDM2"><a href="#AudioLDM2" class="headerlink" title="AudioLDM2"></a>AudioLDM2</h1><p>作者：Haohe Liu, Qiao Tian, Yi Yuan, Xubo Liu, Xinhao Mei, Qiuqiang Kong, Yuping Wang, Wenwu Wang, Yuxuan Wang, Mark D. Plumbley</p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>做Audio generation的大一统工作（有空可以看看这里提到的几篇）</p><blockquote><p>Recent advancements in addressing problems from a unified perspective have yielded substantial progress [16]– [19]. This trend highlights the potential of constructing a unified audio generation framework.</p></blockquote><p>引入了 language of audio(LOA)，作为音频片段的特征表示，这个片段需要能够表示细粒度和粗粒度的音频信息，考虑到这些需求，我们建议使用 audio masked autoencoder(AudioMAE, 一个预训练框架) 来提取这些特征。<br>具体用的是预训练的GPT2，减少计算量<br>总的来说，贡献如下：</p><ul><li>提出了一个新颖的通用的音频生成模型，能够执行音频，音乐和可理解语音的条件生成。</li><li>该方法基于音频的通用表示，可以在没有音频注释的情况下对LDM进行大规模自监督预训练，并有助于将自回归模型的优点结合起来</li></ul><h2 id="AudioLDM2-1"><a href="#AudioLDM2-1" class="headerlink" title="AudioLDM2"></a>AudioLDM2</h2><p><img src="/2023/11/28/AudioLDM2/image-20231130150850706.png" alt=""><br>总体架构如图，与第一代相比，框架变大了很多。首先是由于大统一特征 LOA 的引入，对于Audio和其他模态的信息需要更多的处理方法，图中分为两部分， $\mathcal{A}(\cdot)$ 为Audio to LOA ENcoder，将原始音频编码为原始大一统特征 $Y$； $\mathcal{M}(\cdot)$ 为 Any modality to LOA Translator，将其他模态的信息编码为对原始大一统特征的估计 $\hat Y$ 。</p><p>之后的做法和第一代类似，就是送入Diffusion用于生成VAE的隐变量，然后VAE重建梅尔谱图。不过Diffusion和VAE的结构和第一代都不同。</p><h3 id="AudioMAE-Encoder"><a href="#AudioMAE-Encoder" class="headerlink" title="AudioMAE Encoder"></a>AudioMAE Encoder</h3><p>是一个和 Vision Transformer（ViT）类似的架构的自监督预训练模型。同其他自监督预训练模型相比，有两个优点</p><ul><li>被证实在综合的音频领域内工作的很好，就是不细分特定领域，比如 speech 或者 music</li><li>使用mask后重建的损失相比以前以对比损失或预测分类损失为目标的方法，在生成任务中可能更好。<br>对于一段音频信号 $x$ 首先转换为梅尔谱图 $X\;\in\;\mathbb{R}^{T\times F}$ ，然后当作图片的处理方式，分成 $P\times P$ 大小的 patch 作为 AudioMAE encoder 的输入，每个patch会得到一个 $D$ 维的嵌入，一段音频经过encoder 得到的特征为 $E\;\in\;\mathbb{R}^{T’\times F’\times D}$ ，最后经过一个二维的 pooling 降维，得到 LOA 特征</li></ul><h3 id="Conditioning-Information-to-LOA-Translation-with-GPT-2"><a href="#Conditioning-Information-to-LOA-Translation-with-GPT-2" class="headerlink" title="Conditioning Information to LOA Translation with GPT-2"></a>Conditioning Information to LOA Translation with GPT-2</h3><p>这部分是要生成 $\hat Y$ ，这里把这个特征当成一个 language modelling task，把特征 $Y$ 视为长度为 $L$ 的序列，在训练过程中，GPT2 根据给定的多模态条件 $C$ 和 之前的真实序列 $y_1,\dots,y_{l-1}$ 去预测序列中的第 $l$ 个向量。优化目标为最大似然估计</p><p>然后讲多模态的特征用不同的专家模型来处理</p><h3 id="Latent-Diffusion-Model"><a href="#Latent-Diffusion-Model" class="headerlink" title="Latent Diffusion Model"></a>Latent Diffusion Model</h3><p>这里同第一代类似，但是用的是Transformer-UNet，是在原始的UNet的卷积后面加一层Transformer，为了将原始的LOA中的条件信息合并，最后一个transformer块把self-attention换成了cross-attention，把LOA作为K和V，并将前一个transformer块的输出作为Q。</p><p>然后上面结构图里有一个Prob Switcher，在训练过程中，这个开关会从两个特征中以概率 $P_{gt},P_{pred}$ 随机选取AudioMAE生成的原始特征和GPT2生成的估计特征</p>]]></content>
      
      
      <categories>
          
          <category> 论文笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 论文 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CLAP</title>
      <link href="/2023/11/20/CLAP/"/>
      <url>/2023/11/20/CLAP/</url>
      
        <content type="html"><![CDATA[<h1 id="CLAP-LEARNING-AUDIO-CONCEPTS-FROM-NATURAL-LANGUAGE-SUPERVISION"><a href="#CLAP-LEARNING-AUDIO-CONCEPTS-FROM-NATURAL-LANGUAGE-SUPERVISION" class="headerlink" title="CLAP : LEARNING AUDIO CONCEPTS FROM NATURAL LANGUAGE SUPERVISION"></a>CLAP : LEARNING AUDIO CONCEPTS FROM NATURAL LANGUAGE SUPERVISION</h1><p>作者：Benjamin Elizalde, Soham Deshmukh, Mahmoud Al Ismail, Huaming Wang<br>机构：微软<br>发表情况：未发表</p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>注意到在这之前，也有人用类似的方法训练，Wav2clip和Audioclip从CLIP中提取，并且使用AS中的音频和类别标签训练而不是自然语言（应该就是这里的区别）<br>CLAP使用和CLIP一样的方法，两个Encoder，通过对比学习将音频和文本描述投射到一个联合的多模态空间</p><h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><p><img src="/2023/11/20/CLAP/image-20231121104629378.png" alt=""></p><p>和图中描述的一样<br>首先需要选定一个大小为 $N$ 的 batch，就是图中做矩阵乘法的两个向量组的大小。<br>图里省略了两个线性投影层<br>audio是以梅尔谱图的形式作为输入的</p><script type="math/tex; mode=display">\hat X_a = f_a(X_a);\;\hat X_t = f_t(X_t)</script><p>其中 $\hat X_a \in \mathbb R ^{N\times V},\hat X_t \in \mathbb R ^{N\times U}$ ，也就是说，两个特征向量在经过encoder之后的形状不是一样的，所以加了两个线性投影层，将两个特征表示投影到 $\mathbb R ^{N\times d}$ , 得到 $E_a,E_t$<br>相似性矩阵定义为：</p><script type="math/tex; mode=display">C = \tau *(E_t \cdot E_a^\top)</script><p>$\tau$ 是温度参数，用来缩放logits<br>损失函数定义为：</p><script type="math/tex; mode=display">\begin{aligned}\mathcal L = 0.5*(\ell_{text}(C) + \ell_{audio}(C) )\newline\ell_k = \frac 1 N \sum^N_{i=0} \log diag(softmax(C))\end{aligned}</script><p>实验不看了，看另一个pipeline</p><h1 id="Large-scale-Contrastive-Language-Audio-Pretraining-with-Feature-Fusion-and-Keyword-to-Caption-Augmentation"><a href="#Large-scale-Contrastive-Language-Audio-Pretraining-with-Feature-Fusion-and-Keyword-to-Caption-Augmentation" class="headerlink" title="Large-scale Contrastive Language-Audio Pretraining with Feature Fusion and Keyword-to-Caption Augmentation"></a>Large-scale Contrastive Language-Audio Pretraining with Feature Fusion and Keyword-to-Caption Augmentation</h1><p>作者：Yusong Wu, Ke Chen 2, Tianyu Zhang 1, Yuchen Hui, Taylor Berg-Kirkpatrick, Shlomo Dubnov<br>机构：Mila, Quebec Artificial Intelligence Institute, Universite de Montr ´ eal ´<br>University of California San Diego 3LAION<br>发表情况：未发表</p><p>AudioLDM的CLAP是按照这篇论文提出的流水线进行构造的<br>本文的贡献：</p><ul><li>发布了LAION-Audio-630K数据集</li><li>构建了 <code>contrastive language-audio pretraining</code> 的流水线，使用了两个audio encoder和三个text encoder进行测试，采用特征融合机制来提高性能，使模型能够处理变长输入</li><li>下游任务测试，在文本音频搜索和音频分类任务中达到了SOTA效果</li></ul><h2 id="Model-Architecture"><a href="#Model-Architecture" class="headerlink" title="Model Architecture"></a>Model Architecture</h2><p><img src="/2023/11/20/CLAP/image-20231121160747431.png" alt=""></p><p>总体和微软提出的CLAP类似，不过表述不同，最后特征维度转换的线性投影层就是一个多层感知机。<br>上面提到的损失函数展开就是下面这个形式（太长了不想打<br><img src="/2023/11/20/CLAP/image-20231121162154009.png" alt=""></p><p>可以看到，为了匹配不同长度的输入，对音频在进入Encoder之前，有做一些预处理<br>对于长度为 $Ts$ 的音频和一个固定的窗口长度 $d = 10s$ </p><ul><li>$T\leq d:$ 先重复再0扩充</li><li>$T&gt;d:$ 首先将输入T下采样到 d 秒作为一个全局的输入（什么意思啊，为什么下采样还能改时间长度）<ul><li>看了下代码，是 <code>mel_shrink = torchvision.transforms.Resize(size=[chunk_frames, 64])(mel[None])[0]</code>这个东西实现的</li><li>然后随机的分三个 d 秒的块，作为局部输入，将这四个 d 秒的输入送入第一层得到初始化特征，然后三个局部特征会被加工成一个特征，最后局部特征和全局特征会被融合为：<script type="math/tex; mode=display">\begin{aligned}X^a_{fusion}&=\alpha X^a_{global} + (1-\alpha)X^a_{local}\newline\alpha &= f_{AFF}(X^a_{global},X^a_{local})\end{aligned}</script>特征融合机制具体由下面的步骤实现<br><img src="/2023/11/20/CLAP/image-20231122094914332.png" alt=""></li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> 论文笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 论文 </tag>
            
            <tag> CLAP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>AudioLDM</title>
      <link href="/2023/11/13/AudioLDM/"/>
      <url>/2023/11/13/AudioLDM/</url>
      
        <content type="html"><![CDATA[<h1 id="AudioLDM"><a href="#AudioLDM" class="headerlink" title="AudioLDM"></a>AudioLDM</h1><p>作者：Haohe Liu, Zehua Chen, Yi Yuan, Xinhao Mei, Xubo Liu, Danilo Mandic, Wenwu Wang, Mark D. Plumley<br>机构：CVSSP, University of Surrey, Guildford, UK；Department of EEE, Imperial College London, London, UK<br>发表情况： ICML 2023</p><h2 id="学习过程参考的相关文章"><a href="#学习过程参考的相关文章" class="headerlink" title="学习过程参考的相关文章"></a>学习过程参考的相关文章</h2><p><a href="https://zhuanlan.zhihu.com/p/638442430">Diffusion Model</a></p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>贡献：</p><ul><li>第一次将连续的隐扩散模型（LDM）应用于TTA生成，且取得了SOTA效果</li><li>使用了CLAP嵌入使TTA生成的训练不用依赖音频文本对</li><li>实验证明了在LDM训练中只使用音频数据可以得到高质量和高计算效率的TTA系统</li><li>展示了提出的TTA系统能够在未经过微调的情况下进行文本引导的音频风格操作，比如音频风格迁移，高分辨率生成，音频修复。</li></ul><p><img src="/2023/11/13/AudioLDM/image-20231211115457402.png" alt=""></p><h2 id="Text-Conditional-Audio-Generation"><a href="#Text-Conditional-Audio-Generation" class="headerlink" title="Text-Conditional Audio Generation"></a>Text-Conditional Audio Generation</h2><h3 id="Contrastive-Language-Audio-Pretraining-CLAP"><a href="#Contrastive-Language-Audio-Pretraining-CLAP" class="headerlink" title="Contrastive Language-Audio Pretraining (CLAP)"></a>Contrastive Language-Audio Pretraining (CLAP)</h3><p>参考图像文本预训练（CLIP），提出了CLAP<br>分别由一个text encoder和一个audio encoder，分别提取文本和音频的嵌入特征，两个encoder的架构分别为</p><ul><li>text: RoBERTa</li><li>audio: HTSAT<br>以对称交叉熵损失作为训练目标，将音频和文本特征对齐到同一个嵌入空间。</li></ul><h3 id="Conditional-Latent-Diffusion-Models"><a href="#Conditional-Latent-Diffusion-Models" class="headerlink" title="Conditional Latent Diffusion Models"></a>Conditional Latent Diffusion Models</h3><p>条件隐扩散模型<br>这部分有点绕，先翻译一下<br>TTA系统可以给定一个文本描述 $y$ 生成一个音频样本 $\hat x$ 。在有概率生成模型LDMs的情况下，我们使用模型分布 $p_{\theta}(z_0|E^y)$ 评估了真实的条件数据分布 $q(z_o|E^y)$ ，其中 $z_0 \in \mathbb{R}^{C\times \frac{T}{r} \times \frac{F}{r}}$ 是音频样本 $x$ 在由 $X \in \mathbb{R}^{T\times F}$ 梅尔谱压缩表示组成向量空间的前置，$E^y$ 是由CLAP预训练的文本encoder得到的文本嵌入。$r$ 表示压缩等级，C表示压缩表示的通道数，T 和 F 表示梅尔谱图 X 的时域频域维数。通过预训练的CLAP联合嵌入音频和文本信息，音频嵌入 $E^x$ 和文本嵌入 $E^y$ 共享一个联合的跨模态空间。这允许我们使用 $E^x$ 训练LDMs，同时将 $E^y$ 用于TTA生成。</p><p>这部分的重点在这两个概率分布模型  $p_{\theta}(z_0|E^y)$ 和 $q(z_o|E^y)$ ，其中 $q(z_o|E^y)$ 是真实分布，也就是我们拟合的目标，这是一个条件概率模型，条件是 $E^y$ ，也就是文本描述的嵌入，随机变量是 $z_0$ ，原文描述为</p><blockquote><p>$z_0 \in \mathbb{R}^{C\times \frac{T}{r} \times \frac{F}{r}}$ is the prior of an audio sample $x$ in the space formed from the compressed representation of the mel-spectrogram  $X \in \mathbb{R}^{T\times F}$ </p></blockquote><p>关键是 <code>the prior of an audio sample</code> 就把它当作一个用于重建音频的隐变量就好了，至于是怎么还原的之后应该会提到。<br>所以这个要拟合的分布，就是给定文本描述的情况下，符合文本描述的音频的分布</p><p>然后讲本文用到的扩散模型，基本和参考文章中的扩散模型一样，首先分为前向过程和反向过程，前向过程一层一层加噪声，反向过程一层一层去噪声。<br><strong>forward process</strong><br>预定义一个噪声尺度 $0 &lt; \beta_1 &lt; \cdots &lt; \beta_n &lt; \cdots &lt; \beta_N &lt;1$ ，共做 $N$ 次加噪过程，最终会将数据分布转换为一个标准高斯分布</p><script type="math/tex; mode=display">\begin{aligned}q(z_n|z_{n-1}) &= \mathcal{N}(z_n;\sqrt{1-\beta_n} \; z_{n-1},\beta_n \boldsymbol I)\qquad &(1)\newlineq(z_n|z_0) &= \mathcal{N}(z_n;\sqrt{\bar \alpha_n}\;z_0,(1-\bar \alpha_n)\boldsymbol \epsilon)\qquad &(2) \end{aligned}</script><p>这里的 $\boldsymbol \epsilon \sim \mathcal N (0,I)$ 表示输入噪声，$\alpha_n = 1-\beta_n$ 是构造参数的技巧，让式子更简洁，$\bar \alpha_n := \prod ^n_{s=1}\alpha_s$ 表示每一步的噪声水平。最后一步 $N$ ，$z_N \sim \mathcal N(0,I)$ 是一个标准正态高斯分布</p><p>对于模型优化，使用调整过权重的噪声评估训练目标（在参考文章中有相关证明）</p><script type="math/tex; mode=display">L_n(\theta)=\mathbb E_{z_0,\epsilon,n}\Vert\epsilon-\epsilon_\theta(z_n,n,E^x)\Vert^2_2 \qquad (3)</script><p>其中，$E^x$ 是由audio encoder生成的 音频波形 $x$ 的音频嵌入。<br>在反向过程中，从高斯分布 $p(z_N)\sim \mathcal N(0,I)$ 和文本嵌入 $E^y$ 开始，以 $E^y$ 为条件的去噪过程通过以下过程逐渐产生音频先验（audio prior） $z_0$</p><script type="math/tex; mode=display">\begin{aligned}p_{\theta}(z_{0:N}|E^y) &= p(z_N)\prod^N_{t=n}p_\theta(z_{n-1}|z_n,E^y) \qquad &(4)\newlinep_\theta(z_{n-1}|z_n,E^y) &= \mathcal N (z_{n-1};\mu_\theta(z_n,n,E^y),\sigma^2_nI)\qquad &(5)\end{aligned}</script><p>公式（4）是对整个过程的一个描述，从高斯分布的采样 $p(z_N)$ 开始，连乘 N 个去噪的概率分布得到生成的结果，其中，每一步的去噪过程，可以定义为给定阶段 $n$ 的带噪隐变量 $z_n$ 和文本嵌入 $E^y$ ，加噪声前一阶段 $z_{n-1}$ 的概率分布。</p><p>反向过程在参考文章中给了很清晰的解释</p><blockquote><p>在正向过程中，我们人为设置了 $T$ 步加噪声过程。而在反向过程中，我们希望能够倒过来取消每一步加噪声操作，让一幅纯噪声图像变回数据集里的图像。这样，利用这个去噪声过程，我们就可以把任意一个从标准正态分布里采样出来的噪声图像变成一幅和训练数据长得差不多的图像，从而起到图像生成的目的。<br>现在问题来了：去噪声操作的数学形式是怎么样的？怎么让神经网络来学习它呢？数学原理表明，当 $\beta_t$ 足够小时，每一步加噪声的逆操作也满足正态分布。</p></blockquote><p>重点：<strong>直接计算所有数据的加噪声逆操作的分布是不太现实的</strong>，但是，如果给定了某个训练集输入 $x_0$，多了一个限定条件后，该分布是可以用贝叶斯公式计算的</p><script type="math/tex; mode=display">\begin{aligned}q(x_{t-1}|x_t,x_0) &= \frac{q(x_{t-1},x_t,x_0)}{q(x_t,x_0)} \newline&=\frac{q(x_t,x_{t-1},x_0)}{q(x_0)q(x_t|x_0)} \newline&=\frac{q(x_t|x_{t-1},x_0)q(x_{t-1},x_0)}{q(x_0)q(x_t|x_0)} \newline&=\frac{q(x_t|x_{t-1},x_0)q(x_{t-1}|x_0)q(x_0)}{q(x_0)q(x_t|x_0)}\newline&=q(x_t|x_{t-1},x_0)\frac{q(x_{t-1}|x_0)}{q(x_t|x_0)}\end{aligned}</script><p>等式右边的所有东西都是正向过程中已知的，代入可以计算得到等式左边的均值和方差，具体的推导过程见参考文章</p><p>回到论文，得到去噪的过程的均值和方差如下</p><script type="math/tex; mode=display">\begin{aligned}\mu_\theta(z_n,n,E^y)&=\frac{1}{\sqrt{\alpha_n}}(z_n-\frac{\beta_n}{\sqrt{1-\bar \alpha_n}}\epsilon_\theta(z_n,n,E^y)) \qquad&(6) \newline\sigma^2_n &= \frac{1-\bar\alpha_{n-1}}{1-\bar\alpha_n}\beta_n \qquad&(7)\end{aligned}</script><p>本文的 $\epsilon_\theta$ 和 $DDPM$ 的 $\epsilon$ 好像不是一个东西，DDPM的是从标准高斯中采样的，而这里文章中说这是预测的生成噪声，应该是原始的diffusion没有text prompt这个限制，是随机生成图片，而这里是有一个文本嵌入 $E^y$ 在限制</p><p>在训练阶段，学习给定音频样本 $x$ 的跨模态表示 $E^x$ 的audio prior $z_0$ 的生成。然后在TTA生成，提供文本嵌入 $E^y$ 来预测噪声 $\epsilon_\theta$<br>建立在CLAP嵌入的基础上，LDM可以不需要文本监督也能理解TTA生成</p><h3 id="Conditioning-Augmentation"><a href="#Conditioning-Augmentation" class="headerlink" title="Conditioning Augmentation"></a>Conditioning Augmentation</h3><p>条件作用增强<br>在图像生成中，可以捕捉到图像间微小的差别，其中一个原因是大量的language-image样本对，但是音频没有这么多样本。本文的方法在训练LDMs的时候不需要文本样本，所以数据增强只需要音频，通过以下方式对音频样本 $x_1,x_2$ 进行混频增强</p><script type="math/tex; mode=display">x_{1,2}=\lambda x_1 + (1-\lambda)x_2 \qquad(8)</script><p>其中 $\lambda$ 是从beta分布 $\mathcal B(5,5)$ 中采样的0到1之间的数</p><h3 id="Classifier-free-Guidance"><a href="#Classifier-free-Guidance" class="headerlink" title="Classifier-free Guidance"></a>Classifier-free Guidance</h3><p>用于指导扩散模型的生成。<br>在训练过程中，使用固定的概率随机丢弃条件 $E^x$ 来同时训练条件LDMs $\epsilon_\theta(z_n,n,E^x)$ 和非条件LDMs $\epsilon_\theta(z_n,n)$ 。生成的时候，使用文本嵌入 $E^y$ 作为条件，并使用改进的噪声估计 $\hat \epsilon_\theta(z_n,n,E^y)$ 进行采样：</p><script type="math/tex; mode=display">\hat \epsilon_\theta(z_n,n,E^y) = w\epsilon_\theta(z_n,n) + (1-w)\epsilon_\theta(z_n,n,E^y) \qquad (9)</script><p>其中 $w$ 决定指导尺度<br>看起来就是俩系数，分别决定条件LDMs和非条件LDMs在预测噪声中占的比重，非条件LDMs就是原始的Diffusion的那个正态采样噪声</p><h3 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h3><p>主要有两部分，VAE用作隐变量到梅尔频谱图的转换，HiFi-GAN用作梅尔频谱图到音频样本的重建<br>训练过程，使用VAE将梅尔频谱图 $X \in \mathbb R^{T\times F}$ 压缩进一个小的隐空间 $z\in \mathbb R^{C\times \frac R r \times \frac F r}$ ，其中 $r$ 是压缩等级。VAE的encoder和decoder采用堆叠的卷积块。<br>训练损失包括三个部分：重建损失，对抗损失，高斯约束损失（后面补充）<br>另外，和LDMs的条件增强类似，这里也使用了公式（9）的数据扩充方式</p><h2 id="Text-Guided-Audio-Manipulation"><a href="#Text-Guided-Audio-Manipulation" class="headerlink" title="Text-Guided Audio Manipulation"></a>Text-Guided Audio Manipulation</h2><p><strong>Style Transfer</strong><br>什么玩意儿<br>给定一个原音频样本 $x^{src}$ 我们可以使用公式（2）计算它的预定义时间步长 $n_0&lt;N$ 的噪声潜在表示 $z_{n_0}$<br>使用 $z_{n_0}$ 作为预训练AudioLDM模型逆过程的开始点，我们通过一个 <code>shallow reverse process</code> $p_\theta (z_{0:n_0}|E^y)$ 来通过文本输入 $y$ 操纵音频 $x^{src}$ </p><script type="math/tex; mode=display">p_\theta (z_{0:n_0}|E^y) = p(z_{n_0})\prod^{n_0}_{n=1}p_\theta (z_{n-1}|z_n,E^y) \qquad \qquad(10)</script><p>其中 $n_0$ 控制操作结果，如果我们定义一个 $n_0\approx N$ ，源音频提供的信息将不会被保留，操作将类似于TTA生成。<br>这是个什么东西呢<br>观察公式（10）和（4），他们的差别在于隐变量 $z$ 的下标，完整的LDMs过程，是如公式（4）所描述的，<code>源音频-&gt;高斯噪声-&gt;由文本条件引导的重建音频</code>。而公式（10）的过程是不完全的LDMs过程，<code>源音频-&gt;叠加了部分噪声的隐空间表示-&gt;由文本条件引导的重建音频</code>。这种处理方式会根据 $n_0$ 的选择保留源音频包含的大部分信息，只让文本描述起到修改的作用。</p><p><strong>Inpainting and Super-Resolution</strong><br>chatgpt的解释：<br>Inpainting：是一种用于修复或恢复信号中缺失或损坏部分的技术。在音频生成领域，这可能涉及到填补音频信号中的缺失段落，使其在听觉上更加连贯和完整<br>Super-Resolution：是一种用于增强图像或信号分辨率的技术，通常通过提高像素级别的细节来实现。在音频领域，这可以被理解为通过算法或模型增加音频信号的采样率或提高其频率分辨率，以获得更高质量的音频。这有助于使音频更清晰，更富有细节，提高听觉上的感知质量。</p><p>我们通过将观察到的部分的潜在表示 $z^{ob}$ 合并到生成的潜在表示 $z$ 中来探索这些任务<br>具体来说，在反向过程中，从 $p(z_N) \sim \mathcal N(0,I)$ 开始，在由公式（5）的每次推断之后，用下面的公式修改生成的 $z_{n-1}$ </p><script type="math/tex; mode=display">z_{n-1}' = (1-m) \odot z_{n-1} + m \odot z^{ob}_{n-1}\qquad \qquad (11)</script><p>  其中 $z’$ 是修改过的隐空间表示，$m\in \mathbb R^{\frac T r \times \frac F r}$ 表示一个隐空间观测mask（是什么东西），$z^{ob}_{n-1}$ 是对 $z^{ob}$ 进行前向过程加噪声得到的<br>  这个 $m$ 就是把梅尔谱图中的观测部分置为1，未观测部分置为0，然后上面的公式中，$(1-m)\odot z_{n-1}$ 就是保留反向过程中未观测到的部分，而 $z^{ob}_{n-1}$ 是未经过 $E^y$ 指导的，由原始音频正向生成的隐变量，直观来看就是在生成过程中，每去一次噪声，就把观察到的部分的预测值换为原始观测值的加噪部分，只让模型去生成未被观测到的部分。</p><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p><strong>Training dataset</strong><br>AudioSet, AudioCaps, Freesound, BBC Sound Effect library<br><strong>Evaluation dataset</strong><br>AC, AS把标签串起来</p><p><strong>评估方法</strong><br>客观方法：</p><ul><li>frechet distance（FD）：与图像生成中的frechet起始距离类似，音频中的FD表示生成的样本与目标样本之间的相似度</li><li>Inception score（IS）：用于评估采样质量和多样性的指标</li><li>KL散度：KL在成对样本水平上测量，并作为最终结果平均。</li></ul><p>主观方法：<br>叫了6个<code>audio professionsals</code>评估两个指标</p><ul><li>overall quality(OVL)，总体质量</li><li>relevance to the input text(REL)，和输入文本的相关性</li></ul><p><strong>模型</strong><br>对比模型：DiffSound，AudioGen<br>训练了两个AudioLDM</p><ul><li>AudioLDM-S 181M参数</li><li>AudioLDM-L 739M参数</li></ul><h3 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h3><p><img src="/2023/11/13/AudioLDM/image-20231117151704510.png" alt=""></p><p>虽然RoBERTa和CLAP具有相同的文本编码器结构，但CLAP的优势在于它将音频-文本关系学习与生成模型训练解耦。这种分离是直观的，因为CLAP已经通过对齐音频和文本的嵌入空间来建模音频和文本之间的关系。<br>用BERT既需要学习音频文本之间的关系，又需要学习生成相关的知识，而CLAP不需要，因为它已经对其了音频文本向量空间</p><p>在音乐上的合成质量评估：<br><img src="/2023/11/13/AudioLDM/image-20231117155505639.png" alt=""></p><p><strong>Conditioning Information</strong><br>在训练LDM的时候没有用到文本特征，一个很自然的问题，如果用了文本特征效果会不会更好？然后发现不会<br><img src="/2023/11/13/AudioLDM/image-20231117155906174.png" alt=""><br>作者认为导致这个结果的主要原因是文本嵌入对生成目标的描述没有音频嵌入这么好，由于音频的复杂性和模糊性，文本描述很难做到准确和全面。下面这个图是采样质量和训练步数的函数，可以看到在整个训练过程中，音频嵌入的训练效果明显好于文本嵌入;较大的模型收敛速度较慢，但最终性能较好。<br><img src="/2023/11/13/AudioLDM/image-20231117160627718.png" alt=""></p><p>问题，我把前面的忘了，这个文本嵌入和音频嵌入是放在哪个地方的来着</p><p>压缩率：r<br><img src="/2023/11/13/AudioLDM/image-20231117161337173.png" alt=""><br>r = 1，2的时候，单张RTX3090跑不动，实验默认用r=4</p><p><strong>Text-Guided Audio Manipulation</strong><br>对super-resolution任务，使用了两个模型 AudioUNet 和 NVSR 作为baseline，使用log-spectral distance (LSD）作为评价指标<br>对于Inpainting 任务，使用FAD作为指标，并作为这一任务的新baseline<br><img src="/2023/11/13/AudioLDM/image-20231117163359764.png" alt=""><br>实验结果中，AudioLDM的效果比AudioUNet好，但是不如NVSR。<br>作者认为这是因为AudioLDM的训练数据里包含很多噪声，这可能会导致在超分辨率过程的输出中出现白噪声或其他非语音声音事件，从而潜在地降低性能。<br>最后给出展望，我们的贡献给了一个新的方式，以zero-shot的方式，使用TTA系统完成文本引导的音频处理任务</p><h3 id="消融实验"><a href="#消融实验" class="headerlink" title="消融实验"></a>消融实验</h3><p>对三个部分做了消融实验<br>简化UNet的注意力机制，系统性能显著下降，说明复杂的注意机制是首选的<br>在音频分类中广泛使用的平衡采样机制，在TTA中没有表现出改善。（我看着挺明显的？）<br>条件增强（Conditional augmentation）在主观评价中有提升，但是在客观评价中没有。作者猜测可能是因为条件增强生成的训练数据在AC中没有代表性，导致模型的输出和测试集没有很好的对齐，导致度量分数更低。<br><img src="/2023/11/13/AudioLDM/image-20231117165738362.png" alt=""></p><p><strong>DDIM Sampling Step</strong><br>在DDPM中，反向过程的推断步数会直接影响生成质量，通常，增加采样步数和计算量，采样质量可以得到改善。<br><img src="/2023/11/13/AudioLDM/image-20231117170501557.png" alt=""><br>100步之后提升很小。</p><p><strong>Guidance Scale</strong><br>是生成多样性和条件生成质量之间的权衡，就是前文的参数 $w$ ，但是这里的取值是1234，按照前文的公式应该是取0到1才对啊<br>在看代码的过程中找到了答案，代码里用的 $w=2.5$，该参数最后作用于<code>ddim.py</code>中<br></p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">e_t = e_t_uncond + unconditional_guidance_scale * (e_t - e_t_uncond)</span><br></pre></td></tr></tbody></table></figure><br>这里是用 $\epsilon_\theta(z_n,n)-\epsilon_\theta(z_n,n,E^y)$ ，再乘上 $w$ <p></p><h3 id="一些总结"><a href="#一些总结" class="headerlink" title="一些总结"></a>一些总结</h3><ul><li>参考CLIP的预训练CLAP模型，将text和audio的特征向量对齐</li><li>Latent Diffusion Model，将text encoder得到的文本嵌入通过扩散模型映射到隐变量空间</li><li>VAE，用于从隐变量生成梅尔谱图</li><li>HiFi-GAN，将梅尔谱图还原为音频<h3 id="附录"><a href="#附录" class="headerlink" title="附录"></a>附录</h3>看看一些细节，好多没懂的<br>首先是用到的预训练模型CLAP，最初是微软提出的，这个之后开新坑看以下，本文用到的是另一篇基于CLAP提出的流水线技术。</li></ul><p>LDM使用的是StableDiffusion的backbone，是一个叫UNet的东西，但是怎么看怎么和那个条件概率推导链没什么关系<br>回到最开始的图像LDM<br><img src="https://img-blog.csdnimg.cn/f95df368ee754d13bbccafcd5e28ff11.png" alt="图像LDM"><br>UNet参与的地方是上图中的 Denoising 过程<br>还是看看代码 8️⃣ （服务器down了，之后再看8️⃣）<br>代码看不懂，回来先翻译一下这段</p><blockquote><p>我们采用StableDiffusion的UNet backbone作为AudioLDM的LDM基本架构。如式5所示，UNet模型同时以时间步长t和CLAP嵌入E为条件（就是公式里的 n 和 $E^y$ ），我们将时间步长映射到一维嵌入中，然后将其与E链接为条件消息（conditioning information）。由于我们的条件向量只是一维的，我们没有使用StableDiffusion中的cross-attention mechanism。相反，我们直接使用特征线性调制层（feature-wise linear modulation layer）将conditioning information与UNet卷积块的特征映射合并。我们使用的UNet主干有四个编码块，一个中间块和四个解码块。使用一个basic channel number $c_u$ ，encoder块的通道维数为 $[c_u,2c_u,3c_u,5c_u]$ ，decoder块的通道数相反（对称的），中间块的通道数为 $5c_u$。我们在最后三个encoder块 和 前三个decoder块 中添加了一个注意力块，距离来说，我们添加了两个多头自注意力层和一个全连接层作为注意力块。（忘了 ，之后再看一遍开个新坑），head数为 嵌入层数/$c_u$ 。然后是参数设置，在 forward process，使用了N=1000 steps。采用$\beta_1=0.0015$到$\beta_N=0.0195$的线性噪声范围。采样过程中，使用的是DDIM（看到都说训练用DDPM，采样用DDIM，会更快而且没什么影响）采样 200 steps</p></blockquote><p>VAE的encoder和decoder都是由卷积模块组成的，每个卷积模块由多个残差神经网络块（ResNet blocks，由卷积层和残差链接构成）组成，ResNet感觉可以开新坑</p><p>VAE的Loss，由重建损失，对抗损失，高斯约束损失组成（前面提到过<br>对抗损失：是用来提升重建质量的，具体来说，就是用一个模型（这里用的PatchGAN）来对重建梅尔谱图分块后进行预测，判断这个块是真的还是假的，输出logits，最大化正确识别真实patch的logits，同时最小化错误识别假patch的logits。<br>高斯约束损失：使VAE学习一个连续的结构化的潜在空间，而不是一个无组织的潜在空间，按照原始VAE的说法，这里应该是一个KL散度</p><p>Vocoder用的HiFi-GAN，这个之后开新坑</p><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><p>写了点注释<br>代码看不懂，论文又看不懂，死了得了</p><p>在推理过程中，$\epsilon_\theta(z_n,n),\epsilon_\theta(z_n,n,E^y)$ 两个值是一起从模型出来的，然后在第一维切开成两半<br></p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">e_t_uncond, e_t = self.model.apply_model(x_in, t_in, c_in).chunk(<span class="number">2</span>)</span><br></pre></td></tr></tbody></table></figure><p></p><p>完整的UNet结构在 <code>ddpm.py</code> 里进<br></p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">out = self.diffusion_model(x, t, y=cc)</span><br></pre></td></tr></tbody></table></figure><br>里面确实可以看到四个块，至于通道数我没有深究<p></p><p>大概看了一遍推理的过程，跟着调试走了一遍，训练的代码没看S</p>]]></content>
      
      
      <categories>
          
          <category> 论文笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 论文 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>VQ-VAE相关</title>
      <link href="/2023/11/07/VQ-VAE%E7%9B%B8%E5%85%B3/"/>
      <url>/2023/11/07/VQ-VAE%E7%9B%B8%E5%85%B3/</url>
      
        <content type="html"><![CDATA[<h1 id="VQ-VAE相关"><a href="#VQ-VAE相关" class="headerlink" title="VQ-VAE相关"></a>VQ-VAE相关</h1><h2 id="相关文章"><a href="#相关文章" class="headerlink" title="相关文章"></a>相关文章</h2><p>原理解读：</p><ul><li><a href="https://zhuanlan.zhihu.com/p/657857297">https://zhuanlan.zhihu.com/p/657857297</a></li><li><a href="https://zhuanlan.zhihu.com/p/633744455">https://zhuanlan.zhihu.com/p/633744455</a></li><li><a href="https://www.spaces.ac.cn/archives/6760">https://www.spaces.ac.cn/archives/6760</a></li><li><a href="https://zhuanlan.zhihu.com/p/260627899">https://zhuanlan.zhihu.com/p/260627899</a></li><li><a href="https://zhuanlan.zhihu.com/p/429686815">https://zhuanlan.zhihu.com/p/429686815</a></li></ul><p>实现：</p><ul><li><a href="https://zhuanlan.zhihu.com/p/640000410">https://zhuanlan.zhihu.com/p/640000410</a></li></ul><h2 id="Auto-Encoder"><a href="#Auto-Encoder" class="headerlink" title="Auto Encoder"></a>Auto Encoder</h2><p>自动编码器，由一对编码器和解码器组成，编码器从原始数据中抽取出隐变量特征，解码器使用这个隐变量去还原原始数据<br><img src="https://pic3.zhimg.com/80/v2-6371146dd171e22faf9d5c87ffb163d6_1440w.webp" alt="AE"></p><p>自动编码器是一种数据的压缩算法，其中数据的压缩和解压缩函数是数据相关的、有损的、从样本中自动学习的<br>这个框架在训练完成之后，编解码过程是没有随机性的，常用于特征提取</p><p>对于AE，如果我们想随机生成一个隐变量，送入Decoder中得到一个生成的数据，这样做往往是行不通的（为什么</p><p>这里引入一个向量空间的概念，隐变量实际上就是一个多维向量，每一个输入的数据（就当是图片好了），在Encoder的处理下，都会被映射为n维向量空间中的一个点。但是，对于所有的输入数据，这些点在向量空间中的分布==并不是均匀的==，将MINIST数据集的隐变量降维可视化之后得到的结果如下：<br><img src="https://pic3.zhimg.com/80/v2-81231c59df037eb41ea653c54f21f042_1440w.webp" alt="MINIST"><br>可以看到，对于同类别的输入数据的隐变量，其在向量空间中的距离可能确实会比较接近，但是整个向量空间中==仍然存在许多没有与原始输入数据对应的点==，使用这些点作为Decoder的输入，产生的图像可能毫无意义。</p><blockquote><p>这就是我们所说的潜在空间没有正则化的意思。 这样的潜在空间只有少数具有生成能力的区域/簇，这意味着对潜在空间中簇内的任何点进行采样都会生成与该簇相关的变量， 但是整个潜在空间并不都具备生成能力。 不属于任何簇的区域将产生垃圾输出。 一旦网络被训练，并且训练数据被移除，我们就无法知道解码器从一个随机采样的潜在向量产生的输出是否有效。</p></blockquote><p>对于有效输入，AE能够将它们压缩到更少的维度，基本消除了冗余，所以能够用来做压缩</p><h2 id="Variational-Auto-Encoder"><a href="#Variational-Auto-Encoder" class="headerlink" title="Variational Auto Encoder"></a>Variational Auto Encoder</h2><p>变分自动编码器，这个变分是什么？（科学空间中提到这是二阶范数求导中用到的方法）<br>先看结构（==看一下这个分布的数量是怎么界定的，是定好的还是生成的==）<br>均值和方差的数量是定好的，代码里使用两个线性层分别生成<br></p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">self.mean_linear = nn.Linear(prev_channels * img_length * img_length,</span><br><span class="line">                                     latent_dim)</span><br><span class="line">self.var_linear = nn.Linear(prev_channels * img_length * img_length,</span><br><span class="line">                                    latent_dim)</span><br></pre></td></tr></tbody></table></figure><br>均为隐变量维数，然后在forward的过程中：<br><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">mean = self.mean_linear(encoded)</span><br><span class="line">logvar = self.var_linear(encoded)</span><br><span class="line">eps = torch.randn_like(logvar)</span><br><span class="line">std = torch.exp(logvar / <span class="number">2</span>)</span><br><span class="line">z = eps * std + mean</span><br></pre></td></tr></tbody></table></figure><br>会如下文所讲的，取一个 $\varepsilon$ 然后乘标准差加均值，这样就可以做梯度了<p></p><p><img src="https://pic4.zhimg.com/80/v2-9b4ff3e9dc22347d478e25ceba528363_1440w.webp" alt="VAE"></p><p>相较AE，VAE在Encoder的部分得到的不再是隐变量，而是一堆均值和方差，送入Decoder中的 $Z_i$ 则是以这堆均值和方差的正态分布为基准采样得到的。</p><p>相较AE，VAE的输出则是一个概率分布，而不是一个离散的值。为什么是这么做呢</p><p>看了几篇，有这么几种说法，首先是对上面那个向量空间的说法：</p><blockquote><p>VAE的编码器不输出潜空间中的向量，而是输出每个输入的潜空间中预定义分布的参数。然后VAE对这个潜在分布施加约束，迫使它成为一个正态分布。这个约束确保了潜在空间是正则化的。</p><p>很可惜，AE的编码器编码出来的向量空间是不规整的。也就是说，解码器只认识经编码器编出来的向量，而不认识其他的向量。如果你把自己随机生成出来的向量输入给解码器，解码器是生成不出有意义的图片的。</p></blockquote><p>这样的处理让隐变量具有了语义，在两个不同的分布之间的点，会具有两个分布融合的属性，可视化如下<br><img src="https://pic2.zhimg.com/80/v2-87b589ea7d2b954570509dc7dde8765d_1440w.webp" alt=""></p><p>我只能说，看起来更散</p><p>科学空间中用概率论来解释<br>要做生成，实际上是假设了隐变量 Z 服从某些常见的分布，然后希望训练一个模型 $X=g(Z)$ ，这个模型能够将原来的概率分布映射到训练集的概率分布。<br>对于一个生成模型，我们希望从一批数据 $\{X_1,\dots ,X_n\}$ 中得到 $X$ 的分布 $p(X)$ ，然后直接从这个分布中抽样，就能得到所有可能的 $X$ 了，但是这对于稍微复杂的数据就已经是不可能的了，于是我们将分布改一改，由全概率公式</p><script type="math/tex; mode=display">p(X) = \sum_Z p(X|Z)p(Z)</script><p>变成一个后验概率分布乘一个假设已知的概率分布，这里的 $p(X|Z)$ 就描述了一个由 $Z$ 来生成 $X$ 的模型。<br>但是在训练的过程中，隐变量 $Z$ ，需要与每个输入 $X_k$ 对应，也就是说，对于多个输入 $X_k$ 如果隐变量只有一个分布，那就不能将 $Z$ 与每个输入一一对应，所以对每个输入，我们需要一个专属于这个输入的 $p(Z|X_k)$ ，这是VAE网络拟合的目标</p><p><img src="https://spaces.ac.cn/usr/uploads/2018/03/2584918486.png" alt=""></p><p>于是构建两个神经网络 $\mu _k = f_1(X_k),\; \log \sigma_k^2 = f_2(X_k)$ 使用对数的目的是将非负的 $\sigma_k^2$ 映射到实数域，就不需要加激活函数了，模型训练目标是最小化重建损失，即 $\min(D(X_k,\hat X_k))$ </p><p>然后提到了一个模型退化的问题，在训练过程中，为了最小化重建损失，会使方差尽可能为0，这样模型就会失去随机性，退化为普通的自编码器（就是过拟合问题？）</p><p>为了解决这个问题，需要在重构损失的基础上添加新的损失来约束模型，VAE方法中使用的是目标正态分布与标准正态分布的KL散度作为这个额外的Loss。</p><p>Loss选择完后，开始做模型训练了，但是由分布抽样得到的隐变量 $Z$ 是无法进行反向传播的（抽样过程不可导），文中使用 <code>reparameterization trick</code> 来解决这个问题。</p><p>$Z$ 落在点 $Z$ 处的概率表示为概率密度乘上 $Z$ 的微分</p><script type="math/tex; mode=display">\frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(z-\mu)^2}{2\sigma^2}\right)dz</script><p>文章在这里凑微分，把微分项凑成了 $d\left( \frac{z-\mu}{\sigma}\right)$ ，令 $\varepsilon = \frac{z-\mu}{\sigma}$ ，则 $\varepsilon$ 是服从标准正态分布的，所以从分布中抽样 $Z$ 相当与从标准正态中抽样 $\varepsilon$ ，让 $Z=\mu + \varepsilon\times \sigma$ ，然后对这个抽样结果进行梯度下降</p><h2 id="Vector-Quantisized-Variational-AutoEncoder"><a href="#Vector-Quantisized-Variational-AutoEncoder" class="headerlink" title="Vector Quantisized - Variational AutoEncoder"></a>Vector Quantisized - Variational AutoEncoder</h2><p>VAE编码出来的向量是连续向量，也就是说向量的每一维都是浮点数。对某一维的轻微改动，对生成的影响不大。<br>VQ-VAE的作者认为，VAE的生成图片之所以质量不高，是因为图片被编码成了连续向量，而编码成离散向量更加合理。</p><blockquote><p>比如我们想让画家画一个人，我们会说这个是男是女，年龄是偏老还是偏年轻，体型是胖还是壮，而不会说这个人性别是0.5，年龄是0.6，体型是0.7。</p></blockquote><p>但是Decoder对离散的向量处理是乏力的，对离散的值要做梯度下降，又会默认按照是连续的值来计算，比如对0，1，2三个离散值，神经网络会认为1是在0，2之间的一种状态。为了解决整个这个问题，借用了NLP中对离散单词的处理方法。</p><p>将每个离散标签映射到独一无二的连续向量上，这样完成了一个连续-离散-连续的转换，可以让模型完成计算。<br>这个部分在VQ-VAE中叫做<code>embedding space</code>，在后续文章中则被称为 <code>codebook</code><br><img src="https://pic4.zhimg.com/v2-4382b984165170e238be55b914ff3503_r.jpg" alt="vqvae结构"></p><p>在具体介绍VQ-VAE的细节之前，先作如下说明：</p><ul><li>模型本身无法单独完成生成任务，需要配合其他自回归模型（下面会提到）</li><li>对于生成的离散向量，单条向量的信息载荷是无法承担一张图片或者其他内容的信息的，实际使用过程中是生成一个具有离散值的矩阵，每个值是embedding space中的一个向量下标。</li></ul><p>VQ-VAE用于图像生成的工作过程</p><ul><li>训练VQ-VAE的编码器和解码器，使得VQ-VAE能够把图像变为由离散标签组成的‘小图像’，也能将小图像解码回大图像。</li><li>训练一个自回归模型，能够生成小图像</li><li>随机采样时，先用自回归模型采样出小图像，再用VQ-VAE把小图像翻译成最终生成的图像</li></ul><p>就是说明明两个模型都可以做生成，但为什么要放在一起做，看到两个文章从两个角度分别是这么解释的</p><ol><li>这样做的目的是为了解决一个问题： ==离散空间不好采样== <font color="red">为什么？</font></li><li>自回归模型逐像素生成很慢，且对于大一点的图像，不管是RNN还是CNN模型都无法很好的捕捉这么长的依赖。</li></ol><h3 id="实现细节"><a href="#实现细节" class="headerlink" title="实现细节"></a>实现细节</h3><p><strong>输出离散编码</strong><br>Encoder输出的是一个连续的编码向量，这里用 $Z_e$ 表示，使用<strong>最近邻重构</strong>的方法，将这个输出的编码向量与嵌入空间 $E=[e_1,e_2,\dots,e_k]$ 中的某一个向量关联起来，达到离散化的目的。</p><ol><li>首先，一张 $n\times n\times 3$ 的图片被传入一个 encoder 中，得到连续的编码向量 $Z_e$</li><li>计算向量 $Z_e$ 与嵌入空间中的每个向量的距离，选取距离最小的下标 $k = \underset{j}{\arg\min} \Vert z-e_j\Vert_2$ </li><li>将与 $Z_e$ 距离最小的嵌入空间向量记为 $Z_q$ ，作为最后的编码结果，送入 decoder 中。</li></ol><p>如下：<br><img src="https://pic4.zhimg.com/80/v2-85ebf6142dc52d39497b9161c5099d77_1440w.webp" alt="VQVAE细节"></p><p><strong>梯度无法传递的情况下的优化过程</strong><br>我们优化的最终目标是要让输出的重建图像与原图像一致，所以很自然的有重建损失</p><script type="math/tex; mode=display">L_{reconstruct}=\Vert x - decoder(z_q)\Vert ^2_2</script><p>但是从 $Z_e$ 到 $Z_q$ 的过程是不可导的，这里采用了一个叫 Straight-Through Estimator 的方法，将梯度直接传递到 encoder，在 <a href="BEATS-Audio-Pre-Training-with-Acoustic-Tokenizers.md">BERTS</a> 中也有用到，指的是==前向传播和反向传播的计算可以不对应==<br>基于这一技术，VQ-VAE提出了一种叫做sg（stop gradient）的运算</p><script type="math/tex; mode=display">sg(x)= \left\{\begin{aligned}&x(in\;forward\;propagation)\\&0(in\;backward\;propagation)\end{aligned}\right.</script><p>正向传播不变，反向传播不计算梯度，最终的Loss如下：</p><script type="math/tex; mode=display">L_{reconstruct}=\Vert x-decoder(z_e+sg(z_q-z_e))\Vert_2^2</script><p>按照 $sg(x)$ 的定义，正向传播的Loss为：</p><script type="math/tex; mode=display">L=\Vert x-decoder(z_q)\Vert_2^2</script><p>反向传播求梯度为：</p><script type="math/tex; mode=display">L=\Vert x-decoder(z_e)\Vert_2^2</script><p>对于嵌入空间的优化，我们希望嵌入空间的每一个向量应该能概括一类编码器输出的向量，也就是说，对于一类编码器输出，其对应的嵌入空间向量应该与他们尽量接近，很自然的我们会把这个距离添加到Loss函数中去</p><script type="math/tex; mode=display">L_d = \Vert z_e-z_q\Vert_2^2</script><p>但是作者认为，嵌入空间向量相对来说比较自由，而编码器输出 $z_e$ 要尽可能保证重构效果，所以要让 $z_q$ 比 $z_e$ 更努力的接近对方，于是把上面的Loss拆成两项</p><script type="math/tex; mode=display">L_d = \alpha \Vert sg(z_e)-z_q\Vert_2^2 + \beta \Vert z_e-sg(z_q)\Vert_2^2</script><p>其中第一项误差来自字典学习算法里的经典算法 Vector Quantisation，用于优化嵌入空间<br>第二个误差叫做专注误差，用于约束编码器的输出，不让它跑到离嵌入空间太远的地方。</p><h2 id="总结和思考"><a href="#总结和思考" class="headerlink" title="总结和思考"></a>总结和思考</h2><p>看完VQ-VAE再回去看之前那篇BEATs，一些方法是直接照搬VQ-VAE的做法的，这个离散标签化的做法在图像音频上都取得了很好的效果，但是我觉得这种孤立的，离散的做法无法表示出码本中的向量之间的关系。</p><p>VAE中对隐变量的向量空间正则化是否可以通过某种手段应用到 zero-shot 分类当中去？</p>]]></content>
      
      
      <categories>
          
          <category> 论文笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> VAE </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>BEATS-Audio Pre-Training with Acoustic Tokenizers</title>
      <link href="/2023/10/11/BEATS-Audio-Pre-Training-with-Acoustic-Tokenizers/"/>
      <url>/2023/10/11/BEATS-Audio-Pre-Training-with-Acoustic-Tokenizers/</url>
      
        <content type="html"><![CDATA[<h1 id="BEATs-Audio-Pre-Training-with-Acoustic-Tokenizers"><a href="#BEATs-Audio-Pre-Training-with-Acoustic-Tokenizers" class="headerlink" title="BEATs: Audio Pre-Training with Acoustic Tokenizers"></a>BEATs: Audio Pre-Training with Acoustic Tokenizers</h1><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>自监督学习这几年用的很好，当离散标签预测(dicrete label prediction)被广泛运用于其他领域，最先进的音频SSL模型仍然使用重建损失进行预训练。与重建损失相比，语义丰富的离散标签预测鼓励SSL像人类感知一样抽象高级音频语义并丢弃冗余细节。但是这种token比较难获得。</p><p>本文提出了一个新的框架，BEATs(Bidirectional Encoder representation from Audio Transformers)</p><p>一个迭代音频预训练框架，用于从Audio Transformers(这是什么东西)学习双向编码表示</p><p>其中声学标记器（acoustic tokenizer）和音频SSL模型是通过迭代优化的</p><p><strong>tokenizer：</strong></p><p>可以理解为一个切割的东西，把音频信号分为处理的最小元素</p><p>在第一次迭代中，我们使用随机投影作为声学标记器，以掩码和标签预测的方式训练音频SSL模型。然后，我们通过从预训练或微调的音频SSL模型中提取语义知识来训练下一次迭代的声学标记器。重复迭代，希望声学标记器和音频SSL模型相互促进。</p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>然而，人们普遍认为重建损失只考虑了低层次时频特征的正确性，而忽略了高层次的音频语义抽象</p><p>由于以下原因，离散标签预测可能是比重建更好的音频预训练目标。</p><ul><li>第一，从生物学角度看，人类通过抽取和聚类高级的<strong>语义信息</strong>而不是关注低级的时频细节来理解音频。</li><li>第二，从建模效率来看，重建损失可能会浪费音频模型参数容量和预训练资源来预测语义无关信息，而离散标签预测鼓励模型丢弃冗余细节，可以提高音频建模效率。</li><li>第三，更适配多模态预训练。</li></ul><p>离散标签预测的应用仍然很困难，主要有两个原因：</p><ul><li>第一，由于音频信号是连续的，同一声事件在不同场合可能有不同的持续时间，因此不像语音处理那样直接将音频分割成语义上有意义的token</li><li>另一方面，与语音不同，一般音频信号包含的数据变化过大，包括各种非语音声学事件和环境声音，无法直接应用常用的语音分词器进行音素信息提取</li></ul><p>然后用本文提出的这个BEATs来解决上面的问题</p><p><img src="/2023/10/11/BEATS-Audio-Pre-Training-with-Acoustic-Tokenizers/image-20231011111452071.png" alt="image-20231011111452071"></p><p>就是这么一个循环迭代器，在每轮迭代中，首先用 声学标记器 生成未标记的音频的离散标签，并使用它们来优化带有掩码和离散标签预测损失的音频SSL模型。</p><p>经过收敛后，音频SSL模型充当教师的角色，引导声学标记器通过知识蒸馏学习音频语义</p><p>在第一次迭代中，我们使用随机投影声学标记器来生成离散标签作为冷启动。</p><h2 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h2><p>本文提出的框架优点：</p><p>使用少量标记样本，自监督学习</p><p>创新点：</p><ul><li><p>在自监督预训练方法中第一次使用掩蔽离散标签预测（masked discrete label prediction objective）</p></li><li><p>一个新的tokenizer：在上一轮SSL模型的监督下训练声学分词器，和之前的自动编码（对应reconstruction loss）和adhoc聚类方法不同（之前的我也不懂）</p></li></ul><h2 id="BEATs"><a href="#BEATs" class="headerlink" title="BEATs"></a>BEATs</h2><h3 id="Iterative-Audio-Pre-training"><a href="#Iterative-Audio-Pre-training" class="headerlink" title="Iterative Audio Pre-training"></a>Iterative Audio Pre-training</h3><p>就还是上面那张图那个大流程，对于图中的音频SSL模型，可以用一个预训练的SSL模型或者一个微调的SSL模型。微调的模型从自监督预训练和监督微调中学习语义知识，对于语义蒸馏的工作做的更好。</p><h3 id="Acoustic-Tokenizers"><a href="#Acoustic-Tokenizers" class="headerlink" title="Acoustic Tokenizers"></a>Acoustic Tokenizers</h3><p>这个部件用于为BEATs生成每轮迭代的离散标签。在第一轮迭代中，考虑到教师模型不可用（就是SSL model没训练），用随机投影标记器（Random-Projection Tokenizer）将连续的声学特征聚类到离散的标签中作为冷启动</p><p>第二轮开始，训练一个自蒸馏标记器（Self-Distilled Tokenizer），使用从上次迭代中获得的预训练/微调音频SSL模型中提取的语音感知知识生成精细的离散标签。</p><h4 id="Cold-Start-Random-Projection-Tokenizer"><a href="#Cold-Start-Random-Projection-Tokenizer" class="headerlink" title="Cold Start: Random-Projection Tokenizer"></a>Cold Start: Random-Projection Tokenizer</h4><p><img src="/2023/10/11/BEATS-Audio-Pre-Training-with-Acoustic-Tokenizers/image-20231011190012224.png" alt="image-20231011190012224"></p><p>架构如图</p><p>按照图的样子，首先，输入是 $X = \{x_t\}^T_{t=1}$（corresponding acoustic features），就当是语谱图好了，将语谱图分成小块，通过一个线性层 $W$ ，$W$ 的参数是<strong>随机生成</strong>的，得到向量 $Wx_t$ 。</p><p>然后引入一个 <code>frozen Codebook Embeddings</code>  ，相当于一个<strong>嵌入参考码本</strong>，定义为 $V=\{v_i\}^K_{i=1}$ ，K 是码本大小，其中的每个向量 $v_i$ 的值都是随机生成的。</p><p>接下来，需要计算向量 $Wx_t$ 与 $V$ 中的每个向量的二阶范数，然后将每个输入向量的离散标签定义为：二阶范数最小的码本向量的下标：</p><script type="math/tex; mode=display">\hat z_t= \underset{i}{\arg\min} \Vert v_i-Wx_t\Vert_2^2\qquad(1)</script><h4 id="Iteration-Self-Distilled-Tokenizer"><a href="#Iteration-Self-Distilled-Tokenizer" class="headerlink" title="Iteration: Self-Distilled Tokenizer"></a>Iteration: Self-Distilled Tokenizer</h4><p>第二次迭代，使用上一次迭代的SSL模型作为教师，可以是预训练或者微调模型，来教授当前轮的迭代标记器学习。我们称为 <code>Self-Distilled Tokenizer</code> ，为每个输入音频生成 patch-level 的离散标签。</p><p>具体结构如下图<br><img src="/2023/10/11/BEATS-Audio-Pre-Training-with-Acoustic-Tokenizers/image-20240308114123461.png" alt=""></p><p>这里和冷启动又是一套不同的东西。首先对于输入，还是一样的格式 $X = \{x_t\}^T_{t=1}$ ，送入一个 12层的Transformer-based tokenizer encoder，得到编码向量序列 $E=\{e_t\}^T_{t=1}$ </p><p>然后应该是同样计算向量距离，不过这里计算的码本向量是一个叫 <code>Learnable Codebook Embeddings</code>的东西，不再是之前那个随机生成的而是一个可学习的，得到离散标签，距离计算规则如下：</p><script type="math/tex; mode=display">\hat z_t = \underset{i}{\arg\min} \Vert \ell_2(v_i)-\ell_2(e_t)\Vert_2^2\qquad(2)</script><p>L2正则化的目的是提高码本利用率</p><p>以量化向量序列 $E^q=\{v_{\hat z_t}\}^T_{t=1}$ 作为输入，使用一个3层的 Transformer estimator来预测上一层教师模型的输出 $\{\hat O_t\}^T_{t=1}$</p><p>为了解决向量量化不可微的问题，采用 <code>Straight-Through Gradients</code>机制，在反向传播过程中，直接将梯度从 $E^q$ 中复制到 $E$</p><p>这个 <code>Self-distilled tokenizer</code>的总体训练目标定义为 <code>Tokenizer estimator</code> 和 教师模型的输出之间的 <code>余弦相似度</code> 还有 $E$ 与 $E^q$ 之间的 <code>均方误差</code><br><img src="/2023/10/11/BEATS-Audio-Pre-Training-with-Acoustic-Tokenizers/image-20240308152947443.png" alt=""><br><code>sg</code> 是 stopgradient operator</p><p> 为什么是这样处理，后面两项是干嘛的</p><p>使用指数移动平均进行码本嵌入优化，以实现更稳定的标记器训练。<br>在推断时，舍弃<code>tokenizer estimator</code> ，并利用预训练的标记器编码器和码本嵌入将每个输入音频 X 转换为离散标签，如公式(2) 所示。</p><h3 id="Audio-SSL-Model"><a href="#Audio-SSL-Model" class="headerlink" title="Audio SSL Model"></a>Audio SSL Model</h3><h4 id="Backbone"><a href="#Backbone" class="headerlink" title="Backbone"></a>Backbone</h4><p>使用ViT架构作为主干网络，结构看图</p><h4 id="Pre-Training"><a href="#Pre-Training" class="headerlink" title="Pre-Training"></a>Pre-Training</h4><p>提出了一个 掩盖音频模型（Masked Audio Modeling）任务来进行预训练，如下图：</p><p><img src="/2023/10/11/BEATS-Audio-Pre-Training-with-Acoustic-Tokenizers/image-20231013163709407.png" alt="image-20231013163709407"></p><p>不同于其他预训练方法，本模型是通过预测patch-level的离散标签来进行优化</p><p>给定输入patch序列 $X = \{x_t\}^T_{t=1}$ 和对应的离散声学标签（使用之前的tokenizer生成的） $\hat Z=\{\hat z_t\}^T_{t=1}$ </p><p>随机掩盖 75% 的输入小块（patches），表示为 $M = \{1,\dots,T\}^{0.75T}$ ，然后将未掩盖的块送入ViT encoder，得到表示 $R^U$ ，然后将未被掩盖的块的表示 和 被掩盖的位置的特征 并在一起送入Label Predictor</p><p>和 $\hat Z$ 做交叉熵算Loss</p><p>（感觉文章里好多错误）</p><h4 id="Fine-Tuning"><a href="#Fine-Tuning" class="headerlink" title="Fine-Tuning"></a>Fine-Tuning</h4><p><img src="/2023/10/11/BEATS-Audio-Pre-Training-with-Acoustic-Tokenizers/image-20231013170901829.png" alt="image-20231013170901829"></p><p>在微调阶段，在最后放弃了标签预测器，改用一个特定于任务的（task-specific）线性分类器。以生成下游分类任务的标签。</p><p>首先在时域和频域随机的掩盖声学特征（和spec-augmentation的做法一样，但这个又是什么做法）然后展开成小块序列，和预训练不同的是，不再是将未掩盖的块送入ViT encoder中，而是将全部块都送入，得到表示，最后，使用一个线性分类器来计算每个类别的概率。概率计算如下（线性层出来还有两步图上没有，分别是一个池化和一个softmax）</p><script type="math/tex; mode=display">p(C) = softmax(MeanPool(W_cR))</script><p>采用二元交叉熵损失作为多标签分类任务或混合增强</p><h2 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h2><h3 id="Datasets"><a href="#Datasets" class="headerlink" title="Datasets"></a>Datasets</h3><p>在完整的AudioSet训练集中训练，在六个下游任务中评估，包括三个音频分类任务（AS-2M, AS-20K, ESC-50）三个语音分类任务（KS1, KS2, ER）</p><p>记一下几个不熟悉的数据集</p><p>ESC-50（Environmental Sound Classification）：是音频分类数据集，包含2000个5秒的环境录音，共50个类别，每个录音只有一个类别注释</p><p>KS2（Speech Commands V2）：是一个关键字发现数据集，包括105829个1秒的单词片段，用35个常用词类注释。被细分未训练集，验证集和测试集。</p><p>KS1（Speech Commands V1）：数据集和KS2一样，但是只包含10类关键字，1个silence，1个unknown类</p><p>ER（IEMOCAP）：是一个情感识别数据集，包含大约12小时的情感语音片段，并标注了4个类。</p><h3 id="Implementation-Details"><a href="#Implementation-Details" class="headerlink" title="Implementation Details"></a>Implementation Details</h3><p>讲了预训练网络层数和维度</p><p>声学特征提取，16000采样率，128组梅尔滤波器，25ms Povey窗函数，10ms步长</p><p>标准化为0均值，0.5标准差，展成16乘16小块</p><p>在AS-2M数据集上进行了3轮迭代预训练，分别得到4个模型：$BERTs_{iter1},BEATs_{iter2},BEATs_{iter3},BEATs_{iter3+}$</p><p>iter1是使用随机投射标记器生成的离散标签预训练的，从第二轮迭代开始，标记器不再是随机投射的而是训练的，iter2就是用训练的标记器生成的离散标签预训练的，iter3使用iter1和iter2作为老师自监督训练。</p><p>$BEATs_{iter3+}$ 预训练用的标记器是使用监督微调的$BEATs_{iter2}$作为老师训练的。</p><p>和其他 BEATs模型相比，3+在微调和预训练两个阶段都利用了下游任务的监督数据，</p><h3 id="Comparing-with-the-SOTA-Single-Models"><a href="#Comparing-with-the-SOTA-Single-Models" class="headerlink" title="Comparing with the SOTA Single Models"></a>Comparing with the SOTA Single Models</h3><p>第一轮迭代的模型 $BEATs_{iter1}$ ，使用的是随机投影标记器生成的离散标签，在6个任务中的5个任务上已经获得了比以前工作更好的性能，这表明了离散标签预测损失与重建损失相比的优越性。</p><p>对于第三次迭代，2和3 的性能相似，这说明自蒸馏标记器对不同给的SSL教师模型具有鲁棒性。并且如果使用经过微调的BEAT2模型作为教师模型，BEAT3+可以在as2m和as20k任务上带来显著的性能提升，通过在迭代训练中引入监督微调数据，Tokenizer和SSL model相互学习更多特定于任务的语义知识，这将有效提高在下游任务的性能。</p><h3 id="Comparing-Different-BEATs-Tokenizers"><a href="#Comparing-Different-BEATs-Tokenizers" class="headerlink" title="Comparing Different BEATs Tokenizers"></a>Comparing Different BEATs Tokenizers</h3><p><img src="/2023/10/11/BEATS-Audio-Pre-Training-with-Acoustic-Tokenizers/image-20231013164319872.png" alt="image-20231013164319872"></p><p>可以看出，自蒸馏标记器比随机投影标记器的表现更好，特别是数据量少的情况下更为明显。</p><p>这是因为具有简单特征聚类过程的随机投影标记器<strong>不足以提供具有高级音频语义抽象的标签</strong>，而自提取标记器能够从预训练良好的音频SSL模型中提取语义知识到生成的离散标签。</p><p>另外，自蒸馏标记器，对不同的自监督学习老师模型不敏感，但是对不同的监督学习老师敏感。</p><h3 id="Comparing-Differrent-Pre-Training-Targets-via-Visualization"><a href="#Comparing-Differrent-Pre-Training-Targets-via-Visualization" class="headerlink" title="Comparing Differrent Pre-Training Targets via Visualization"></a>Comparing Differrent Pre-Training Targets via Visualization</h3><p><img src="/2023/10/11/BEATS-Audio-Pre-Training-with-Acoustic-Tokenizers/image-20231012161311295.png" alt="image-20231012161311295"></p><p>使用ESC-50的音频样本，来比较不同SSL模型的预训练目标</p><p>上面三个图分别是，基于重建损失的SSL模型的声学特征，使用自监督学习预训练的老师学习的标记器量化的表示，使用监督学习微调的老师学习的标记器量化的表示</p><p>同一个颜色代表同一个类别，同一个颜色的不同点代表对原始数据添加的不同干扰，比如混响和噪声。</p><p>我的理解：</p><p>可以看出，对tokenizer的训练中引入监督学习微调，会提高表示之间的距离，语义信息更加丰富</p><h3 id="Comparing-with-the-SOTA-Ensemble-Models"><a href="#Comparing-with-the-SOTA-Ensemble-Models" class="headerlink" title="Comparing with the SOTA Ensemble Models"></a>Comparing with the SOTA Ensemble Models</h3><p><img src="/2023/10/11/BEATS-Audio-Pre-Training-with-Acoustic-Tokenizers/image-20231013102456908.png" alt="image-20231013102456908"></p><p>第一次了解到 ensemble models的概念，大概就是把训练的几个模型的结果汇总，然后用一些方法计算最终结果，比如投票机制，均值机制等</p><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>BEATs，一个预训练框架，一个自监督学习模型。</p><p>提出了 self-distilled tokenizer，在预训练中，使用tokenizer生成的离散标签代替重建损失。</p>]]></content>
      
      
      <categories>
          
          <category> 论文笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 论文 </tag>
            
            <tag> SED </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>音频信号处理</title>
      <link href="/2023/08/23/%E9%9F%B3%E9%A2%91%E4%BF%A1%E5%8F%B7%E5%A4%84%E7%90%86/"/>
      <url>/2023/08/23/%E9%9F%B3%E9%A2%91%E4%BF%A1%E5%8F%B7%E5%A4%84%E7%90%86/</url>
      
        <content type="html"><![CDATA[<h1 id="音频信号处理"><a href="#音频信号处理" class="headerlink" title="音频信号处理"></a>音频信号处理</h1><h2 id="采样"><a href="#采样" class="headerlink" title="采样"></a>采样</h2><p>模拟信号采样，转变成数字信号再使用计算机处理</p><p>对于每个采样点，都是离散的值，对于计算机存储来说很不方便，所以会用到量化</p><p>一般有三种方式</p><ul><li>零记忆量化</li><li>分组量化</li><li>序列量化</li></ul><p><strong>短时加窗处理</strong></p><p>在看tfgrednet的代码时，看到了汉明窗和汉宁窗两个概念，书里给出了这两种窗函数的公式，</p><p>评价窗函数的指标：泄露指数（对应旁瓣泄露现象），主瓣宽度，旁瓣衰减，旁瓣滚降率</p><p>不懂</p><h2 id="python实践"><a href="#python实践" class="headerlink" title="python实践"></a>python实践</h2><p>一直看书感觉懵的，上手实践一下感觉会好一点</p><p>使用 <code>librosa</code> 库读取wav文件</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">samples, sr = librosa.load(wavefile, sr=<span class="number">8000</span>)</span><br></pre></td></tr></tbody></table></figure><p>函数原型：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">(function) <span class="keyword">def</span> <span class="title function_">load</span>(<span class="params"></span></span><br><span class="line"><span class="params">    path: <span class="built_in">str</span> | <span class="built_in">int</span> | PathLike[<span class="type">Any</span>] | SoundFile | AudioFile | BinaryIO,</span></span><br><span class="line"><span class="params">    *,</span></span><br><span class="line"><span class="params">    sr: <span class="built_in">float</span> | <span class="literal">None</span> = <span class="number">22050</span>,</span></span><br><span class="line"><span class="params">    mono: <span class="built_in">bool</span> = <span class="literal">True</span>,</span></span><br><span class="line"><span class="params">    offset: <span class="built_in">float</span> = <span class="number">0</span>,</span></span><br><span class="line"><span class="params">    duration: <span class="built_in">float</span> | <span class="literal">None</span> = <span class="literal">None</span>,</span></span><br><span class="line"><span class="params">    dtype: DTypeLike = np.float32,</span></span><br><span class="line"><span class="params">    res_type: <span class="built_in">str</span> = <span class="string">"soxr_hq"</span></span></span><br><span class="line"><span class="params"></span>) -&gt; <span class="type">Tuple</span>[ndarray, <span class="built_in">float</span>]</span><br></pre></td></tr></tbody></table></figure><p>库中的函数注释：</p><blockquote><p>Load an audio file as a floating point time series.</p><p>Audio will be automatically resampled to the given rate (default <code>sr=22050</code>).</p><p>To preserve the native sampling rate of the file, use <code>sr=None</code>.</p></blockquote><p>该函数在指定采样率的时候会对wav文件进行重采样，不传入sr参数的时候默认是22050，传入<code>sr=None</code>时，按照文件默认采样率读入</p><p>返回值是一个元组，分别为wav采样点的序列（支持多通道）和采样率</p><h3 id="使用自相关函数计算信号的周期"><a href="#使用自相关函数计算信号的周期" class="headerlink" title="使用自相关函数计算信号的周期"></a>使用自相关函数计算信号的周期</h3><p>找了一段之前录的电动车报警声的音频，截取了一段周期性强的作为目标音频文件</p><p>按音频本身的采样率读入</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">y,sr = librosa.load(wavfile, sr=<span class="literal">None</span>)</span><br></pre></td></tr></tbody></table></figure><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 计算自相关函数得到周期</span></span><br><span class="line">acf = np.correlate(y,y,mode=<span class="string">'full'</span>)[-<span class="built_in">len</span>(y):]</span><br></pre></td></tr></tbody></table></figure><p>在这段代码中，<code>np.correlate</code>函数计算了时间序列数据<code>y</code>的自相关函数，<code>mode='full'</code>表示使用完全相关模式。<code>[-len(data):]</code>用于截取出与原始数据长度相同的自相关函数值。</p><p>得到的acf序列是在偏移<code>lag</code>个采样点之后计算的自相关值，画出图，第二大的地方就是周期：</p><p><img src="/2023/08/23/%E9%9F%B3%E9%A2%91%E4%BF%A1%E5%8F%B7%E5%A4%84%E7%90%86/image-20230825105350812.png" alt="image-20230825105350812"></p><p>0的时候自己和自己重合所以自然大</p><h2 id="失真"><a href="#失真" class="headerlink" title="失真"></a>失真</h2><p>分为线性与非线性失真：线性失真指的是在振幅或相位上有变化但不会增加新频率的失真。 而当有新的频率分量被激发时的失真是非线性失真。非线性失真才是人们通常说的“失真”。</p><p>非线性失真又分为谐波失真与非谐波失真：谐波失真中基频的所有整数倍频率都被考虑进来。这就是被称为 THD （总谐波失真）的经典失真测量。在某些特殊应用中，会考虑一些特定谐波的数量。</p><p>非谐波失真是那些频率不是基频倍数的失真，也就是 IMD （互调失真）的情况。</p><h2 id="幅频响应与相频响应"><a href="#幅频响应与相频响应" class="headerlink" title="幅频响应与相频响应"></a>幅频响应与相频响应</h2><p>见 <a href="https://zhuanlan.zhihu.com/p/111762941">https://zhuanlan.zhihu.com/p/111762941</a></p><p>大概就是信号在通过一个双端系统时，输出端的信号会随着输入端的不同信号成分的频率产生相位和幅度的变化</p><h2 id="预加重"><a href="#预加重" class="headerlink" title="预加重"></a>预加重</h2><p>见 <a href="https://zhuanlan.zhihu.com/p/34798429">https://zhuanlan.zhihu.com/p/34798429</a></p><p>简单点的结论就是：介质作为声能量的载体，在声源尺寸一定的情况下，<strong>频率越高，介质对声能量的损耗越严重</strong>。这里涉及到声音辐射阻抗等知识，太物理学了就不看了</p><p>为了弥补这个损耗，采用预加重</p><h2 id="各种图"><a href="#各种图" class="headerlink" title="各种图"></a>各种图</h2><p>时域波形通过傅里叶变换得到频域图</p><p>时域波形通过stft得到语谱图</p><p>语谱图的频率部分将线性距离修改为对数距离得到对数谱</p><p>语谱图的振幅（amplitude）做对数运算改用db，可以得到功率谱</p><p>将功率谱的频率标度用Mel标度表示，得到梅尔谱图，<a href="https://zhuanlan.zhihu.com/p/198900624">https://zhuanlan.zhihu.com/p/198900624</a></p><p><img src="https://img-blog.csdnimg.cn/f40338ded2e34ad19a38e73c0bd69bd0.png" alt="在这里插入图片描述"></p><h1 id="youtube网课"><a href="#youtube网课" class="headerlink" title="youtube网课"></a>youtube网课</h1><p>MIDI 音阶，A4对应的频率是440HZ，相邻半音的频率比是 $F(p)/F(p-1)=2^{\frac{1}{12}}$ </p><p><strong>Sound Power 声音功率</strong></p><p>Energy per unit of time emitted by a sound source in all directions</p><p>单位 瓦(W)</p><p><strong>Sound Intensity 声音强度</strong></p><p>Sound power per unit area</p><p>单位 $W/m^2$</p><p><strong>人的听觉对声音强度的阈值</strong></p><script type="math/tex; mode=display">Threshold\;of\; hearing: TOH=10^{-12}W/m^2 \\\\Threshold\;of\;pain:10\cdot W/m^2</script><p>分贝是用来衡量Sound intensity level 的单位，设Sound intensity为 $I$</p><script type="math/tex; mode=display">dB(I)=10\cdot \log_{10}(\frac{I}{I_{TOH}})</script><p><strong>Frequency to Mel Scale</strong></p><script type="math/tex; mode=display">m= 2595\cdot log_{10}(1+\frac{f}{700})\\\\f=700\cdot (10^{m/2595}-1)</script><ul><li>Choose number of mel bands</li></ul><p>这里的band好像是翻译成频段，在AI audio的文章里经常被作为一个hyper parameter设置，常取40，60，90，128，类比键盘上的88个黑白键覆盖了一段很长的频域，mel bands也是这样类似的东西，在频率范围映射的mel频率范围中等分（然后量化？我猜的）</p><ul><li><p>Construct mel filter banks</p></li><li><p>又有五小步</p><ul><li>首先，将stft变换后的频率的min和max转换到mel频率（使用上面的公式）</li><li>假设取了M个mel bands，包括min,max共M+2个点，将上面的min和max的mel频带分成M+1份</li><li>然后把这M个点使用上面的mel2freq转换回频率</li><li>然后把这些频率近似到最近的frequency bin（stft中每个采样点取的离散频率）</li><li>然后按照这些得到的映射在频率单位的点构建三角滤波器（好复杂不想看）</li></ul></li><li><p>Apply mel filter banks to spectrogram</p></li></ul><p>这里有两个矩阵，一个是梅尔滤波矩阵，乘上功率谱矩阵，就得到了梅尔谱图</p><h3 id="Mel-Frequency-Cepstral-Coefficients"><a href="#Mel-Frequency-Cepstral-Coefficients" class="headerlink" title="Mel-Frequency Cepstral Coefficients"></a>Mel-Frequency Cepstral Coefficients</h3><p>计算cepstrum（倒谱率）</p><script type="math/tex; mode=display">C(x(t))=F^{-1}[log(F[x(t)])]</script><p>其中 $x(t)$ 是时域信号，$F[\cdot]$ 是傅里叶变换，不是很懂为什么这里明明是傅里叶的逆变换但是视频里说相当于对频谱图做了一次傅里叶变换得到了 the spectrogram of the spectrogram</p><p>先给了一个结论，语音就是声道频率响应和声门激励的卷积</p><p>时域信号:  $x(t)=e(t)\cdot h(t)$</p><p>频域信号:  $X(t)=E(t)\cdot H(t)$ </p><p>e和h分别是声门激励(glotta pulse)和声道频率响应(vocal tract frequency response)</p><p><img src="/2023/08/23/%E9%9F%B3%E9%A2%91%E4%BF%A1%E5%8F%B7%E5%A4%84%E7%90%86/image-20230831162304687.png" alt="image-20230831162304687"></p><p>对上面那个公式两边取对数</p><script type="math/tex; mode=display">log(X(t))=log(E(t))+log(H(t))</script><p>可以把这两个部分分开</p><p>然后我们语音分析所要的信息，音色，语义等都包含在声道频率响应里，对mel频谱进行逆变换后得到的倒谱系数 $h(t)$ 就称为Mel频率倒谱系数，简称MFCC</p><p>然后有一种不用iDFT的方法，叫做离散余弦变换（DCT, Discrete Cosine Transform）</p><p><img src="/2023/08/23/%E9%9F%B3%E9%A2%91%E4%BF%A1%E5%8F%B7%E5%A4%84%E7%90%86/image-20230831154825951.png" alt="image-20230831154825951"></p><p>因为不需要iDFT之后的虚数部分，只要实数</p>]]></content>
      
      
      <categories>
          
          <category> 声学相关 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 信号处理 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>数学知识</title>
      <link href="/2023/08/02/%E6%95%B0%E5%AD%A6%E7%9F%A5%E8%AF%86/"/>
      <url>/2023/08/02/%E6%95%B0%E5%AD%A6%E7%9F%A5%E8%AF%86/</url>
      
        <content type="html"><![CDATA[<h1 id="学习过程中遇到的数学知识"><a href="#学习过程中遇到的数学知识" class="headerlink" title="学习过程中遇到的数学知识"></a>学习过程中遇到的数学知识</h1><h2 id="傅里叶变换"><a href="#傅里叶变换" class="headerlink" title="傅里叶变换"></a>傅里叶变换</h2><p>网上已经有很多关于傅里叶变换在信号处理中的应用了，大概就是用一堆正弦余弦函数的叠加去模拟任何<strong>周期函数</strong>（必须满足狄利克雷条件）</p><blockquote><p>狄利克雷条件</p><ol><li>在一个周期内，连续或只有有限个第一类间断点</li><li>在一个周期内，极大值和极小值的数目应是有限个</li><li>在一个周期内，函数是绝对可积的</li></ol></blockquote><p>先搞懂傅里叶级数是干嘛的：</p><p>对于一个振荡系统，可以表述为：</p><script type="math/tex; mode=display">f(t)=A\sin(\omega t+\psi)</script><p>其中$t,A,\omega,\psi$分别为时间，振幅，角速度，初相，其中的角速度也可以用周期表示：$\omega=\frac{2\pi}{T}$</p><p>类比线性代数中的n维向量空间的正交基，对n维向量空间中的任意一个向量$\boldsymbol\eta$，总可以用标准正交基的线性组合表示</p><p><img src="/2023/08/02/%E6%95%B0%E5%AD%A6%E7%9F%A5%E8%AF%86/image-20230802145529990.png" alt="image-20230802145529990"></p><blockquote><p>函数正交的定义：如果两个函数在[a,b]正交，就是两个函数在[a,b]区间的每一个点的函数值相乘的和为零。</p></blockquote><p>将傅里叶级数写为一般公式：</p><script type="math/tex; mode=display">f(x)=\frac{a_0}{2}+\sum^{\infty}_{n=1}\left(a_n\,\cos\frac{2\pi nx}{T}+b_n\,\sin\frac{2\pi nx}{T}\right)\qquad(1)\\a_n=\frac{2}{T}\int^{x_0+T}_{x_0}f(x)\cos\frac{2\pi nx}{T}dx\\b_n=\frac{2}{T}\int^{x_0+T}_{x_0}f(x)\sin\frac{2\pi nx}{T}dx</script><p>$a_n$推导如下，$b_n$类似，对公式(1)两边同乘$\sin\frac{2\pi kx}{T}$，再做(0,T)的积分：</p><script type="math/tex; mode=display">\int_0^T f(x)\sin\frac{2\pi kx}{T}dx = \int_0^T\frac{a_0}{2}\sin\frac{2\pi kx}{T}dx+\sum^\infty_{n=1}\left(\int_0^Ta_n\cos\frac{2\pi nx}{T}\sin\frac{2\pi kx}{T}dx+\int_0^Tb_n\sin\frac{2\pi n x}{T}\sin\frac{2\pi kx}{T}dx\right)</script><p>由上面那个正交积分为0的结论：</p><script type="math/tex; mode=display">\int_0^T\frac{a_0}{2}\sin\frac{2\pi kx}{T}dx=0\\\int_0^Ta_n\cos\frac{2\pi nx}{T}\sin\frac{2\pi kx}{T}dx=0\\\int_0^Tb_n\sin\frac{2\pi n x}{T}\sin\frac{2\pi kx}{T}dx=0\qquad n\neq k</script><p>化简得到：</p><script type="math/tex; mode=display">\int_0^T f(x)\sin\frac{2\pi kx}{T}dx = \int_0^Tb_n\left(\sin\frac{2\pi nx}{T}\right)^2 dx\\=\int_0^Tb_n\frac{1-\cos\frac{4\pi nx}{T}}{2}dx\\=\frac{T}{2}b_n</script><p><strong>傅里叶级数与幅度相位之间的关系</strong></p><script type="math/tex; mode=display">c_n=\sqrt{a_n^2+b_n^2}\\\varphi=\arctan\left(-\frac{b_n}{a_n}\right)</script><p>接下来推到离散形式的傅里叶级数：</p><p>又有欧拉公式，连接复数域与三角的桥梁：</p><script type="math/tex; mode=display">e^{i\varphi}=\cos\varphi + i\,\sin\varphi</script><p>关于这个复数 i 的理解，可以见下面这篇文章的说法，在实数轴中旋转90°，将实数轴变成复平面</p><p>概念理解：<a href="https://zhuanlan.zhihu.com/p/317237264">通俗易懂的理解傅里叶变换</a></p><p>将上面公式的相位 $\varphi$ 用 $\omega t$ 替换</p><script type="math/tex; mode=display">e^{i\omega t} = \cos \omega t+ i\,\sin\omega t</script><p>设一组三角函数，频率为 $\cos \omega t$的n倍，那么可以定义这一组三角函数为：</p><script type="math/tex; mode=display">\cos\,n\omega t=\frac{e^{in\omega t}+e^{-in\omega t}}{2}\\\sin\,n\omega t=\frac{e^{in\omega t}-e^{-in\omega t}}{2i}</script><p>代回式(1)中，得到（这里把自变量换成t了）：</p><script type="math/tex; mode=display">f(t)=\frac{a_0}{2}+\sum^\infty_{n=1}\left(a_n\frac{e^{in\omega t}+e^{-in\omega t}}{2}+b_n\frac{e^{in\omega t}-e^{-in\omega t}}{2i}\right)</script><p>合并同类项得：</p><script type="math/tex; mode=display">f(t)=\frac{a_0}{2}+\sum^{\infty}_{n=1}\left[\left(\frac{a_n}{2}+\frac{b_n}{2i}\right)\,e^{in\omega t}+\left(\frac{a_n}{2}-\frac{b_n}{2i}\right)\,e^{-in\omega t}\right]\quad(2)</script><p>又有：</p><script type="math/tex; mode=display">a_{-n}=\frac{2}{T}\,\int_0^Tf(x)\,\cos -n\omega x\,dx=a_n\\b_{-n}=\frac{2}{T}\,\int_0^Tf(x)\,\sin -n\omega x\,dx=-b_n</script><p>所以式（2）可以变换为：</p><script type="math/tex; mode=display">f(t)=\frac{a_0}{2}+\sum^\infty_{n=1}\left(\frac{a_n}{2}+\frac{b_n}{2i}\right)e^{in\omega t}+\sum^\infty_{n=1}\left(\frac{a_{-n}}{2}+\frac{b_{-n}}{2i}\right)e^{-in\omega t}\\=\frac{a_0}{2}+\sum^\infty_{n=1}\left(\frac{a_n}{2}+\frac{b_n}{2i}\right)e^{in\omega t}+\sum_{-\infty}^{n=-1}\left(\frac{a_{n}}{2}+\frac{b_{n}}{2i}\right)e^{in\omega t}\\</script><p>这里直流分量 $\frac{a_0}{2}$ 对应的是n=0的情况，所以上式可化简为：</p><script type="math/tex; mode=display">f(t) =\sum^\infty_{-\infty}\left(\frac{a_n}{2}+\frac{b_n}{2i}\right)e^{in\omega t}</script><p>高中就学过 $i^2=-1$ ，所以 $\frac{b_n}{2i}=-\frac{ib_n}{2}$ ，最终得到：</p><script type="math/tex; mode=display">f(t) =\sum^\infty_{-\infty}\frac{\left(a_n-ib_n\right)}{2}e^{in\omega t}</script><p>令 <script type="math/tex">c_n=\frac{(a_n-ib_n)}{2}</script> ，最终得到：</p><script type="math/tex; mode=display">f(t)=\sum^\infty_{-\infty}c_n\,e^{in\omega t} \qquad (3)</script><p>可同样由正交性推导到$c_n$的公式：</p><script type="math/tex; mode=display">c_n=\frac1T\int^T_0f(t)e^{-in\omega t}dt</script><p>这个公式表达的意思，按照我的理解，就是一个信号频率和幅度的分布</p><p>首先这是一堆值的连加，频率由第二项中的$\omega$反映，$f=\frac{\omega}{2\pi}$ ，幅度由 $c_n$ 反映</p><script type="math/tex; mode=display">\left|c_n\right|=\frac12\sqrt{a_n^2+b_n^2}=\frac12c_n</script><p>而通过对其虚部与实部反正切,就可以求得该频率波的相位</p><p><strong>注意注意注意！上面那个只是复数形式的傅里叶级数</strong></p><p><img src="https://pic4.zhimg.com/80/v2-aa528e3bd3871dfc7292242ed282bce3_1440w.webp" alt="img"></p><p>上面来源知乎的图，说明了傅里叶级数到傅里叶变换的关系</p><ul><li><p>周期函数可以根据傅里叶级数画出频域图</p></li><li><p>当周期函数周期变大，频率图逐渐变得密集</p></li><li>$T=\infty$, 得到傅里叶变换，频域图变为连续的曲线</li></ul><p>我们把公式 (3) 用周期的形式重写一遍：</p><script type="math/tex; mode=display">f(x)=\sum^\infty_{n=-\infty}\;c_n\cdot e^{i\frac{2\pi nx}{T}} \qquad (4)</script><p>这里套用最开头说的N维空间正交基的概念，$e^{i\frac{2\pi nx}{T}}$是正交基， $c_n$ 是基坐标</p><p>当 $T\rightarrow \infty$ 时，$n \rightarrow \infty$ 频域的一个个离散的线变的连续，对每个线段的累加就变成了对整个频域的积分</p><p>由上面推导的 $c_n$ 的公式，带入(4) 中：</p><script type="math/tex; mode=display">\lim_{T\rightarrow \infty}f(t)=\lim_{T\rightarrow \infty}\sum^\infty_{n=-\infty}\; \frac1T\int^T_0f(t)e^{\frac{-in 2\pi t}{T}}dt\cdot e^{i\frac{2\pi nx}{T}} \qquad (5)</script><p><del>推不出来</del></p><p>把 $\frac{2\pi}{T}$ 写成 $\omega$ , 当 $T\rightarrow\infty$ 时，$\omega \rightarrow \infty$ ，引入一个 $\Delta \omega=(n+1)\omega - n\omega=\omega$ 的概念</p><script type="math/tex; mode=display">(5)=\lim_{T\rightarrow \infty}\sum^\infty_{n=-\infty}\; \frac{\Delta \omega}{2\pi}\int^T_0f(t)e^{-in\omega t}dt\cdot e^{in\omega t}</script><p>把 $n\omega$ 看成 W，把 $\Delta \omega$ 当作积分，得到：</p><script type="math/tex; mode=display">f(t)=\frac{1}{2\pi}\int^{+\infty}_{-\infty}(\int^{+\infty}_{-\infty}f(t)e^{-iWt}dt)e^{iWt}dW</script><p>其中 $\int^{+\infty}_{-\infty}f(t)e^{-tWt}dt$ 的积分变量是t，是W的函数，令</p><script type="math/tex; mode=display">F(W)=\int^{+\infty}_{-\infty}f(t)e^{-iWt}dt\qquad (6)</script><p>(6) 就是 $f(t)$ 的傅里叶变换，把 $F(W)$ 带入，得到傅里叶变换的逆变换</p><script type="math/tex; mode=display">f(t)=\frac{1}{2\pi}\int^{+\infty}_{-\infty}F(W)e^{iWt}dW</script><h2 id="离散傅里叶变换"><a href="#离散傅里叶变换" class="headerlink" title="离散傅里叶变换"></a>离散傅里叶变换</h2><p>由于计算机是一个离散系统，而上面的傅里叶变换及其逆变换是一个连续的积分，为了在计算机中进行信号分析，引入了离散傅里叶变换</p><p>首先了解离散时间傅里叶变换</p><h3 id="离散时间傅里叶变换"><a href="#离散时间傅里叶变换" class="headerlink" title="离散时间傅里叶变换"></a>离散时间傅里叶变换</h3><p>对连续信号 $x(t)$ ，使用采样频率 $f_s$ 进行采样，采样点时间间隔为 $T_s=\frac{1}{f_s}$ 冲击采样序列为：</p><script type="math/tex; mode=display">\delta_s(t)=\sum^{\infty}_{n=-\infty}\delta(t-nT_s)</script><p>取样后的信号为：</p><script type="math/tex; mode=display">x_s(t)=\sum^{\infty}_{n=-\infty}x(t)\delta(t-nT_s)</script><p>连续信号的傅里叶变换公式是(6)，则采样后的形式为：</p><script type="math/tex; mode=display">F(W)=\int^{\infty}_{-\infty}(\sum^{\infty}_{n=-\infty}x(t)\delta(t-nT_s))e^{-i\omega t}dt</script><p>交换积分和求和的顺序，然后由 $\delta(t)$ 的筛选性（冲击函数在 $x=0$ 处的取值为正无穷，然后在整个时域上的积分为1，所以乘一个函数在整个时域上的积分就会等于这个函数在冲激函数取0时对应的值？）</p><script type="math/tex; mode=display">\int^{+\infty}_{-\infty}f(x)\delta(x-x_0)dx=f(x_0)</script><p>得</p><script type="math/tex; mode=display">F_s(W)=\sum^{\infty}_{n=-\infty}x(nT_s)e^{-i\omega n T_s}</script><p>在python中，我们读入的波形文件是一个按照采样率得到的时序序列 $x(n)$ ，每个采样点间的时间间隔为 $\frac{1}{sampling\,rate}$ ，最后用这个时序序列替代上面的方程，最后得到：</p><script type="math/tex; mode=display">X_s(\omega)=\sum^{\infty}_{n=-\infty}x(n)e^{-i\omega n T_s}\qquad (7)</script><p>式(7) 称为离散时间傅里叶变换，简称DTFT</p><h3 id="离散傅里叶变换-1"><a href="#离散傅里叶变换-1" class="headerlink" title="离散傅里叶变换"></a>离散傅里叶变换</h3><p>我们的目标是将连续的信号转换为计算机能够处理的数据，而式(7)中虽然对时域信号离散化了，但是在频域上是对无穷个采样点进行类和，而计算机只能处理有限的数据，所以频域的处理是将无限变成有限，DFT的处理是将无限长的离散信号进行截短至N个采样点，然后将这N个采样点进行周期延拓，变成周期信号，这样其频率就离散了</p><p>对连续信号进行N次采样后的 $x_s(t)$ ，进行周期延拓，其周期为 $T=N\times T_s$ ，频率为 $f=\frac{2\pi}{T}$ 在一个周期T内，表达式为：</p><script type="math/tex; mode=display">x_s(t)=\sum^{\infty}_{n=-\infty}x(t)\delta(t-nT_s)</script><p>令 $\omega=f=\frac{2\pi}{T}$ 对式 (6) ，将积分区间限制为单个周期，信号 $f(t)$ 用上面采样的 $x_s(t)$ 进行替换，且将连续频率使用基准频率 $f$ 的倍数来表示：</p><script type="math/tex; mode=display">X[k\omega]=\frac{1}{T}\int^T_0(\sum^{N-1}_{n=0}x(t)\delta(t-nT_s))e^{-i\frac{2\pi}{T}kt}dt</script><p>然后套用冲击函数的筛选性，最后得到：</p><script type="math/tex; mode=display">X[k\omega]=\frac{1}{T}\sum ^{N-1}_{n=0}x(nT_s)e^{-i\frac{2\pi}{NT_s}knT_s}=\frac{1}{NT_s}\sum^{N-1}_{n=0}x(nT_s)e^{-i\frac{2\pi}{N}kn}</script><p>令</p><script type="math/tex; mode=display">X[k\omega]\cdot T_s=X[k]</script><p>得</p><script type="math/tex; mode=display">X[k]=\frac1N\sum^{N-1}_{n=0}x[n]e^{-i\frac{2\pi}{N}kn}\qquad(8)</script><p>这就是离散周期信号的傅里叶变换</p><p>同时由 $\omega$ 的定义，离散周期傅里叶变换后的第k个数对应的频率是为 $f_k=\frac{k\omega}{2\pi}$</p><script type="math/tex; mode=display">k\omega=kf=k\frac{2\pi}{T}=k\frac{2\pi}{NT_s}=\frac{k}{N}2\pi f_s</script><p>其中 $f_s$ 是采样频率，所以</p><script type="math/tex; mode=display">f_k=\frac kN f_s</script><h2 id="短时傅里叶变换"><a href="#短时傅里叶变换" class="headerlink" title="短时傅里叶变换"></a>短时傅里叶变换</h2><p>涉及到分帧加窗的概念，对于非平稳信号，傅里叶变换无法体现信号在时间上的变化信息，只有仅仅关心信号中是否包含某个频率分量而不关心它出现的时间的时候，傅里叶变换才可以用于处理非平稳信号</p><p>而STFT就是在傅里叶变换中加了一维的自由度（帧），对每帧进行傅里叶变换，使频谱可以反映其不同时间所处的频率状态</p><p>直接考虑在计算机中离散的情况，提出一个窗函数 $w[n]$ （一般有矩形窗，汉明窗，汉宁窗）</p><p>加窗的操作就是在傅里叶变换的基础上乘一个窗函数：</p><script type="math/tex; mode=display">X[k]=\sum^{N-1}_{n=0}w[n]x[n+iH]e^{-j\frac{2\pi}{N}kn}</script><p>H 是窗函数每次取帧移动的步长，一般为窗长的 25%-50%，N不再是总的采样次数而是窗长，因为是在每帧中做傅里叶变换，i是第几个窗</p><h2 id="范数"><a href="#范数" class="headerlink" title="范数"></a>范数</h2><p>是一种类似距离的定义，写作 $\Vert \boldsymbol x\Vert$ 或 $\Vert \boldsymbol X\Vert$ ，分别表示向量和矩阵</p><p>为方便统一，一般将任意向量 $\boldsymbol x$ 的 $l_p$ 范数定义为</p><script type="math/tex; mode=display">\Vert \boldsymbol x\Vert_p = \sqrt[p]{\sum_i\vert x_i\vert^p}</script><p>当 $p=0$ 时，$\Vert \boldsymbol x\Vert_0$  表示向量 $\boldsymbol x$ 中非0元素的个数</p><p>为了解决过拟合问题，会在损失函数后面加上一个 $l_1$ 范数，挺复杂的，见 <a href="https://www.zhihu.com/tardis/zm/art/26884695?source_id=1005">链接</a></p><p>对于矩阵，使用Frobenius范数来描述类似的东西</p><script type="math/tex; mode=display">\Vert \boldsymbol X \Vert_F = \sqrt{\sum^m_{i=1}\sum^n_{j=1}x_{ij}^2}</script><h2 id="等错误率-EER"><a href="#等错误率-EER" class="headerlink" title="等错误率 EER"></a>等错误率 EER</h2><p>首先，给出结论，EER值越低，生物识别系统的准确度越高</p><p>以下将逐步引导给出EER的定义</p><h4 id="混淆矩阵"><a href="#混淆矩阵" class="headerlink" title="混淆矩阵"></a>混淆矩阵</h4><p>对于一个分类问题，模型给出的每个样本的预测为一个概率值，我们需要选择一个阈值 $\tau$ ，超出该阈值则为正例，低于则为反例，由此得到混淆矩阵</p><p>针对预测值和真实值之间的关系，可以将样本分为四个部分，分别是:</p><ul><li>真正例（True Positive, TP），预测值为1，真实值为1</li><li>假正例（False Positive, FP），预测值为1，真实值为0</li><li>真负例（True Negative, TN），预测值为0，真实值为0</li><li>假负例（False Negative, FN），预测值为0，真实值为1</li></ul><p>混淆矩阵定义为：</p><div class="table-container"><table><thead><tr><th>真实/预测</th><th>1</th><th>0</th></tr></thead><tbody><tr><td>1</td><td>TP</td><td>FN</td></tr><tr><td>0</td><td>FP</td><td>TN</td></tr></tbody></table></div><h4 id="ROC曲线"><a href="#ROC曲线" class="headerlink" title="ROC曲线"></a>ROC曲线</h4><p>ROC曲线的横纵坐标分别为假正例率（False Positive Rate）与真正例率（True Positive Rate），如下：</p><script type="math/tex; mode=display">TPR=\frac{TP}{TP+FN}\\FPR=\frac{FP}{TN+FP}</script><p>以FPR为x轴，TPR为y轴画图，就得到了ROC曲线。</p><p><img src="https://ask.qcloudimg.com/http-save/yehe-2144603/3vgpjjb0hm.png" alt="img"></p><h4 id="等错误率"><a href="#等错误率" class="headerlink" title="等错误率"></a>等错误率</h4><p>预测正确的概率（TPR）和预测错误的概率（TNR）相同时的TPR和TNR，TNR=1-FPR</p>]]></content>
      
      
      <categories>
          
          <category> 查阅用 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 傅里叶变换 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
