<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0"><title>音乐生成隐写 | 脚踏车的日志站</title><meta name="author" content="脚踏车没有脚"><meta name="copyright" content="脚踏车没有脚"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="音乐生成隐写先列一下要做的几件事找近三年文生音乐的论文找音频隐写，视频隐写，图像隐写的论文 音乐生成应该多找点，然后筛48k的暂未录用：High Fidelity Text-Guided Music Generation and Editing via Single-Stage Flow Matchingtext controlled, 48k stereo，支持文本细粒度控制，支持长时间的风格转">
<meta property="og:type" content="article">
<meta property="og:title" content="音乐生成隐写">
<meta property="og:url" content="http://yypyyds.github.io/2024/07/08/%E9%9F%B3%E4%B9%90%E7%94%9F%E6%88%90%E9%9A%90%E5%86%99/index.html">
<meta property="og:site_name" content="脚踏车的日志站">
<meta property="og:description" content="音乐生成隐写先列一下要做的几件事找近三年文生音乐的论文找音频隐写，视频隐写，图像隐写的论文 音乐生成应该多找点，然后筛48k的暂未录用：High Fidelity Text-Guided Music Generation and Editing via Single-Stage Flow Matchingtext controlled, 48k stereo，支持文本细粒度控制，支持长时间的风格转">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://yypyyds.github.io/img/touxiang.jpg">
<meta property="article:published_time" content="2024-07-08T08:20:58.000Z">
<meta property="article:modified_time" content="2024-08-06T07:35:40.134Z">
<meta property="article:author" content="脚踏车没有脚">
<meta property="article:tag" content="隐写">
<meta property="article:tag" content="生成">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://yypyyds.github.io/img/touxiang.jpg"><link rel="shortcut icon" href="/img/favicon.ico"><link rel="canonical" href="http://yypyyds.github.io/2024/07/08/%E9%9F%B3%E4%B9%90%E7%94%9F%E6%88%90%E9%9A%90%E5%86%99/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":true,"languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  }
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '音乐生成隐写',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-08-06 15:35:40'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
    win.getCSS = url => new Promise((resolve, reject) => {
      const link = document.createElement('link')
      link.rel = 'stylesheet'
      link.href = url
      link.onload = () => resolve()
      link.onerror = () => reject()
      document.head.appendChild(link)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 6.3.0">
<style>.github-emoji { position: relative; display: inline-block; width: 1.2em; min-height: 1.2em; overflow: hidden; vertical-align: top; color: transparent; }  .github-emoji > span { position: relative; z-index: 10; }  .github-emoji img, .github-emoji .fancybox { margin: 0 !important; padding: 0 !important; border: none !important; outline: none !important; text-decoration: none !important; user-select: none !important; cursor: auto !important; }  .github-emoji img { height: 1.2em !important; width: 1.2em !important; position: absolute !important; left: 50% !important; top: 50% !important; transform: translate(-50%, -50%) !important; user-select: none !important; cursor: auto !important; } .github-emoji-fallback { color: inherit; } .github-emoji-fallback img { opacity: 0 !important; }</style>
</head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/touxiang.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">67</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">61</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">9</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header"><nav id="nav"><span id="blog-info"><a href="/" title="脚踏车的日志站"><span class="site-name">脚踏车的日志站</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">音乐生成隐写</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-07-08T08:20:58.000Z" title="发表于 2024-07-08 16:20:58">2024-07-08</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-08-06T07:35:40.134Z" title="更新于 2024-08-06 15:35:40">2024-08-06</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/">论文笔记</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="音乐生成隐写"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="音乐生成隐写"><a href="#音乐生成隐写" class="headerlink" title="音乐生成隐写"></a>音乐生成隐写</h1><p>先列一下要做的几件事<br>找近三年文生音乐的论文<br>找音频隐写，视频隐写，图像隐写的论文</p>
<h2 id="音乐生成"><a href="#音乐生成" class="headerlink" title="音乐生成"></a>音乐生成</h2><p>应该多找点，然后筛48k的<br>暂未录用：<br><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2407.03648">High Fidelity Text-Guided Music Generation and Editing via Single-Stage Flow Matching</a><br>text controlled, 48k stereo，支持文本细粒度控制，支持长时间的风格转换，没开源</p>
<p><a target="_blank" rel="noopener" href="https://www.arxiv.org/abs/2407.02049">Accompanied Singing Voice Synthesis with Fully Text-controlled Melody</a><br>text, MIDI controlled, 伴奏合成, 24k mono, 合成人声和伴奏，听着对不太上，没开源</p>
<p>ICML2024: <a target="_blank" rel="noopener" href="https://openreview.net/pdf?id=kOczKjmYum">MusicFlow: Cascaded Flow Matching for Text Guided Music Generation</a></p>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2306.05284">MusicGen: Simple and Controllable Music Generation </a> <a target="_blank" rel="noopener" href="https://github.com/facebookresearch/audiocraft">code</a><br><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2308.01546">MusicLDM: Enhancing Novelty in Text-to-Music Generation Using Beat-Synchronous Mixup Strategies</a> <a target="_blank" rel="noopener" href="https://github.com/RetroCirce/MusicLDM/">code</a> 只放了推理代码<br><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2310.17162">CONTENT-BASED CONTROLS FOR MUSIC LARGE LANGUAGE MODELING</a> <a target="_blank" rel="noopener" href="https://github.com/Kikyo-16/coco-mulla-repo">code</a><br>只有推理代码<br><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2311.08355">Mustango: Toward Controllable Text-to-Music Generation</a> <a target="_blank" rel="noopener" href="https://github.com/AMAAI-Lab/mustango">code</a><br>多条件文本可控，UNet为Music-Domain-Knowledge-Informed UNet</p>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2401.04577">Masked Audio Generation using a Single Non-Autoregressive Transformer</a> <a target="_blank" rel="noopener" href="https://github.com/facebookresearch/audiocraft/blob/main/docs/MAGNET.md">code</a><br>自回归生成音频，也包括音乐</p>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2402.06178">MusicMagus: Zero-Shot Text-to-Music Editing via Diffusion Models</a> <a target="_blank" rel="noopener" href="https://github.com/ldzhangyx/MusicMagus">code</a><br>修改文本生成音乐的部分音乐元素</p>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2402.04825">Fast Timing-Conditioned Latent Audio Diffusion</a> <a target="_blank" rel="noopener" href="https://github.com/Stability-AI/stable-audio-tools">code</a><br>长时立体声生成<br><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2404.10301">Long-form music generation with latent diffusion</a> <a target="_blank" rel="noopener" href="https://github.com/Stability-AI/stable-audio-tools/">code</a><br>这两篇工作都是SD他们的，然后代码也是一个仓库，不知道什么情况，然后这个二代更长了</p>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.18386">Instruct-MusicGen: Unlocking Text-to-Music Editing for Music Language Models via Instruction Tuning</a> <a target="_blank" rel="noopener" href="https://github.com/ldzhangyx/instruct-musicgen">code</a><br>文本诱导音乐编辑</p>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2405.18503">SoundCTM: Uniting Score-based and Consistency Models for Text-to-Sound Generation</a> <a target="_blank" rel="noopener" href="https://github.com/sony/soundctm">code</a><br>快速生成音乐，在低采样步的情况下尽量保持生成质量</p>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2406.08384">Diff-A-Riff: Musical Accompaniment Co-creation via Latent Diffusion Models</a><br>没开源，做的伴奏生成</p>
<p>首先明确一下目标，这么多工作，需要能生成wav文件的<br>风格转换，歌声转换也可以看看？</p>
<h2 id="生成式隐写"><a href="#生成式隐写" class="headerlink" title="生成式隐写"></a>生成式隐写</h2><p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/document/10418202">Provably Secure Public-Key Steganography Based on Elliptic Curve Cryptography</a><br>ECC+图像生成模型<br><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/document/10418193">Toward Secure and Robust Steganography for Black-Box Generated Images</a><br>针对黑盒系统生成的图片的隐写，什么稳健系数<br><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/document/10306313">Provably Secure Robust Image Steganography</a><br>GAN隐变量映射密文隐写<br><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/document/10179287">Discop: Provably Secure Steganography in Practice Based on “Distribution Copies”</a><br>在扩散模型的分布采样中设置分布副本作为隐写信息源<br><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/document/9931443">Cover Reproducible Steganography via Deep Generative Models</a><br>没太看懂<br><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/document/9477049">Distribution-Preserving Steganography Based on Text-to-Speech Generative Models</a><br>使用生成模型保护隐写分布<br><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/abstract/document/10354433">Steganography With Generated Images: Leveraging Volatility to Enhance Security</a><br>根据生成模型生成图像的分布掩盖隐写修改<br><a target="_blank" rel="noopener" href="https://www.mdpi.com/2227-7390/12/4/615">Image Steganography and Style Transformation Based on Generative Adversarial Network</a><br>通过风格转换这个变换通道隐藏信息，使用GAN训练降低隐写分析准确率</p>
<h2 id="生成式水印"><a href="#生成式水印" class="headerlink" title="生成式水印"></a>生成式水印</h2><p><a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content/CVPR2024/html/Yang_Gaussian_Shading_Provable_Performance-Lossless_Image_Watermarking_for_Diffusion_Models_CVPR_2024_paper.html">Gaussian Shading: Provable Performance-Lossless Image Watermarking for Diffusion Models</a>无需训练，没有性能损失的水印方案<br><a target="_blank" rel="noopener" href="https://dl.acm.org/doi/abs/10.1145/3581783.3612448">Flexible and Secure Watermarking for Latent Diffusion Model</a><br>在前向传播过程中融合信息矩阵和中间输出，将水印信息嵌入，无需再次训练LDM</p>
<p>7.12小组会<br>STC 嵌入代价</p>
<h1 id="生成式隐写研究综述"><a href="#生成式隐写研究综述" class="headerlink" title="生成式隐写研究综述"></a>生成式隐写研究综述</h1><p><img src="/2024/07/08/%E9%9F%B3%E4%B9%90%E7%94%9F%E6%88%90%E9%9A%90%E5%86%99/image-20240709111739805.png" alt=""></p>
<p><img src="/2024/07/08/%E9%9F%B3%E4%B9%90%E7%94%9F%E6%88%90%E9%9A%90%E5%86%99/image-20240709112306614.png" alt=""><br>都是老方法，我想找新的，基于扩散模型的生成方法</p>
<h1 id="CRoSS-Diffusion-Model-Makes-Controllable-Robust-and-Secure-Image-Steganography"><a href="#CRoSS-Diffusion-Model-Makes-Controllable-Robust-and-Secure-Image-Steganography" class="headerlink" title="CRoSS: Diffusion Model Makes Controllable, Robust and Secure Image Steganography"></a>CRoSS: Diffusion Model Makes Controllable, Robust and Secure Image Steganography</h1><p><img src="/2024/07/08/%E9%9F%B3%E4%B9%90%E7%94%9F%E6%88%90%E9%9A%90%E5%86%99/image-20240712113759215.png" alt=""><br>现有方法无论有没有cover，都不能在安全性、可控性和鲁棒性方面达到很好的统一</p>
<p>使用可控的扩散模型，可以天然的适配这些问题<br>安全性：利用DDIM可逆技术进行基于扩散的图像翻译，保证了翻译过程的可逆性。这种可逆的翻译过程实现了无cover的隐写框架，确保了隐藏图像的安全性。<br>可控性：条件扩散模型强大的控制能力使容器图像具有高度可控性，扩散模型的生成先验保证了容器图像的视觉质量;<br>鲁棒性：扩散模型本质上是高斯去噪器，对噪声和扰动具有天然的鲁棒性。即使在传输过程中容器图像被加噪，我们仍然可以揭示秘密图像的主要内容。</p>
<p>很重要，无需任何额外的训练！围绕SD社区，提出了基于lora，controlnet，prompts的变体</p>
<h2 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h2><h3 id="Definition-of-Image-Steganography"><a href="#Definition-of-Image-Steganography" class="headerlink" title="Definition of Image Steganography"></a>Definition of Image Steganography</h3><p><img src="/2024/07/08/%E9%9F%B3%E4%B9%90%E7%94%9F%E6%88%90%E9%9A%90%E5%86%99/image-20240709161312159.png" alt=""><br>$X_{sec}$ 是需要隐写的秘密图像，通过 <code>Hide Process</code> 嵌入容器图像 $X_{cont}$ ，在网络传播过程中，容器图像可能会遭遇损失，导致质量下降，变为 $X’_{cont}$ ，但是经过 <code>Reveal Process</code> 之后，可以得到语义一致的结果。</p>
<h3 id="Invertible-Image-Translation-using-Diffusion-Model"><a href="#Invertible-Image-Translation-using-Diffusion-Model" class="headerlink" title="Invertible Image Translation using Diffusion Model"></a>Invertible Image Translation using Diffusion Model</h3><p>本方法使用Diffusion达成可提取性的方法是，把扩散采样DDIM过程中的噪声去掉，变成一个固定的常微分方程求解<br><strong>Diffusion Model Defined by DDIM</strong><br>原始的DDIM去噪过程可表示为</p>
<script type="math/tex; mode=display">
x_s=\sqrt{\bar \alpha_s} \boldsymbol{f}_{\theta}(x_t,t)+\sqrt{1-\bar \alpha_s - \sigma^2_s}\; \epsilon_{\theta}(x_t,t) + \sigma_s \epsilon</script><p>当公式中的 $\sigma_s$ 设为 0 时，DDIM采样过程变为确定性。这时采样结果仅仅由 $x_t$ 确定。<br>采样过程可以等同于解决一个常微分方程</p>
<script type="math/tex; mode=display">
x_0 = ODESolver(x_T;\epsilon_{\theta},T,0)</script><p>使用扩散模型进行图像转换<br><img src="/2024/07/08/%E9%9F%B3%E4%B9%90%E7%94%9F%E6%88%90%E9%9A%90%E5%86%99/image-20240712101558808.png" alt=""><br>加上控制条件</p>
<script type="math/tex; mode=display">
x_0=ODESolver(x_T;\epsilon_{\theta},c,T,0)</script><p>图像转换有两个重要的点：两张图片的结构一致性和转换过程是否可逆<br><img src="/2024/07/08/%E9%9F%B3%E4%B9%90%E7%94%9F%E6%88%90%E9%9A%90%E5%86%99/image-20240712103110712.png" alt=""><br><img src="/2024/07/08/%E9%9F%B3%E4%B9%90%E7%94%9F%E6%88%90%E9%9A%90%E5%86%99/image-20240712103118929.png" alt=""><br><img src="/2024/07/08/%E9%9F%B3%E4%B9%90%E7%94%9F%E6%88%90%E9%9A%90%E5%86%99/image-20240712105202624.png" alt=""></p>
<p>使用私钥作为控制条件，为秘密图像加噪，使用公钥作为控制条件，从噪声生成秘密信息载体。<br>接收方使用公钥加噪，得到噪声，再使用私钥作为控制条件，从噪声生成秘密信息。<br>为了实现图像的可逆转换，采用确定性DDIM倒转(Inversion)过程。</p>
<p>这里很有意思，有空要详细研究下DDIM的过程<br>这里提到上面的DDIM去噪过程，在 $s<t$ 时，进行的是反向过程；在="" $s="">t$ 时，进行的是正向过程，这两个过程的操作是相似的，输入和输出的图片也是相似的，这个中间噪声 $x_T$ 可以被视作倒转过程的隐变量。</t$></p>
<script type="math/tex; mode=display">
x_T = ODESolve(x_0;\epsilon_{\theta},c,0,T)\qquad x'_0=ODESolve(x_T;\epsilon_{\theta},c,T,0)</script><h3 id="The-Coverless-Image-Steganography-Framework-CRoSS"><a href="#The-Coverless-Image-Steganography-Framework-CRoSS" class="headerlink" title="The Coverless Image Steganography Framework CRoSS"></a>The Coverless Image Steganography Framework CRoSS</h3><p>和上面讲的差不多，就是用私钥掩盖秘密图像，公钥生成掩盖图像<br><strong>安全性</strong><br>使用不同prompt从噪声中生成的图像质量都很高，故攻击者很难从穷举的私钥中判断哪副生成的图片是真实的秘密图像。<br>载体不包含秘密信息。<br>Keys不但可以包含prompt，还可以选择ControlNet或LoRA<br>等于是把控制diffusion生成的方法都加进来了</p>
<h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>隐写分析<br><img src="/2024/07/08/%E9%9F%B3%E4%B9%90%E7%94%9F%E6%88%90%E9%9A%90%E5%86%99/image-20240712112552121.png" alt=""><br>使用Size-Independent-Detector 的分析结果<br><img src="/2024/07/08/%E9%9F%B3%E4%B9%90%E7%94%9F%E6%88%90%E9%9A%90%E5%86%99/image-20240712113043543.png" alt=""><br>NIQE是一个（no-reference image quality assessment）模型，用来评估自然程度和视觉安全性<br>右边三个是三种隐写分析模型<br><strong>抗压缩和噪声</strong><br><img src="/2024/07/08/%E9%9F%B3%E4%B9%90%E7%94%9F%E6%88%90%E9%9A%90%E5%86%99/image-20240712113259972.png" alt=""><br>信道损失<br><img src="/2024/07/08/%E9%9F%B3%E4%B9%90%E7%94%9F%E6%88%90%E9%9A%90%E5%86%99/image-20240712113517670.png" alt=""></p>
<p>总结<br>这也能发，纯纯风格转换拿来说是隐写</p>
<h1 id="Distribution-Preserving-Steganography-Based-on-Text-to-Speech-Generative-Models"><a href="#Distribution-Preserving-Steganography-Based-on-Text-to-Speech-Generative-Models" class="headerlink" title="Distribution-Preserving Steganography Based on Text-to-Speech Generative Models"></a>Distribution-Preserving Steganography Based on Text-to-Speech Generative Models</h1><p><img src="/2024/07/08/%E9%9F%B3%E4%B9%90%E7%94%9F%E6%88%90%E9%9A%90%E5%86%99/image-20240712153402896.png" alt=""><br>隐写安全性可以用cover和stego的相对熵量化<br>$D(P_c\Vert P_s)=0$ 意味着绝对的安全，攻击者无法区分cover和stego<br>设计了两种分布保持隐写方法<br>引入了一种 reversible flow-based generative model 作为采样器<br>将加密的数据映射到latent code，该空间符合高斯分布然后将 latent code送入生成模型，得到生成数据。<br>一旦生成模型得到良好的训练和固定，生成模型就可以生成遵循相同分布的数据，从而满足采样器的要求。生成模型可以从公共站点下载，例如github。接收方与发送方共享相同的生成模型，生成模型的可逆性保证了从生成的数据中正确提取消息。<br>本文以文本-语音生成模型WaveGlow为例验证其实际安全性。<br>对于基于压缩的隐写系统，采用自回归模型，因为他们生成数据的良好分布<br>看不懂<br>总之就是针对两个场景 基于采样的分布保持，基于压缩的分布保持 提出了两个隐写方案。<br><img src="/2024/07/08/%E9%9F%B3%E4%B9%90%E7%94%9F%E6%88%90%E9%9A%90%E5%86%99/image-20240712165457292.png" alt=""><br>通信模型<br>隐写安全性在信息论中，用cover和stego的分布的KL散度衡量<br><img src="/2024/07/08/%E9%9F%B3%E4%B9%90%E7%94%9F%E6%88%90%E9%9A%90%E5%86%99/image-20240712165703042.png" alt=""></p>
<h3 id="Existing-Distribution-Preserving-Methods"><a href="#Existing-Distribution-Preserving-Methods" class="headerlink" title="Existing Distribution-Preserving Methods"></a>Existing Distribution-Preserving Methods</h3><p>基于采样器的分布保持方法可以这样表示<br><img src="/2024/07/08/%E9%9F%B3%E4%B9%90%E7%94%9F%E6%88%90%E9%9A%90%E5%86%99/image-20240715113040335.png" alt=""><br>给定一个映射函数 $f_k(\cdot):K\times C \rightarrow R$，其中 $K,C,R$ 分别是 密钥空间，载体空间，和信息空间<br>使用密钥 $k$ 和 e 比特的标志 $b=\{0,1\}^e\in R$ ，嵌入隐写系统的消息是基于拒绝采样算法的<br>就是依托这个 $O^C$ 采样一个隐写载体 $s$ ，可以从里面提取出 b 来</p>
<p>另一类保持分布的隐写是基于压缩的隐写<br>基于压缩的方案需要知道确切的cover分布，这在数字媒体对象的分布中很难捕获。对于基于采样器的系统，很难获得完美的采样器，现有方案的容量也很低。为此，我们将生成模型引入到分布保持隐写术中。</p>
<h3 id="TTS-Models"><a href="#TTS-Models" class="headerlink" title="TTS Models"></a>TTS Models</h3><p>使用WaveGlow和WaveRNN<br>看不懂，先往后看吧</p>
<h2 id="Distribution-preserving-steganography-based-on-an-implicit-generative-model"><a href="#Distribution-preserving-steganography-based-on-an-implicit-generative-model" class="headerlink" title="Distribution-preserving steganography based on an implicit generative model"></a>Distribution-preserving steganography based on an implicit generative model</h2><p><img src="/2024/07/08/%E9%9F%B3%E4%B9%90%E7%94%9F%E6%88%90%E9%9A%90%E5%86%99/image-20240723104356397.png" alt=""><br>这个图上看，Waveglow生成wav需要一个高斯向量，消息嵌入就是通过对高斯向量的映射实现的</p>
<h3 id="Message-Mapping"><a href="#Message-Mapping" class="headerlink" title="Message Mapping"></a>Message Mapping</h3><p>经典的将均匀分布映射到高斯分布<br><img src="/2024/07/08/%E9%9F%B3%E4%B9%90%E7%94%9F%E6%88%90%E9%9A%90%E5%86%99/image-20240723110127401.png" alt=""><br>首先对长为 n 的秘密消息 $m’$ ，要生成latent code，然后用这个latent送到waveglow里做TTS<br>这个latent code是符合高斯分布的<br>将高斯分布的分布函数划分为 $2^p$ 个区间，然后得到在 $(-\infty, \infty)$ 上对应的 x 值的点<br>将秘密信息按照每组 p 比特划分<br>然后就是拒绝采样算法得到样本 s</p>
<h3 id="Message-Embedding-and-Extraction"><a href="#Message-Embedding-and-Extraction" class="headerlink" title="Message Embedding and Extraction"></a>Message Embedding and Extraction</h3><p>得到latent code之后，给定一段文本和一个说话人ID，SPN（spectrogram generation model）将他们转换为梅尔谱图 $F_{mel}$ ，然后使用一堆Flow模型生成wav<br><img src="/2024/07/08/%E9%9F%B3%E4%B9%90%E7%94%9F%E6%88%90%E9%9A%90%E5%86%99/image-20240723113238926.png" alt=""></p>
<p>提取就是Flow的逆过程<br><img src="/2024/07/08/%E9%9F%B3%E4%B9%90%E7%94%9F%E6%88%90%E9%9A%90%E5%86%99/image-20240723113948579.png" alt=""><br>消息通过消息映射的逆得到</p>
<h2 id="Distribution-preserving-steganography-based-on-an-explicit-generative-model"><a href="#Distribution-preserving-steganography-based-on-an-explicit-generative-model" class="headerlink" title="Distribution-preserving steganography based on an explicit generative model"></a>Distribution-preserving steganography based on an explicit generative model</h2><p>上面的是基于隐式生成模型的，这个是基于显示生成模型<br><img src="/2024/07/08/%E9%9F%B3%E4%B9%90%E7%94%9F%E6%88%90%E9%9A%90%E5%86%99/image-20240723145234299.png" alt=""><br>说这个数据隐藏和数据压缩是差不多的过程<br>说是自然媒体的分布很难获得，但是生成模型可以拟合自然数据的分布，可以解决上述问题</p>
<p>为了解决这个问题，我们设计了基于文本到语音自回归模型WaveRNN的隐写系统，该系统可以生成具有丰富语义的音频。此外，语义自然地嵌入到音频中，这意味着接收方可以获得与发送方相同的文本，这样它们就可以使用相同的WaveRNN模型重新实现相同的生成过程。<br>方案流程图<br><img src="/2024/07/08/%E9%9F%B3%E4%B9%90%E7%94%9F%E6%88%90%E9%9A%90%E5%86%99/image-20240723150917044.png" alt=""><br>这个WaveRNN都什么年代的论文了，18年的老东西</p>
<h3 id="Message-Embedding-and-Extraction-1"><a href="#Message-Embedding-and-Extraction-1" class="headerlink" title="Message Embedding and Extraction"></a>Message Embedding and Extraction</h3><p>这里涉及到一个自适应算术（adaptive arithmetic）<br>嵌入过程和解码相关，提取过程和编码相关<br>看不懂啊<br>$A=\{a_1,a_2,\cdots , a_m\}$ 是生成的音频样本值按一定顺序排列的字母表，什么意思<br>其中元素的分布概率为 $P=\{P(a_1),P(a_2),\cdots,P(a_m)\}$<br>分布函数为</p>
<script type="math/tex; mode=display">
F(a_i) = \sum^i_{k=1}P(a_k)</script><p><strong>Message embedding</strong><br>给定加密信息 $m’=[m_1m_2m_3\cdots m_L]$ ，可以被解释为一个小数 $q$ </p>
<script type="math/tex; mode=display">
m_1m_2m_3\cdots m_L \rightarrow q=0.m_1m_2m_3\cdots m_L = \sum^L_{i=1}m_i \cdot 2^{-i}</script><p>采用自适应算术解码算法，从区间 $[0,1)$ 开始，根据符号的概率 $P$ 将其细分为子区间 $[l,h)$<br>然后将和这个子区间相关的符号 $a_j$ ，其中 $q$ 在这个子区间内，并入隐写信息</p>
<script type="math/tex; mode=display">
y=y::a_j</script><p>其中 $::$ 表示将后面的符号附加到前面的向量中<br>符号表的概率 $P$ 定期更新<br>这个更新是按照设定更新的，预设好了一堆离散概率分布</p>
<p>然后按照下式根据更新的概率 $P$ 重新计算子区间</p>
<script type="math/tex; mode=display">
\begin{aligned}
h_k &= h_{k-1}+(h_{k-1}-l_{k-1}) *F(a_j)
\newline
l_k &= l_{k-1}+(h_{k-1}-l_{k-1}) *F(a_{j-1})
\end{aligned}</script><p>其中 $h_k$ 和 $l_k$ 是第k步<strong>子区间</strong>的上下界<br>重复这个过程，直到小数 $q$ 满足如下约束</p>
<script type="math/tex; mode=display">
\begin{aligned}
a+(0.5)^L &\notin[l_k,h_k)
\newline
q-(0.5)^L &\notin[l_k,h_k)
\end{aligned}</script><p>这个约束保证了二元分数 $q$ 是在区间 $[l_k,h_k)$ 中的唯一的长为 $L$ 的分数<br>这样可以正确的提取消息，信息长度 $L$ 和符号的概率 $P$ 与接收方共享<br><strong>Message extraction</strong><br>在接收端，区间 $[l,h)$ 从 $[0,1)$ 开始划分到和符号概率正比长度的子区间<br>如果第 k 个元素 $y_k$ 和符号 $a_j$ 相关，按照如下更新子区间</p>
<script type="math/tex; mode=display">
\begin{aligned}
h_k &= h_{k-1}+(h_{k-1}-l_{k-1}) *F(a_j)
\newline
l_k &= l_{k-1}+(h_{k-1}-l_{k-1}) *F(a_{j-1})
\end{aligned}</script><p>和上面一样<br>循环这个过程，直到次数达到和 $y$ 的长度一致<br>最后，找到满足 $l_L\leq q \leq h_L$ 的小数 $q=\sum^L_{i=1}m_i 2^{-i}$<br>其中 $m_i \in \{0,1\}$ 是消息比特，L 是消息长度</p>
<p>所以这些东西和基于压缩的隐写有什么关系呢？</p>
<h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><h3 id="Evaluation-Metric"><a href="#Evaluation-Metric" class="headerlink" title="Evaluation Metric"></a>Evaluation Metric</h3><p>主观指标：让人听听是否自然</p>
<p>这个主观评估，好复杂，看不懂一点<br>上文介绍到分布保持的定义是基于KL散度，因为KL散度会输出不稳定的结果，这里使用<code>energy distance</code>和<code>Wasserstein distance</code>来测量一阶分布（一阶分布是什么）<br>好像就是一阶矩，看公式<br>分布$u,v$的<code>energy distance</code>为<br><img src="/2024/07/08/%E9%9F%B3%E4%B9%90%E7%94%9F%E6%88%90%E9%9A%90%E5%86%99/image-20240730103519226.png" alt=""><br>其中$X,Y,X’,Y’$为独立随机变量<br>看不懂，什么<code>length of vector</code><br><code>Wasserstein distance</code>如下，也看不懂<br><img src="/2024/07/08/%E9%9F%B3%E4%B9%90%E7%94%9F%E6%88%90%E9%9A%90%E5%86%99/image-20240730104747567.png" alt=""><br>$\Gamma(u,v)$ 是边缘分布为 $u,v$ 的二元分布</p>
<p>高阶分布的表现是通过隐写分析判断的<br>选取 <code>combined feature of time-Markov and mel-frequency(CTM)</code>特征组合来评估安全性能，是SoTA特征，不同嵌入算法和载荷训练不同的分类器</p>
<p>最后的安全性使用平均错误率度量</p>
<p>为了说明隐写分析算法的有效性，引入基于修改的隐写算法作为比较，包括LSBM和DFR</p>
<h3 id="Sampler-Based-Stegosystem"><a href="#Sampler-Based-Stegosystem" class="headerlink" title="Sampler-Based Stegosystem"></a>Sampler-Based Stegosystem</h3><p>SPN使用的是Flowtron，然后用WaveGlow生成wav<br>几个距离如下，文章说这个数值很小，可以认为两个分布没有区别<br><img src="/2024/07/08/%E9%9F%B3%E4%B9%90%E7%94%9F%E6%88%90%E9%9A%90%E5%86%99/image-20240730191525538.png" alt=""><br>在LJSpeech数据集上，嵌入比特数和隐写分析错误率的图<br><img src="/2024/07/08/%E9%9F%B3%E4%B9%90%E7%94%9F%E6%88%90%E9%9A%90%E5%86%99/image-20240730191628855.png" alt=""></p>
<p><img src="/2024/07/08/%E9%9F%B3%E4%B9%90%E7%94%9F%E6%88%90%E9%9A%90%E5%86%99/image-20240730191641323.png" alt=""><br>嵌入比特数对性能的影响</p>
<h3 id="Compression-Based-Stegosystem"><a href="#Compression-Based-Stegosystem" class="headerlink" title="Compression-Based Stegosystem"></a>Compression-Based Stegosystem</h3><p>Flowtron+WaveRNN<br>两个数据集上的两个分布距离<br><img src="/2024/07/08/%E9%9F%B3%E4%B9%90%E7%94%9F%E6%88%90%E9%9A%90%E5%86%99/image-20240730192053680.png" alt=""><br><img src="/2024/07/08/%E9%9F%B3%E4%B9%90%E7%94%9F%E6%88%90%E9%9A%90%E5%86%99/image-20240730192059649.png" alt=""></p>
<p><img src="/2024/07/08/%E9%9F%B3%E4%B9%90%E7%94%9F%E6%88%90%E9%9A%90%E5%86%99/image-20240730192203504.png" alt=""></p>
<h1 id="Fast-Timing-Conditioned-Latent-Audio-Diffusion"><a href="#Fast-Timing-Conditioned-Latent-Audio-Diffusion" class="headerlink" title="Fast Timing-Conditioned Latent Audio Diffusion"></a>Fast Timing-Conditioned Latent Audio Diffusion</h1><p>Stable Diffusion团队，ICML2024<br>大多数之前的工作都没有解决音乐和音效在持续时间上自然变化的问题。（开局给我干蒙了<br>研究重点是使用text prompt生成长时间，可变长度的44.1kHZ立体声音乐和的音频。</p>
<p>之前的工作要么推理速度很慢（自回归模型），要么不能生成可变长度的长时间音频，该工作依赖于latent diffusion，能干这些事<br>提问，如何生成可变长度的长时间音频</p>
<h2 id="Related-works"><a href="#Related-works" class="headerlink" title="Related works"></a>Related works</h2><p><strong>Timming condition</strong><br>是第一个在扩散模型上使用可变时间控制的<br><strong>Evaluation metrics</strong><br>常用的定量音频指标是在评估短时单通道16kHz音频生成的，<br>提出了新的定量指标来评估长时全带立体声生成，还有音乐性，立体声正确性和音乐结构的定量指标<br><strong>Multitask generative modeling</strong><br>能够生成音乐和音效</p>
<h2 id="Architecture"><a href="#Architecture" class="headerlink" title="Architecture"></a>Architecture</h2><p><img src="/2024/07/08/%E9%9F%B3%E4%B9%90%E7%94%9F%E6%88%90%E9%9A%90%E5%86%99/image-20240718150315046.png" alt=""><br>分为三个部分，VAE，conditioning signal，diffusion model</p>
<h3 id="VAE"><a href="#VAE" class="headerlink" title="VAE"></a>VAE</h3><p>为了能够给任意长度的音频编解码，使用了一个<code>fully-convolutional</code>结构，来自Descript Audio Codec<br>压缩率为32，将 $2\times L$ 的输入压缩为 $64 \times L / 1024$ 长度的隐变量</p>
<h3 id="Conditioning"><a href="#Conditioning" class="headerlink" title="Conditioning"></a>Conditioning</h3><p><strong>Text encoder</strong><br>自己从头训练了一个CLAP，发现要比预训练的效果好<br>并且在SD的实验中，text encoder的输出中，倒数第二层的文本特征可以提供比最终输出更好的控制信号，这些文本特征通过交叉注意力层提供给UNet</p>
<p><strong>Timing embeddings</strong><br><img src="/2024/07/08/%E9%9F%B3%E4%B9%90%E7%94%9F%E6%88%90%E9%9A%90%E5%86%99/image-20240718153433837.png" alt=""><br>在搜集每一个音频块（从一段完整的音频上切出来的95 sec音频片段）的时候，计算两个属性，音频块的起始时间<code>seconds_start</code>，和原始音频的总时间<code>seconds_total</code>，对于比训练窗口段的音频样本，填充静音片段</p>
<h3 id="Diffuison-model"><a href="#Diffuison-model" class="headerlink" title="Diffuison model"></a>Diffuison model</h3><p>好大的Diffuison，907M参数<br>结构好像也有创新，但是看他的描述和这里引用的Schneider的论文不一样<br>4层对称的上下采样块+skip connection<br>通道数分别为 1024,1024,1024,1280<br>每一块在卷积后还有一系列自注意力和交叉注意力层<br>prompt和时间条件由cross-attention传入</p>
<h3 id="Inference"><a href="#Inference" class="headerlink" title="Inference"></a>Inference</h3><p>使用基于DPMSolver++的采样策略<br>可变长度是通过生成带静音片段的音频，然后修剪静音片段达到的</p>
<h2 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h2><h3 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h3><p>好多数据，806,284条audio共19500小时，使用来自AudioSparx的文本元数据</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>class</th>
<th>number of files</th>
<th>GBs of content</th>
</tr>
</thead>
<tbody>
<tr>
<td>music</td>
<td>66%</td>
<td>94%</td>
</tr>
<tr>
<td>sound effects</td>
<td>25%</td>
<td>5%</td>
</tr>
<tr>
<td>instrument</td>
<td>9%</td>
<td>1%</td>
</tr>
</tbody>
</table>
</div>
<h3 id="VAE-1"><a href="#VAE-1" class="headerlink" title="VAE"></a>VAE</h3><p>编解码器差分训练的，先共同训练，再单独继续训练解码器<br>为了保证立体声重建的一致性，专门为立体声信号设计了 <code>multi-resolution sum, difference STFT loss</code><br>为此，我们在STFT之前应用A-weighting，并使用2048、1024、512、256、128、64和32的窗口长度。我们还使用修改后的多尺度STFT鉴别器来接受立体声音频<br>判别器使用复数域的STFT表示和一个patch-based判别损失作为关键损失<br>这个STFT representation of the real and reconstructed audio不知道是干嘛的</p>
<h3 id="Prompt-presentation"><a href="#Prompt-presentation" class="headerlink" title="Prompt presentation"></a>Prompt presentation</h3><p>这个有说法的<br>对每个音频文件都有对应的自然语言描述，以及对于音乐的BPM，流派，情绪和音轨的乐器。<br>在text encoder和diffusion的训练过程中，通过将metadata的随机子串连接成字符串作为text prompt<br>这允许在推理过程中指定特定的属性，而不要求这些属性始终存在<br>数据分成两半，一般用结构表示 <code>Instruments:Gutar|Moods:Energetic</code>，一般用逗号隔开</p>
<h2 id="Methodology"><a href="#Methodology" class="headerlink" title="Methodology"></a>Methodology</h2><h3 id="Quantitative-metrics"><a href="#Quantitative-metrics" class="headerlink" title="Quantitative metrics"></a>Quantitative metrics</h3><p><strong>FD(Frechet Distance)</strong><br>不同于之前文章中使用VGGish计算的，这里用的Openl3模型，因为他接收的是48kHz的信号<br>将所有评估音频重新采样到44.1kHz，对于立体声，将左右声道独立的投影到Openl3的特征空间，然后拼接得到立体声特征，单通道的就复制一份</p>
<p><strong>KL</strong><br>使用PaSST来计算生成音频和参考音频标签的KL散度<br>对KL散度做了修改，以适应不同长度和更长的音频，包括将音频切割成重叠的分析窗口（10s，主要是要和PaSST训练的长度对齐，5s的重叠区域）<br>计算不同窗口同类别的均值，做softmax之后计算KL</p>
<p><strong>CLAP score</strong><br>使用CLAP LAION计算<br>从生成的长音频的，前，中，后，随机取三段，拼接起来重采样到48k</p>
<h3 id="Qualitative-metrics"><a href="#Qualitative-metrics" class="headerlink" title="Qualitative metrics"></a>Qualitative metrics</h3><p>Audio quality, Text alignment, Musicality Stereo correctness, Musical structure</p>
<h3 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h3><p>在Musiccaps上的结果<br><img src="/2024/07/08/%E9%9F%B3%E4%B9%90%E7%94%9F%E6%88%90%E9%9A%90%E5%86%99/image-20240721203615281.png" alt=""><br>分别是自编码器对音频保真度的影响<br>文本编码器的消融实验<br>和其他音频生成模型的对比<br><img src="/2024/07/08/%E9%9F%B3%E4%B9%90%E7%94%9F%E6%88%90%E9%9A%90%E5%86%99/image-20240721204754471.png" alt=""><br>时序控制</p>
<h3 id="思考"><a href="#思考" class="headerlink" title="思考"></a>思考</h3><p>音乐隐写求的是立体声高采样率？<br>生成式隐写和传统的音乐隐写，好就好在他可以控制生成<br>要同时利用音乐载体的高采样率带来的高隐写容量和生成式隐写对生成数据的分布可控性<br>那就是从带载体的分布保持相关的工作入手？</p>
<p>思考一个方法，如果我知道VAE的latent分布空间，通过加入嵌入比特，训练一个维护分布的模型，在输入嵌入比特后，输出对应的控制信息，让生成的数据保持分布</p>
<p>看看代码<br>先弄明白这几点<br>端到端的VAE是怎么运作的<br>控制信息是怎样传入的，没看到classifier-free guidance之类的</p>
<p>这个VAE啊，全是卷积，而且也没有采样过程，纯纯的ae<br>对于立体声，就是最后输出是双通道</p>
<p>每个控制条件（prompt，seconds_start，seconds_total）都有一个mask，暂时不知道这个mask是干嘛的<br></p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> key <span class="keyword">in</span> self.cross_attn_cond_ids:</span><br><span class="line">	cross_attn_in, cross_attn_mask = conditioning_tensors[key]</span><br></pre></td></tr></tbody></table></figure><p></p>
<p>然后在sequence dimension把这几个控制tensor连接起来<br><code>[128,768]+[1,768]+[1,768]=[130,768]</code></p>
<h1 id="Message-Driven-Generative-Music-Steganography-Using-MIDI-GAN"><a href="#Message-Driven-Generative-Music-Steganography-Using-MIDI-GAN" class="headerlink" title="Message-Driven Generative Music Steganography Using MIDI-GAN"></a>Message-Driven Generative Music Steganography Using MIDI-GAN</h1><p><img src="/2024/07/08/%E9%9F%B3%E4%B9%90%E7%94%9F%E6%88%90%E9%9A%90%E5%86%99/image-20240804151307698.png" alt=""><br>聚焦于MIDI生成的隐写</p>
<ul>
<li>我们提出了一种结合纠错码(error correction code, ECC)的消息映射机制，将二进制秘密消息转换为浮点数组来合成MIDI。特别是，所提出的机制可以确保相同的 bit/bits 每次映射到完全不同的和弦，这有利于抵御隐写分析。</li>
<li>我们开发了一种基于GAN的消息驱动生成式MIDI隐写方案，以逃避基于深度学习的隐写分析器的检测。在我们的方法中，称为MIDI-GAN，生成器将秘密信息转换为人工隐写MIDI文件，该文件体积小，旋律优美，并且无法被隐写分析器检测到。特别是，隐写MIDI还可以以和弦序列的形式传输，使其与现有的生成图像/音频隐写术不同，并且没有理由怀疑。此外，鉴别器使stego MIDI文件尽可能与真实的MIDI文件相似。提取器从隐写MIDI或和弦序列中恢复秘密消息。</li>
<li>在我们的MIDI-GAN中，我们考虑了生成器和鉴别器之间的对抗性学习，以及生成器和提取器之间的对抗性学习。这种增强使生成器、鉴别器和提取器相互竞争，共同进步，提高了隐蔽性、不可检测性和鲁棒性。<br><img src="/2024/07/08/%E9%9F%B3%E4%B9%90%E7%94%9F%E6%88%90%E9%9A%90%E5%86%99/image-20240804155154884.png" alt=""></li>
</ul>
<h2 id="proposed-scheme"><a href="#proposed-scheme" class="headerlink" title="proposed scheme"></a>proposed scheme</h2><p>信息载体的前提是载体的广泛使用，确保载体的存在或出现不会引起怀疑<br>基本框架如图2所示。首先，用纠错码(error correction code, ECC)对秘密信息进行编码，将其映射到浮点数组中，并在消息驱动生成隐写步骤中通过MIDI-GAN生成隐写MIDI文件。接下来，对于消息恢复，在MIDI-GAN架构中起关键作用的基于CNN的提取器，从隐音MIDI文件或和弦数序列中提取浮点数组。然后通过逆映射机制检索秘密消息，并由ECC进行校正。在随后的小节中，我们将进一步详细介绍MIDI-GAN方法。</p>
<h3 id="MIDI-File-Format-and-Chord-Dataset"><a href="#MIDI-File-Format-and-Chord-Dataset" class="headerlink" title="MIDI File Format and Chord Dataset"></a>MIDI File Format and Chord Dataset</h3><p>MIDI(Musical Instrucent Digital Interface)<br>乐器数字接口文件<br>MIDI文件的内容主要包括所演奏的音符，音符的持续时间以及每个音符所需的响度<br>一般MIDI文件中的音符编号范围从1到128，每个数字代表一个标准化的声音，可以用来在MIDI作曲中创建旋律。对于本研究中的MIDI合成，我们使用了一个开放的Pokemon MIDI数据集，其中包含来自的307个真实MIDI文件。然后使用music21工具包分析该数据集并生成一个和弦数据集，表示为Π。</p>
<script type="math/tex; mode=display">
\Pi =\{(1,N{t_1}),\dots,(n,Nt_n)\}</script><p>其中，i表示和弦的索引，n是总的和弦数，$Nt_i$ 表示和弦i中并发音符的集合<br>为了更方便的合成，每个和弦的持续时间统一设置为t秒</p>
<p>将这个索引和和弦之间的映射关系作为密钥，只要映射关系没泄露，隐写系统就是安全的<br><img src="/2024/07/08/%E9%9F%B3%E4%B9%90%E7%94%9F%E6%88%90%E9%9A%90%E5%86%99/image-20240804212401013.png" alt=""></p>
<h3 id="纠错码"><a href="#纠错码" class="headerlink" title="纠错码"></a>纠错码</h3><p>为了提高恢复的准确性，我们实现了特定的(3,1)重复码来对密电进行编码和纠错。这种ECC原理包括在传输前将每个数据位增加三倍，使提取过程中出现的错误可以通过复制的数据来纠正。</p>
<h3 id="消息映射和反映射"><a href="#消息映射和反映射" class="headerlink" title="消息映射和反映射"></a>消息映射和反映射</h3><p>感觉公式写错了<br>看了原论文，就是按照嵌入的比特数$\sigma$，将区间 $[-1,1]$ 划分为 $2^{\sigma -1}$ 个小区间，然后小区间有间隔以免产生误差<br><img src="/2024/07/08/%E9%9F%B3%E4%B9%90%E7%94%9F%E6%88%90%E9%9A%90%E5%86%99/image-20240804200024451.png" alt=""><br>就是均匀分布到高斯分布的那个东西</p>
<h3 id="MIDI-GAN"><a href="#MIDI-GAN" class="headerlink" title="MIDI-GAN"></a>MIDI-GAN</h3><p><img src="/2024/07/08/%E9%9F%B3%E4%B9%90%E7%94%9F%E6%88%90%E9%9A%90%E5%86%99/image-20240804200550007.png" alt=""><br>生成器是一个CNN网络<br><img src="/2024/07/08/%E9%9F%B3%E4%B9%90%E7%94%9F%E6%88%90%E9%9A%90%E5%86%99/image-20240804210744927.png" alt=""><br>把嵌入映射$F_1$，映射到一个新的$F_2$<br>将$F_2$归一化到整数区间<br><img src="/2024/07/08/%E9%9F%B3%E4%B9%90%E7%94%9F%E6%88%90%E9%9A%90%E5%86%99/image-20240804211021678.png" alt=""><br>提取的时候，用类似的方法将整数转换为浮点数</p>
<p>损失函数：<br>提取器的MSE损失<br>判别器为损失为输出的期望的差值加上线性组合的输出的梯度<br><img src="/2024/07/08/%E9%9F%B3%E4%B9%90%E7%94%9F%E6%88%90%E9%9A%90%E5%86%99/image-20240804213050475.png" alt=""><br>梯度保持策略？</p>
<h2 id="实验-1"><a href="#实验-1" class="headerlink" title="实验"></a>实验</h2><h3 id="指标"><a href="#指标" class="headerlink" title="指标"></a>指标</h3><p>MOS：平均主观得分<br>BER：比特错误率，衡量鲁棒性<br>PSNR：峰值信噪比<br>能量距离和Wasserstein距离，上一篇分布保持的论文里讲过，衡量分布距离的<br>召回率用于衡量抗检测<br><img src="/2024/07/08/%E9%9F%B3%E4%B9%90%E7%94%9F%E6%88%90%E9%9A%90%E5%86%99/image-20240804213750657.png" alt=""><br>主观得分，重复，声调跨度<br>重复衡量的是曲子的结构是否合理<br><img src="/2024/07/08/%E9%9F%B3%E4%B9%90%E7%94%9F%E6%88%90%E9%9A%90%E5%86%99/image-20240804214416238.png" alt=""><br>对几个损失的消融实验</p>
<p>分布保持的性能<br><img src="/2024/07/08/%E9%9F%B3%E4%B9%90%E7%94%9F%E6%88%90%E9%9A%90%E5%86%99/image-20240804214545475.png" alt=""><br><img src="/2024/07/08/%E9%9F%B3%E4%B9%90%E7%94%9F%E6%88%90%E9%9A%90%E5%86%99/image-20240804214609505.png" alt=""><br>使用三种不同的神经网络隐写分析器的平均召回率</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="http://yypyyds.github.io">脚踏车没有脚</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://yypyyds.github.io/2024/07/08/%E9%9F%B3%E4%B9%90%E7%94%9F%E6%88%90%E9%9A%90%E5%86%99/">http://yypyyds.github.io/2024/07/08/%E9%9F%B3%E4%B9%90%E7%94%9F%E6%88%90%E9%9A%90%E5%86%99/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://yypyyds.github.io" target="_blank">脚踏车的日志站</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E9%9A%90%E5%86%99/">隐写</a><a class="post-meta__tags" href="/tags/%E7%94%9F%E6%88%90/">生成</a></div><div class="post_share"><div class="social-share" data-image="/img/touxiang.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2024/07/18/%E8%AF%AD%E9%9F%B3%E4%BF%A1%E5%8F%B7%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/" title="语音信号处理笔记"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">语音信号处理笔记</div></div></a></div><div class="next-post pull-right"><a href="/2024/06/11/Talking%20face%20detection%E7%9B%B8%E5%85%B3/" title="Talking face detection相关"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">Talking face detection相关</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2023/11/30/StegaDDPM/" title="StegaDDPM"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-11-30</div><div class="title">StegaDDPM</div></div></a></div><div><a href="/2024/02/02/%E9%9F%B3%E9%A2%91%E7%94%9F%E6%88%90%E5%B7%A5%E4%BD%9C%E6%B1%87%E6%80%BB/" title="未命名音频生成工作汇总"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-02-02</div><div class="title">未命名音频生成工作汇总</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/touxiang.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">脚踏车没有脚</div><div class="author-info__description">不积跬步，无以至千里</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">67</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">61</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">9</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/yypyyds"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">这是一个努力学习的笨蛋的博客</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E9%9F%B3%E4%B9%90%E7%94%9F%E6%88%90%E9%9A%90%E5%86%99"><span class="toc-text">音乐生成隐写</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%9F%B3%E4%B9%90%E7%94%9F%E6%88%90"><span class="toc-text">音乐生成</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%94%9F%E6%88%90%E5%BC%8F%E9%9A%90%E5%86%99"><span class="toc-text">生成式隐写</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%94%9F%E6%88%90%E5%BC%8F%E6%B0%B4%E5%8D%B0"><span class="toc-text">生成式水印</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%94%9F%E6%88%90%E5%BC%8F%E9%9A%90%E5%86%99%E7%A0%94%E7%A9%B6%E7%BB%BC%E8%BF%B0"><span class="toc-text">生成式隐写研究综述</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#CRoSS-Diffusion-Model-Makes-Controllable-Robust-and-Secure-Image-Steganography"><span class="toc-text">CRoSS: Diffusion Model Makes Controllable, Robust and Secure Image Steganography</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Method"><span class="toc-text">Method</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Definition-of-Image-Steganography"><span class="toc-text">Definition of Image Steganography</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Invertible-Image-Translation-using-Diffusion-Model"><span class="toc-text">Invertible Image Translation using Diffusion Model</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#The-Coverless-Image-Steganography-Framework-CRoSS"><span class="toc-text">The Coverless Image Steganography Framework CRoSS</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%9E%E9%AA%8C"><span class="toc-text">实验</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Distribution-Preserving-Steganography-Based-on-Text-to-Speech-Generative-Models"><span class="toc-text">Distribution-Preserving Steganography Based on Text-to-Speech Generative Models</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Existing-Distribution-Preserving-Methods"><span class="toc-text">Existing Distribution-Preserving Methods</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#TTS-Models"><span class="toc-text">TTS Models</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Distribution-preserving-steganography-based-on-an-implicit-generative-model"><span class="toc-text">Distribution-preserving steganography based on an implicit generative model</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Message-Mapping"><span class="toc-text">Message Mapping</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Message-Embedding-and-Extraction"><span class="toc-text">Message Embedding and Extraction</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Distribution-preserving-steganography-based-on-an-explicit-generative-model"><span class="toc-text">Distribution-preserving steganography based on an explicit generative model</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Message-Embedding-and-Extraction-1"><span class="toc-text">Message Embedding and Extraction</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Experiments"><span class="toc-text">Experiments</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Evaluation-Metric"><span class="toc-text">Evaluation Metric</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Sampler-Based-Stegosystem"><span class="toc-text">Sampler-Based Stegosystem</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Compression-Based-Stegosystem"><span class="toc-text">Compression-Based Stegosystem</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Fast-Timing-Conditioned-Latent-Audio-Diffusion"><span class="toc-text">Fast Timing-Conditioned Latent Audio Diffusion</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Related-works"><span class="toc-text">Related works</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Architecture"><span class="toc-text">Architecture</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#VAE"><span class="toc-text">VAE</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Conditioning"><span class="toc-text">Conditioning</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Diffuison-model"><span class="toc-text">Diffuison model</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Inference"><span class="toc-text">Inference</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Training"><span class="toc-text">Training</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Dataset"><span class="toc-text">Dataset</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#VAE-1"><span class="toc-text">VAE</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Prompt-presentation"><span class="toc-text">Prompt presentation</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Methodology"><span class="toc-text">Methodology</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Quantitative-metrics"><span class="toc-text">Quantitative metrics</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Qualitative-metrics"><span class="toc-text">Qualitative metrics</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Results"><span class="toc-text">Results</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%80%9D%E8%80%83"><span class="toc-text">思考</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Message-Driven-Generative-Music-Steganography-Using-MIDI-GAN"><span class="toc-text">Message-Driven Generative Music Steganography Using MIDI-GAN</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#proposed-scheme"><span class="toc-text">proposed scheme</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#MIDI-File-Format-and-Chord-Dataset"><span class="toc-text">MIDI File Format and Chord Dataset</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BA%A0%E9%94%99%E7%A0%81"><span class="toc-text">纠错码</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%B6%88%E6%81%AF%E6%98%A0%E5%B0%84%E5%92%8C%E5%8F%8D%E6%98%A0%E5%B0%84"><span class="toc-text">消息映射和反映射</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#MIDI-GAN"><span class="toc-text">MIDI-GAN</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%9E%E9%AA%8C-1"><span class="toc-text">实验</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%8C%87%E6%A0%87"><span class="toc-text">指标</span></a></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/07/19/%E6%89%BE%E7%8F%AD%E4%B8%8A/" title="无题">无题</a><time datetime="2024-07-19T07:37:01.740Z" title="发表于 2024-07-19 15:37:01">2024-07-19</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/07/18/%E8%AF%AD%E9%9F%B3%E4%BF%A1%E5%8F%B7%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/" title="语音信号处理笔记">语音信号处理笔记</a><time datetime="2024-07-18T11:14:40.000Z" title="发表于 2024-07-18 19:14:40">2024-07-18</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/07/08/%E9%9F%B3%E4%B9%90%E7%94%9F%E6%88%90%E9%9A%90%E5%86%99/" title="音乐生成隐写">音乐生成隐写</a><time datetime="2024-07-08T08:20:58.000Z" title="发表于 2024-07-08 16:20:58">2024-07-08</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/06/11/Talking%20face%20detection%E7%9B%B8%E5%85%B3/" title="Talking face detection相关">Talking face detection相关</a><time datetime="2024-06-11T03:08:05.000Z" title="发表于 2024-06-11 11:08:05">2024-06-11</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/05/22/%E9%9F%B3%E9%A2%91%E6%95%B0%E6%8D%AE%E9%9B%86/" title="音频数据集">音频数据集</a><time datetime="2024-05-22T03:51:40.000Z" title="发表于 2024-05-22 11:51:40">2024-05-22</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2024 By 脚踏车没有脚</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div></div></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.min.js"></script><script src="/js/search/local-search.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        insertScript: [200, () => {
          document.querySelectorAll('mjx-container').forEach(node => {
            if (node.hasAttribute('display')) {
              btf.wrap(node, 'div', { class: 'mathjax-overflow' })
            } else {
              btf.wrap(node, 'span', { class: 'mathjax-overflow' })
            }
          });
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typeset()
}</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>