<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0"><title>生成相关 | 脚踏车的日志站</title><meta name="author" content="脚踏车没有脚"><meta name="copyright" content="脚踏车没有脚"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="生成相关AudiogenICLR 2023Meta AI包括两个主要的阶段第一阶段将原始音频编码成离散的token序列，通过一个压缩模型进行该模型以端到端的方式进行训练，使用压缩表示重建输入音频，并以一组鉴别器的形式添加感知损失。第二阶段使用一个自回归的Transformer-decoder language-model，在文本条件的基础上重建音频序列 主要贡献：  sota方法 提高TTA生成性">
<meta property="og:type" content="article">
<meta property="og:title" content="生成相关">
<meta property="og:url" content="http://yypyyds.github.io/2024/04/08/%E7%94%9F%E6%88%90%E7%9B%B8%E5%85%B3/index.html">
<meta property="og:site_name" content="脚踏车的日志站">
<meta property="og:description" content="生成相关AudiogenICLR 2023Meta AI包括两个主要的阶段第一阶段将原始音频编码成离散的token序列，通过一个压缩模型进行该模型以端到端的方式进行训练，使用压缩表示重建输入音频，并以一组鉴别器的形式添加感知损失。第二阶段使用一个自回归的Transformer-decoder language-model，在文本条件的基础上重建音频序列 主要贡献：  sota方法 提高TTA生成性">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://yypyyds.github.io/img/touxiang.jpg">
<meta property="article:published_time" content="2024-04-08T09:00:48.000Z">
<meta property="article:modified_time" content="2024-07-26T12:00:19.992Z">
<meta property="article:author" content="脚踏车没有脚">
<meta property="article:tag" content="论文">
<meta property="article:tag" content="深度学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://yypyyds.github.io/img/touxiang.jpg"><link rel="shortcut icon" href="/img/favicon.ico"><link rel="canonical" href="http://yypyyds.github.io/2024/04/08/%E7%94%9F%E6%88%90%E7%9B%B8%E5%85%B3/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":true,"languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  }
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '生成相关',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-07-26 20:00:19'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
    win.getCSS = url => new Promise((resolve, reject) => {
      const link = document.createElement('link')
      link.rel = 'stylesheet'
      link.href = url
      link.onload = () => resolve()
      link.onerror = () => reject()
      document.head.appendChild(link)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 6.3.0">
<style>.github-emoji { position: relative; display: inline-block; width: 1.2em; min-height: 1.2em; overflow: hidden; vertical-align: top; color: transparent; }  .github-emoji > span { position: relative; z-index: 10; }  .github-emoji img, .github-emoji .fancybox { margin: 0 !important; padding: 0 !important; border: none !important; outline: none !important; text-decoration: none !important; user-select: none !important; cursor: auto !important; }  .github-emoji img { height: 1.2em !important; width: 1.2em !important; position: absolute !important; left: 50% !important; top: 50% !important; transform: translate(-50%, -50%) !important; user-select: none !important; cursor: auto !important; } .github-emoji-fallback { color: inherit; } .github-emoji-fallback img { opacity: 0 !important; }</style>
</head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/touxiang.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">67</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">61</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">9</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header"><nav id="nav"><span id="blog-info"><a href="/" title="脚踏车的日志站"><span class="site-name">脚踏车的日志站</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">生成相关</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-04-08T09:00:48.000Z" title="发表于 2024-04-08 17:00:48">2024-04-08</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-07-26T12:00:19.992Z" title="更新于 2024-07-26 20:00:19">2024-07-26</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="生成相关"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="生成相关"><a href="#生成相关" class="headerlink" title="生成相关"></a>生成相关</h1><h2 id="Audiogen"><a href="#Audiogen" class="headerlink" title="Audiogen"></a>Audiogen</h2><p>ICLR 2023<br>Meta AI<br>包括两个主要的阶段<br>第一阶段将原始音频编码成离散的token序列，通过一个压缩模型进行<br>该模型以端到端的方式进行训练，使用压缩表示重建输入音频，并以一组鉴别器的形式添加感知损失。<br>第二阶段使用一个自回归的Transformer-decoder language-model，在文本条件的基础上重建音频序列</p>
<p>主要贡献：</p>
<ol>
<li>sota方法</li>
<li>提高TTA生成性能的两个方法：classifier free guidance，动态文本和音频混合来提高组合性</li>
<li>可以做条件和非条件的音频延续</li>
<li>探索了音频保真度和采样时间之间的关系<h3 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h3><img src="/2024/04/08/%E7%94%9F%E6%88%90%E7%9B%B8%E5%85%B3/image-20240411103419341.png" alt=""><h4 id="Audio-representation"><a href="#Audio-representation" class="headerlink" title="Audio representation"></a>Audio representation</h4>一个时长为d的音频信号可以表示为一个序列 $x\in[-1,1]^{C_a \times T}$ ，$C_a$ 是通道数，$T=d\cdot f_{sr}$ 是采样点数，至于为什么是 $[-1,1]$ ，是因为py库读取wav文件会自动归一化<br>Audio representation model 包含三个部分</li>
</ol>
<ul>
<li>encoder network E：将音频片段作为输入，输出一个latent representation $z$ </li>
<li>quantization layer Q：使用Vector Quantizaiton将 $z$ 压缩为 $z_q$</li>
<li>decoder net work G：从压缩表示 $z_q$ 中重建时域信号 $\hat x$<br>整个系统端到端进行训练，损失包括时域和频域上的重建损失，以及不同时间分辨率的感知损失，这一部分通过多个鉴别器来实现。</li>
</ul>
<p>训练目标：<br>时域重建损失：$\ell_t(x,\hat x)=\Vert x-\hat x\Vert_1$<br>频域重建损失：</p>
<script type="math/tex; mode=display">
\ell_f(x,\hat x)=\frac{1}{|\alpha|\cdot|s|}\sum_{\alpha_i\in \alpha}\sum_{i\in e}\Vert S_i(x)-S_i(\hat x) \Vert_1 +\alpha_i\Vert S_i(x)-S_i(\hat x) \Vert_2</script><p>其中 $S_i$ 是根据 $i$ 改变的不同窗长和步长的64-bins梅尔谱图，系数 $\alpha_i$ 用于平衡两项损失，但是这里设置的是1<br>感知损失：<br>使用一个multi-scale STFT-based discriminator来捕获声音信号中的不同结构。<br>判别器的结构是一样的，但是时间分辨率不同，对抗损失如下</p>
<script type="math/tex; mode=display">
\ell_g(\hat x)=\frac{1}{K}\sum_k \max(0,1-D_k(\hat x))</script><p>根据公式，这个 $D_k(\hat x)$ 输出的应该是为真实样本的概率<br>还有特征匹配损失（这个在VITS中也出现了），L是判别器的层数</p>
<script type="math/tex; mode=display">
\ell_{feat}(x,\hat x)=\frac{1}{KL}\sum^K_{k=1}\sum^L_{l=1}\Vert D^l_k(x)-D^l_k(\hat x) \Vert_1</script><p>判别器的总损失还加了一个部分，总的如下</p>
<script type="math/tex; mode=display">
L_d(x, \hat x)=\frac{1}{K}\sum^K_{k=1}\max(0,1-D_k(x))+\max(0,1+D_k(\hat x))</script><p>对抗损失的两部分，对真样本 $x$ 输出要大，假的输出要小，因为是损失，所以二者都取负，得到的就是上面这种形式。<br><strong>Audio language modeling</strong><br>看不懂，什么卵<br>给定一个文本输入 $C$ ，Audio Language Model（ALM）组件输出一个音频token序列 $\hat z_q$ 然后在其上面做音频重建<br>给定如下：<br>一个文本编码器F，将原始文本输出投射成一个语义稠密表示（semantic dense representation）$F(c)=u$<br>一个查阅表（Look-Up-Table, LUT）将音频token $\hat z_q$ 嵌入一个连续的空间，$LUT(\hat z_q)=v$<br>然后将 $u,v$ 连接，得到 $Z=u_1,\dots,u_{T_u},v_1,\dots,v_{T_v}$<br>然后使用上面这个表示 $Z$ ，训练一个Transformer-decoder language-model，使用如下损失函数</p>
<script type="math/tex; mode=display">
L_{LM}=-\sum^N_{i=1}\sum^{T_v}_{j=1}\log p_{\theta}(v^i_j|u_1^1,\dots,u^i_{T_u},v_1^1,\dots,v^i_{j-1})</script><p>两个连乘，N是样本数吗？然后下标是音频嵌入连续空间序列<br>突然懂了<br>首先这是一个Transformer序列预测器，目标是给定文本，得到音频token序列，上面损失函数是一个条件熵的形式，对于一条样本，最开始的条件是文本表示 $u$ ，然后依次预测下一个音频token，每次预测计算一次条件熵。<br><strong>长序列预测问题</strong><br>本方法对原始音频下采样32倍，让每个audiotoken关联2ms，这会导致很长的序列，因为每秒就需要500个token。<br>使用Multi-stream audio inputs的方法缓解这个问题<br>看不懂，看代码的时候再研究研究</p>
<h4 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h4><p>一些评估指标<br>Frechet Audio Distance：FAD，是一种与人类感知密切相关的无参考评估指标<br>还评估了KL散度和主观的标准<br>做了音频续写的实验，使用的条件包括音频片段，音频相关文本，音频无关文本</p>
<h4 id="Limitations"><a href="#Limitations" class="headerlink" title="Limitations"></a>Limitations</h4><p>缺乏理解时间顺序的能力，经常生成莫名其妙的语音</p>
<h2 id="Diffsound"><a href="#Diffsound" class="headerlink" title="Diffsound"></a>Diffsound</h2><p>TASLP2023<br>北大, Tencent AI lab<br>Dongchao Yang, Jianwei Yu<br>传统的自回归token解码器，有两个问题</p>
<ul>
<li>梅尔谱token总是按顺序预测，这会限制模型的生成能力，因为有些声音的事件位置可能来自文本的两端</li>
<li>预测阶段，来自之前的错误的预测token会导致累计的预测错误<br>主要贡献：</li>
<li>第一次在音频生成任务中使用Diffusion</li>
<li>提出了一种基于掩码的文本生成策略（mask-based text generation strategy, MBTG），有助于在AudioSet数据集上构建大规模文本-音频数据集</li>
<li>提出了三个客观的评价指标 <h3 id="Proposed-text-to-sound-framework"><a href="#Proposed-text-to-sound-framework" class="headerlink" title="Proposed text-to-sound framework"></a>Proposed text-to-sound framework</h3><img src="/2024/04/08/%E7%94%9F%E6%88%90%E7%9B%B8%E5%85%B3/image-20240412150427637.png" alt=""><br><strong>Text Encoder</strong><br>有点搞，用的是BERT和CLIP的text encoder，但是效果更好，说明和图片的对比学习会让textencoder携带更多的语义信息<br><strong>Learning Discrete Latent Space of Mel-Spectrograms via VQ-VAE</strong><br><img src="/2024/04/08/%E7%94%9F%E6%88%90%E7%9B%B8%E5%85%B3/image-20240412152710502.png" alt=""><br>和TTS任务不同，TTA任务生成的音频与文本没有直接的对应关系<br>使用VQ-VAE来生成梅尔谱，问题就被转换成生成token序列<br>这里就是普通的VQ-VAE，不再详细记录<br><strong>Token-Decoder</strong><br>用于将文本特征变换到离散的梅尔谱图token序列，本文首先提出采用自回归的token-decoder<br>啥意思，前面说不用自回归，这里又用的自回归<br>说后面给了一个不同自回归的方法，等下看看<br>这里是用自回归预测前面VQ-VAE的encoder生成的梅尔token序列<br>在训练的时候，因为累计错误问题，使用<code>"teacher-forcing" strategy</code>，使用ground truth作为预测第 $i$ 个token时前 $i-1$ 个token条件<br><strong>Vocoder</strong><br>用的MelGAN，不懂<br>这还是自己训练的，在AS上训练的<h3 id="Diffusion-based-decoder"><a href="#Diffusion-based-decoder" class="headerlink" title="Diffusion-based decoder"></a>Diffusion-based decoder</h3>就是经典Diffusion，参考<a href="../_posts/AudioLDM.md">AudioLDM</a>中关于Diffusion的部分<br>这里给了一个好高大上的loss，叫负对数似然变分上界（variational upper bound on the negative log-likelihood）<script type="math/tex; mode=display">
\mathcal{L}_{vb}=\mathbb{E}_{q(\boldsymbol{x}_0)}\left[
D_{KL}[q(\boldsymbol{x}_t|\boldsymbol{x}_0)\Vert p(\boldsymbol{x}_t)] + \sum^T_{t=1}\mathbb{E}_{q(\boldsymbol{x}_t|\boldsymbol{x}_0)}
\left[D_{KL}[q(\boldsymbol{x}_{t-1}|\boldsymbol{x}_t,\boldsymbol{x}_0)\Vert
p_{\theta}(\boldsymbol{x}_{t-1}|\boldsymbol{x}_t)]
\right]
\right]</script>这一大坨狗屎是什么<br>首先，$q$ 是前向过程的分布，是加噪过程，$p$ 是不变的高斯分布，$p_\theta$ 是反向过程去噪的分布<br>第一个KL散度是加噪过程的损失，第二个是T步去噪过程的损失，这下懂了</li>
</ul>
<p><strong>Discrete Diffuison Model</strong><br>因为送入Decoder的是离散的整数，不能在这上面加噪声，引入一个转移概率矩阵，用于知道前向过程中 $\boldsymbol{x}_0$ 如何一步一步转换为 $\boldsymbol{x}_t$<br>定义为：</p>
<script type="math/tex; mode=display">
[\boldsymbol{Q}_t]_{ij}=q(x_t=i|x_{t-1}=j) \in \mathbb{R}^{P\times P}</script><p>对整个序列的前向过程可以写为：</p>
<script type="math/tex; mode=display">
q(X_t|X_{t-1})=\boldsymbol{c}^T(x_t)\boldsymbol{Q}_t \boldsymbol{c}(x_{t-1})</script><p>其中 $\boldsymbol{c}(\cdot)$ 是一个将标量元素转换成one-hot向量的函数<br>沟槽的后面好多，这里暂时解释不清，继续往后看<br>然后说按照马尔可夫链和贝叶斯公式，可以计算出 $q(x_t|x_0)$ 和 $q(x_{t-1}|x_t,x_0)$<br><strong>Non-Autoregressive Mel-Spectrograms Generation via Diffsound</strong><br>作者希望预测的结果可以同时获得，并利用扩散模型T步的迭代优化预测结果<br>但是这里的特征是离散的，这里提到，离散扩散模型训练的关键是设计一种合适的策略来预定义马尔可夫转移矩阵 $Q_t$<br>注意到上面用的是离散标签在这搞扩散，我直接一个问号<br>这是将离散标签one-hot之后类似一个分类过程？<br>发现one-hot之后就是一个单位向量啊，然后左乘右乘就是选择 $Q_t$ 中的一个元素嘛<br>定义了三种转移矩阵</p>
<ul>
<li>Uniform transition matrix</li>
<li>mask transition matrix</li>
<li>mask and uniform transition matrix<br>这个就是传统diffusion的扩展，不想看了<br><strong>Pre-Training Diffsound on AudioSet Dataset</strong><br>对于文本描述，采用的做法是在标签两边随机插入一到两个mask标记，以使模型主要关注于声音事件而不是文本（感觉不太靠谱<br>采用渐进式训练，从单标签开始训练，然后再多标签<h3 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h3>相比之前的文章，引入了一个Audio Caption Loss，是将生成的音频送入Audio caption模型，执行caption任务</li>
</ul>
<h2 id="Make-an-Audio"><a href="#Make-an-Audio" class="headerlink" title="Make an Audio"></a>Make an Audio</h2><p>ICML2023<br>浙大，北大，字节跳动<br>Rongjie Huang, Jiawei Huang, Dongchao Yang<br>引入了一种数据增强方法，解决音频文本对数据稀缺的问题<br>使用频谱自编码器预测自监督音频表示而不是波形<br>支持多模态输入生成音频<br><img src="/2024/04/08/%E7%94%9F%E6%88%90%E7%9B%B8%E5%85%B3/image-20240419092228304.png" alt=""></p>
<h3 id="Pseudo-Prompt-Enhancement-Distill-then-Reprogram"><a href="#Pseudo-Prompt-Enhancement-Distill-then-Reprogram" class="headerlink" title="Pseudo Prompt Enhancement: Distill-then-Reprogram"></a>Pseudo Prompt Enhancement: Distill-then-Reprogram</h3><p>这个方法包含两个阶段，一个expert distillation approach，使用audio caption方法，从无标签音频中得到文本描述，一个dynamic reprogramming procedure来构建不同的音频文本组合<br><img src="/2024/04/08/%E7%94%9F%E6%88%90%E7%9B%B8%E5%85%B3/image-20240419092220640.png" alt=""></p>
<p><strong>Expert Distillation</strong><br>使用两个预训练模型来做这个事情，分别是audio caption的工作和audio-text retrieval的工作，我记得两个都是DCASE的task<br>没看懂，这个audio-text retrieval的模型是拿来干嘛的？<br><strong>Dynamic reprogramming</strong><br>根据几个模板，把 $N\in \{0,1,2\}$ 个音频文本对使用预设的几个方法连接</p>
<h3 id="Textual-Representation"><a href="#Textual-Representation" class="headerlink" title="Textual Representation"></a>Textual Representation</h3><p>就说用了CLAP和T5-Large(一个大语言模型)，实验表明二者的结果相近，但是CLAP更高效，因为不需要offline computation</p>
<h3 id="Audio-Representation"><a href="#Audio-Representation" class="headerlink" title="Audio Representation"></a>Audio Representation</h3><p>经典的梅尔自编码器带判别器提高重建质量<br>这个除了重建损失和对抗损失还有一个KL散度惩罚损失</p>
<p>后面还有经典的介绍Diffusion和CLassifier-Free Guidance</p>
<h3 id="X-To-Audio-No-Modality-Left-Behind"><a href="#X-To-Audio-No-Modality-Left-Behind" class="headerlink" title="X-To-Audio: No Modality Left Behind"></a>X-To-Audio: No Modality Left Behind</h3><p>包括三个部分，text to audio, audio inpainting visual to audio(video or image)<br><img src="/2024/04/08/%E7%94%9F%E6%88%90%E7%9B%B8%E5%85%B3/image-20240419092200720.png" alt=""></p>
<h4 id="Personalized-Text-To-Audio-Generation"><a href="#Personalized-Text-To-Audio-Generation" class="headerlink" title="Personalized Text-To-Audio Generation"></a>Personalized Text-To-Audio Generation</h4><p>没搞懂他想干嘛<br>这里给了个例子，在给定打雷的声音时，让模型生成babycrying的音频，就会生成<code>a baby crying in the thunder day</code><br>大概就是通过文本描述修改音频，添加背景声，或者插入说话的主体之类的<br>做法是对给定的音频片段，选取一个特定的时间段$t_0$然后对其进行Diffuision的加噪去噪过程<br>这里提到音频的文本对其度和真实度之间随着T的变大有一个trade-off</p>
<h4 id="Audio-Inpainting"><a href="#Audio-Inpainting" class="headerlink" title="Audio Inpainting"></a>Audio Inpainting</h4><p>通过mask来实现Inpainting的训练，采用了两种mask方法<br>一个是<a href="Suvorov, R., Logacheva, E., Mashikhin, A., Remizova, A., Ashukha, A., Silvestrov, A., Kong, N., Goka, H., Park, K., and Lempitsky, V. Resolution-robust large mask inpainting with fourier convolutions. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pp. 2149–2159, 2022">LaMa</a>中的什么polygonal chains和任意宽高比的矩形mask（听起来像是在mel谱上做mask，因为原来的工作是图像上的<br><img src="/2024/04/08/%E7%94%9F%E6%88%90%E7%9B%B8%E5%85%B3/image-20240419092131181.png" alt=""><br>大概长这样<br>另一个是在语音相关文献中常用的帧级掩蔽策略</p>
<h4 id="Visual-To-Audio-Generation"><a href="#Visual-To-Audio-Generation" class="headerlink" title="Visual-To-Audio Generation"></a>Visual-To-Audio Generation</h4><p>用CLIP来抽取图像特征，用文本表示来弥合视觉和音频世界之间的模态差距，用一个或多个Flow模型将CLIP的特征向量空间映射到CLAP的特征向量空间<br>对于视频，抽取固定的4帧，池化得到4帧的平均表示，退化为图片转音频</p>
<h3 id="Training-and-Evaluation"><a href="#Training-and-Evaluation" class="headerlink" title="Training and Evaluation"></a>Training and Evaluation</h3><p>笑死用了18块V100<br>在常用的评估方法FID和KL散度之上，添加了CLAP分数来评估audio-text alignment<br>对比了Diffsound，AudioLDM和AudioGen<br><img src="/2024/04/08/%E7%94%9F%E6%88%90%E7%9B%B8%E5%85%B3/image-20240419170509432.png" alt=""></p>
<p>IS: inception score，在AudioLDM中，对他的解释是</p>
<blockquote>
<p>IS is effective in evaluating both sample quality and diversity.</p>
</blockquote>
<p>网上找的都说什么是用谷歌一个分类模型计算的，对于音频，需要专门的分类器 $C$<br>可以自己训练，计算公式如下</p>
<script type="math/tex; mode=display">
IS(G)=\exp(\mathbb{E}_{z\sim P_g} D_{KL}(p(y|x)\Vert p(y)))</script><p>在AudioLDM中有讲，是用的PANNs分类器做的评估</p>
<p>消融实验中提到，对于文本特征提取，CLAP和T5表现差不多，但是CLAP性能高</p>
<h3 id="与AudioLDM2的对比"><a href="#与AudioLDM2的对比" class="headerlink" title="与AudioLDM2的对比"></a>与AudioLDM2的对比</h3><p>二者同为多模态内容生成音频的工作，从结构、特征工程、结果三个角度来看<br><strong>结构</strong><br>AudioLDM2</p>
<ul>
<li>使用AudioMAE编码音频特征</li>
<li>使用GPT-2生成其他模态特征</li>
<li>使用含Transformer块的U-net<br>Make-an-Audio</li>
<li>使用自己训练的梅尔自编码器结构网络编码音频特征</li>
<li>使用CLIP和CLAP编码其他模态特征</li>
<li>使用的普通的卷积U-net</li>
</ul>
<p><strong>特征工程</strong><br>AudioLDM2</p>
<ul>
<li>使用各模态的专家模型抽取模态特征 </li>
<li>使用GPT2生成音频大一统特征<br>Make-an-Audio</li>
<li>也是用的专家模型抽取模态特征</li>
<li>使用Flow映射分布到自编码器特征空间</li>
</ul>
<p><strong>结果</strong><br><img src="file:///C:/Users/89492/Desktop/%E5%91%A8%E6%8A%A5/2024-4-22%E9%84%A2%E6%B9%A7%E6%A3%9A%E5%91%A8%E6%8A%A5/image-20240422100319446.png?lastModify=1713926396" alt="image-20240422100319446"><br>AudioLDM使用的训练数据少很多，主观评价相对也更好<br>OVL：Overall quality<br>REL：relevance t the input text</p>
<h3 id="复现"><a href="#复现" class="headerlink" title="复现"></a>复现</h3><p>在Audiocaps数据集上进行了训练<br>总的训练时间大约为两天<br>VAE训练了35 epochs<br>Diffusion 训练了40 epochs<br><strong>VAE</strong><br>原始输入<br><img src="/2024/04/08/%E7%94%9F%E6%88%90%E7%9B%B8%E5%85%B3/image-20240426163758549.png" alt=""><br>VAE编解码重建，质量很高<br><img src="/2024/04/08/%E7%94%9F%E6%88%90%E7%9B%B8%E5%85%B3/image-20240426163837870.png" alt=""></p>
<p><strong>diffusion</strong><br>inputs: batch中的音频<br>reconstruction<br>diffusion_row: 随机噪声，经过Diffusion每200 step记录一次<br><img src="/2024/04/08/%E7%94%9F%E6%88%90%E7%9B%B8%E5%85%B3/image-20240426182907899.png" alt=""></p>
<p>sample<br><img src="/2024/04/08/%E7%94%9F%E6%88%90%E7%9B%B8%E5%85%B3/image-20240426182848923.png" alt=""><br><strong>指标</strong></p>
<div class="table-container">
<table>
<thead>
<tr>
<th>模型</th>
<th>FD</th>
<th>FAD</th>
<th>IS</th>
</tr>
</thead>
<tbody>
<tr>
<td>作者给的</td>
<td>26.6</td>
<td>2.39</td>
<td>7.83</td>
</tr>
<tr>
<td>我训练的</td>
<td>38.9</td>
<td>6.43</td>
<td>5.32</td>
</tr>
</tbody>
</table>
</div>
<p>FD: 曲线相似性度量<br>FAD：人类感知指标<br>IS：评估采样质量和多样性</p>
<h2 id="Retrieval-Augmented-text-to-audio-generation"><a href="#Retrieval-Augmented-text-to-audio-generation" class="headerlink" title="Retrieval-Augmented text to audio generation"></a>Retrieval-Augmented text to audio generation</h2><p>ICASSP 2024<br>University of Surrey<br>Yi Yuan, Haohe Liu<br>提出的问题：由于TTA模型训练数据的类别不平衡问题，其生成性能存在偏差，这些模型可以为常见的声音事件生成真实的音频片段，但是当遇到不太频繁或看不见的声音事件时，他们可能会生成不正确或不相关的音频。作者把这个问题定义为 <code>long-tailed text-to-audio generation problem</code><br>本文提出了一个训练框架，用在作者之前AudioLDM的工作上，使用了Audio retrieval来增强训练时的样本丰富度。称之为 <code>Re-AudioLDM</code></p>
<blockquote>
<p>具体来说，我们首先使用输入文本提示从数据集(例如AudioCaps)中检索相关引用（比如音频文本对），然后使用预训练的音频模型和语言模型分别提取声学和文本特征。然后将这些提取的特征进一步交给LDM的cross attention模块，以指导生成过程。检索到的音频文本对作为补充信息，有助于改进训练阶段低频发生的音频事件的建模。在推理阶段，检索增强策略还提供了与文本提示相关的参考，确保了更加准确和忠实的音频生成结果。</p>
</blockquote>
<p>注意这几点：检索的做法，cross attention是在哪里，怎么指导的，和原来的训练方法有什么不同，推理时是怎么提供参考的<br>检索的做法：在Audiocaps中选k邻居<br>cross attention: 在Unet的注意力层<br>下面几个没讲，沟槽的</p>
<h3 id="Method-1"><a href="#Method-1" class="headerlink" title="Method"></a>Method</h3><p><strong>Text and Retrieval Embedding Encoder</strong><br>作为一个级联结构模型，使用两个平行的输入<br>一个文本输入 $c_t$ 用于提供低级语义信息<br>一系列音频文本 $c_r$ 用于提供高级语义音频信息<br>音频文本对：$c_r = [<text_1,audio_1>,<text_2,audio_2>,\dots,<text_k,audio_k>]$ 是通过目标的caption 的embedding与检索数据库进行相似度比较选择的top-k邻居，最后将这些音频文本对特征连接：</text_k,audio_k></text_2,audio_2></text_1,audio_1></p>
<script type="math/tex; mode=display">
\begin{aligned}
E^{ra} &= CAT[f_{mae}(audio_1),\dots,f_{mae}(audio_k)]
\newline
E^{rt} &= CAT[f_{t5}(text_1),\dots,f_{t5}(text_k)]
\end{aligned}</script><p><strong>Retrieval-Augmented Diffusion Generator</strong><br>上面两个特征表示是在Unet的每个cross attention中引入的</p>
<blockquote>
<p>将检索到的文本和音频信息与其余层中的所有交叉注意力块共享，采用 re-weighted training objective，训练目标如下：</p>
</blockquote>
<p><img src="/2024/04/08/%E7%94%9F%E6%88%90%E7%9B%B8%E5%85%B3/image-20240425155735124.png" alt=""><br>看公式根本看不出啥啊，一会儿看看什么是re-weighted training objective</p>
<h3 id="Experiments-1"><a href="#Experiments-1" class="headerlink" title="Experiments"></a>Experiments</h3><p><img src="/2024/04/08/%E7%94%9F%E6%88%90%E7%9B%B8%E5%85%B3/image-20240425162911590.png" alt=""><br>相较之前的工作，各个指标都提升了很多<br>在Retrieval info中，选择音频文本对作为检索目标，会比纯音频好<br>对于top-k的k的选择，综合效果和开销，文章说选取3-5最好<br><img src="/2024/04/08/%E7%94%9F%E6%88%90%E7%9B%B8%E5%85%B3/image-20240425163127181.png" alt=""><br>对于少样本事件类别的CLAP得分<br><img src="/2024/04/08/%E7%94%9F%E6%88%90%E7%9B%B8%E5%85%B3/image-20240425163204049.png" alt=""><br>zero-shot性能高了很多<br>未来这个组会探索大数据集下的效果和下游任务中的效果</p>
<h2 id="VoiceBox"><a href="#VoiceBox" class="headerlink" title="VoiceBox"></a>VoiceBox</h2><p>这个好像是TTS，暂时先不看</p>
<h2 id="TANGO"><a href="#TANGO" class="headerlink" title="TANGO"></a>TANGO</h2><p>本文认为更换TTA中的文本编码器将提高文本理解和整体音频生成<br>替换为一个指令微调大语言模型（instruction-tuned large language model），而且无需任何微调，因为LLM的梯度下降模仿特性(gradient-descent mimicking property)<br>提到现有的样本增强方法没有考虑音频的声压级<br><img src="/2024/04/08/%E7%94%9F%E6%88%90%E7%9B%B8%E5%85%B3/image-20240428113108732.png" alt=""><br>贡献：</p>
<ol>
<li>没有使用模态对比训练</li>
<li>不需要微调文本编码器</li>
<li>使用<code>pressure levels</code>的样本增强<h3 id="Method-2"><a href="#Method-2" class="headerlink" title="Method"></a>Method</h3><strong>Textual-Prompt Encoder</strong><br>和AudioLDM的区别就是，把CLAP换成了LLM（FLAN-T5）<br>这个做法的根据来自：<blockquote>
<p>Due to the pre-training of FLAN-T5 models on a large-scale chain-of-thought- (CoT) and instruction-based dataset, Dai et al [4] posit that they are able to learn a new task very well from the in-context information by mimicking gradient descent through attention weights.<br>由于FLAN-T5模型在大规模思维链(CoT)和基于指令的数据集上进行了预训练，Dai等人[4]假设他们能够通过注意力权重模拟梯度下降，从上下文信息中很好地学习新任务。</p>
</blockquote>
</li>
</ol>
<p><strong>Latent Diffusion Model for Text-Guided Generation</strong><br>不懂<br>作者说这个能力在其他工作用的文本编码器上是没有的<br>还说和音频联合微调文本编码器会降低其上下文学习的能力，所以冻结了文本编码器</p>
<p>训练过程也不一样<br>在AudioLDM中，扩散模型的训练过程中，Guidance使用的是音频特征，然后在推理时使用文本的CLAP特征。<br>这里训练和推理都使用文本特征（？）<br>感觉还挺合理，毕竟AudioLDM中使用CLAP时提到，可以不需要音频文本对数据。</p>
<p><strong>Augmentation</strong><br>这一小节讲的就是按照平衡声压级的方式做样本增强<br>首先按照声压级计算混合权重系数</p>
<script type="math/tex; mode=display">
p=\left(1+10^{\frac{G_1-G_2}{20}}\right)^{-1}</script><p>其中$G_1,G_2$是两个样本的声压级<br>混合样本如下：</p>
<script type="math/tex; mode=display">
mix(x_1,x_2)=\frac{px_1+(1-p)x_2}{\sqrt{p^2+(1-p)^2}}</script><p>纯纯的物理学，不看了</p>
<h3 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h3><p>数据集也是用的Audiocaps<br><img src="/2024/04/08/%E7%94%9F%E6%88%90%E7%9B%B8%E5%85%B3/image-20240428170716835.png" alt=""></p>
<p>这有个新看到的实验，是把Audiocaps的caption分成两类，一种是包含多个事件的，一种是单个事件，分开比较</p>
<h2 id="中途小记"><a href="#中途小记" class="headerlink" title="中途小记"></a>中途小记</h2><p>看了这几篇，感觉收获不大，有点浮于表面，中午和武洋聊天，好多都不知道，或者是他和我讲了，然后我左耳进右耳出了。所以现在把脑子里有的都记下来</p>
<p>首先，他说听别人说，目前Audio generation的大方法差不多这样了，应该往精细化的方向去走，比如控制每个事件在音频中的持续时间，对于音频中的前景声和背景声的合成控制，对于一些精细语义的控制，比如狗叫了多少下，文本中事件的先后顺序。<br>我之前一拍脑门想到的那个控制时长的方法，不是科学研究的正确做法，想是要想，更多的应该是去找有没有类似的做法。这个方向已经有很多人在做了，只不过还没迁移到音频上来。比如图像上的control net，应该意识到这一点，然后去图像上找相应的方法。<br>然后是那个生成引导域适应的工作，目前他想到两个做法，一个是给数据集，一个是给方法，都是用来提高分类模型在域迁移问题下的分类性能。</p>
<p>对于我现在看的这些，刚才看了下周报，发现也没有看几周这个内容，目前的几个问题是，对生成的各个环节，没有了解的很透彻，比如评价生成模型的几个指标，叫什么，怎么计算的，表明什么特征。生成的各个环节，具体的工作原理，有没有深入代码，让我修改一下要从哪里入手，有没有拿生成模型干过事情。现在还仅仅停留在运行代码的层面，还远远不够。</p>
<p>现阶段要做的事情：</p>
<ul>
<li>详细看看音频生成的几个指标是怎么计算的，表示什么</li>
<li>调研一下控制音频生成的工作</li>
<li>调研在图像上控制精细生成的工作</li>
<li>尝试将图像上的工作搬到音频</li>
</ul>
<h2 id="音频生成指标"><a href="#音频生成指标" class="headerlink" title="音频生成指标"></a>音频生成指标</h2><p>看到个人写了点东西<br><a target="_blank" rel="noopener" href="https://blog.csdn.net/Blackoutdragon/article/details/132687747">https://blog.csdn.net/Blackoutdragon/article/details/132687747</a></p>
<h3 id="主观指标"><a href="#主观指标" class="headerlink" title="主观指标"></a>主观指标</h3><h4 id="Overall-quality-OVL"><a href="#Overall-quality-OVL" class="headerlink" title="Overall quality, OVL"></a>Overall quality, OVL</h4><p>评分者被要求在1到100之间的范围内对所提供样本的感知质量进行评分</p>
<h4 id="relevance-to-the-text-input"><a href="#relevance-to-the-text-input" class="headerlink" title="relevance to the text input"></a>relevance to the text input</h4><p>评分者被要求在1到100的范围内对音频和文本的匹配程度进行评分</p>
<h4 id="Fidelity"><a href="#Fidelity" class="headerlink" title="Fidelity"></a>Fidelity</h4><p>Diffsound中使用，保真度，也是衡量音频质量的指标，1-5分</p>
<h4 id="Intelligibility"><a href="#Intelligibility" class="headerlink" title="Intelligibility"></a>Intelligibility</h4><p>Diffsound中使用，可理解度，应该是度量和文本的照应程度的，是1-5分</p>
<h4 id="mean-opinion-score-MOS"><a href="#mean-opinion-score-MOS" class="headerlink" title="mean opinion score, MOS"></a>mean opinion score, MOS</h4><p>在make an audio中，将这个指标分成了两部分，生成的质量（是否自然, MOS-Q），是否与自然语言描述相符(MOS-F)<br>和上面的OVL和relevance相照应<br><img src="/2024/04/08/%E7%94%9F%E6%88%90%E7%9B%B8%E5%85%B3/image-20240508162318997.png" alt=""><br>给了一些阶段，然后找人评价</p>
<h3 id="客观指标"><a href="#客观指标" class="headerlink" title="客观指标"></a>客观指标</h3><h4 id="Frechet-Audio-Distance-FAD"><a href="#Frechet-Audio-Distance-FAD" class="headerlink" title="Frechet Audio Distance, FAD"></a>Frechet Audio Distance, FAD</h4><p>是谷歌提出的一个评估指标，最初是用于评估<code>music enhancement</code>算法<br>谷歌还专门有篇论文讲这个，看了下这篇论文<br>是因为之前的那些指标，在衡量音乐增强的工作时，基于信号的指标通常与人对增强音乐的主观评估不一致，为此提出了FAD：用于衡量给定音频片段与干净的录音室录制的音乐的比较<br><img src="/2024/04/08/%E7%94%9F%E6%88%90%E7%9B%B8%E5%85%B3/image-20240509102604162.png" alt=""><br>不使用单个音频片段来计算得分，而是将整个需要评估的数据集上生成的<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/111996722">VGGish</a>的嵌入的统计数据，与大型干净音乐数据集（比如训练集，这里其他也行，我看AudioLDM用的是PANNs）<br>评估集和背景干净数据集上得到的嵌入的高斯统计特征分别为$\mathcal N_e(\mu_e,\Sigma_e), \mathcal N_b(\mu_b,\Sigma_b)$<br>如下计算得到FAD：</p>
<script type="math/tex; mode=display">
\boldsymbol{F}(\mathcal{N_b,N_e})=\Vert \mu_b-\mu_e \Vert^2 + tr(\Sigma_b + \Sigma_e - 2\sqrt{\Sigma_b \Sigma_e})</script><h4 id="Frechet-Inception-Distance-FID"><a href="#Frechet-Inception-Distance-FID" class="headerlink" title="Frechet Inception Distance, FID"></a>Frechet Inception Distance, FID</h4><p>这个是图像上的，但是不知道为什么Diffsound要用这个评估，不同FAD<br>不过Diffsound用的是相似架构的InceptionV3模型，然后自己在AudioSet上训练了，用自己训练的模型计算FID</p>
<h4 id="sota-分类模型对生成样本和原始样本的KL散度"><a href="#sota-分类模型对生成样本和原始样本的KL散度" class="headerlink" title="sota 分类模型对生成样本和原始样本的KL散度"></a>sota 分类模型对生成样本和原始样本的KL散度</h4><p>问题：怎么选取这个原始样本和生成的样本<br>就说计算生成样本和真实样本经过分类模型的输出，然后输出的两个概率分布之间的KL散度</p>
<h4 id="SPICE，CIDEr"><a href="#SPICE，CIDEr" class="headerlink" title="SPICE，CIDEr"></a>SPICE，CIDEr</h4><p>这个是Audio Caption损失，仅在Diffsound中用过，计算的是生成的音频经过Caption之后，得到的描述和用于生成的描述之间的相似度</p>
<h4 id="Frechet-Distance-FD"><a href="#Frechet-Distance-FD" class="headerlink" title="Frechet Distance, FD"></a>Frechet Distance, FD</h4><p>就是衡量两条曲线的相似性，在音频中应该是算音频信号？还是那个经典的问题，怎么选取两条波形信号，一个是生成的，另一个呢<br>但是在audioldm_eval中，又涉及到了一个分类模型<br>后面两个在audioldm_eval中都有<br>看了下代码，就是FAD的计算过程啊，就是用的是PANNs而不是FAD</p>
<h4 id="Inception-Score-IS"><a href="#Inception-Score-IS" class="headerlink" title="Inception Score, IS"></a>Inception Score, IS</h4><p>这也是从图像上搞过来的<br>从两个方面评价GAN生成的图片的质量：</p>
<ul>
<li>清晰度：对于清晰的图片，属于分类器某一类的概率应该非常大，$p(y|x)$的熵应该很小</li>
<li>多样性：从所有的图片的角度考虑，在生成的一堆图片中，如果这些图片是多样性的，那么每个类别的数目应该是差不多的，也就是说$p(y)$的熵应该很大<br>所以这两个分布的距离要大，用KL散度衡量<script type="math/tex; mode=display">
\boldsymbol{IS}(G)=\exp \left( \mathbb{E}_{x\sim p_g} D_{KL}(p(y|x)\Vert p(y)) \right)</script>在audioldm_eval中，使用的是多个样本，然后取一个样本窗口，在窗口内计算输出概率的平均值，和没求平均的概率算KL散度。<h4 id="CLAP-score"><a href="#CLAP-score" class="headerlink" title="CLAP score"></a>CLAP score</h4>算的是CLAP特征的余弦相似度</li>
</ul>
<h1 id="控制图像生成的工作"><a href="#控制图像生成的工作" class="headerlink" title="控制图像生成的工作"></a>控制图像生成的工作</h1><p>牛魔，一搜一堆<br>这几个先看看<br><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/626346656">https://zhuanlan.zhihu.com/p/626346656</a><br><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/687862680">https://zhuanlan.zhihu.com/p/687862680</a><br><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/613909875">https://zhuanlan.zhihu.com/p/613909875</a><br><a target="_blank" rel="noopener" href="https://cloud.tencent.com/developer/article/2379807">https://cloud.tencent.com/developer/article/2379807</a></p>
<h2 id="Adding-Conditional-Control-to-Text-to-Image-Diffusion-Models"><a href="#Adding-Conditional-Control-to-Text-to-Image-Diffusion-Models" class="headerlink" title="Adding Conditional Control to Text-to-Image Diffusion Models"></a>Adding Conditional Control to Text-to-Image Diffusion Models</h2><p>Lvmin Zhang, Anyi Rao<br>Stanford University</p>
<h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>仅仅通过文本提示来精确表达复杂的布局、姿势、形状和形式是很困难的。<br>直接对大型预训练模型训练或微调会导致过拟合和遗忘<br>ControlNet通过锁定参数保持大模型的质量和功能，并将encoding layer复制一份可训练部分<br>可训练部分和原始锁定的模型使用 <code>zero convolution layers</code>连接，就是0初始化的卷积层<br>这样做的目的是保证在训练开始时，大扩散模型的深层特征中不会加入有害噪声，并保证了可训练副本中主干网络不被这些噪声破坏<br>实验表明，可以用多种控制条件控制Stable Diffusion</p>
<h3 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h3><p>其他微调神经网络的方法：HyperNetwork，Adapter，Additive Learning, LoRA<br>控制图像扩散模型的方法：<br>文本引导控制方法侧重于调整提示、操纵CLIP特征和修改交叉注意力<br>巴拉巴拉讲一堆，看不懂的专业词汇，留在这之后再一篇一篇具体看</p>
<h3 id="Method-3"><a href="#Method-3" class="headerlink" title="Method"></a>Method</h3><h4 id="ControlNet"><a href="#ControlNet" class="headerlink" title="ControlNet"></a>ControlNet</h4><p><img src="/2024/04/08/%E7%94%9F%E6%88%90%E7%9B%B8%E5%85%B3/image-20240514191522795.png" alt=""><br>ControlNet将额外的条件信息注入神经网络块中，如上图<br>x,y 是两个特征图，ControlNet通过拷贝一个副本，在输入输出加上一个1x1的0初始化卷积层，将条件和原输入加入副本，输出加入原输出</p>
<h4 id="ControlNet-for-Text-to-Image-Diffusion"><a href="#ControlNet-for-Text-to-Image-Diffusion" class="headerlink" title="ControlNet for Text-to-Image Diffusion"></a>ControlNet for Text-to-Image Diffusion</h4><p><img src="/2024/04/08/%E7%94%9F%E6%88%90%E7%9B%B8%E5%85%B3/image-20240514194010649.png" alt=""><br>上图为Stable Diffusion的Unet结构，和上面对每个小块的处理类似，在Unet的Encoder部分，只在最前面用了一个零卷积，然后在之后decoder部分，每一小块都有一个0卷积层（因为Diffusion的decoder每一块还要和encoder的每一块做cross-attention）<br>SD是在latent space做的扩散，所以最先的Condition $C_f$ 也要映射到相同形状，作者这里用了4个卷积层加ReLU。</p>
<h4 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h4><p>优化目标和普通的Diffusion一样，也是预测每一步的噪声<br>在训练过程中，随机将50%的文本描述换成空字符串，这个方法会提高ControlNet直接识别输入语义条件的能力。<br>而且在实验中，模型是突然学会依照condition去生成图片的</p>
<h4 id="Inference"><a href="#Inference" class="headerlink" title="Inference"></a>Inference</h4><p><strong>Classifier free guidance</strong><br>这里对CFG的系数做了一些工作，按照Unet的块的大小，分别计算 $w_i = 64/h_i$<br><strong>Composing multiple ControlNets</strong><br>使用多个ControlNets conditions时，比如姿势和线稿，我们可以直接将对应condition相加输入到SD，不需要额外的工作</p>
<h3 id="Experiments-2"><a href="#Experiments-2" class="headerlink" title="Experiments"></a>Experiments</h3><p>消融实验证明了0卷积层和可训练副本的有效性<br>人类感知实验证明了ControlNet生成的条件依赖性和质量，并且和SDv2-D2I的生成结果几乎难以分辨</p>
<p>不会改变Stable Diffusion的网络结构，所以可以直接应用于SD社区中的其他模型</p>
<h1 id="控制音频生成的工作"><a href="#控制音频生成的工作" class="headerlink" title="控制音频生成的工作"></a>控制音频生成的工作</h1><p>也是有的，不过没有图像那么多</p>
<h2 id="Simple-and-Controllable-Music-Generation"><a href="#Simple-and-Controllable-Music-Generation" class="headerlink" title="Simple and Controllable Music Generation"></a>Simple and Controllable Music Generation</h2><p>发表情况：NeurIPS 2023<br>作者：Jade Copet, Felix Kreuk<br>机构：Meta AI<br>这还是我看的第一篇关于音乐生成的文章，所以仔细看看<br>音乐生成的几个问题：长序列，需要全频段，采样率要求也更高<br>要控制旋律和和声，使结构很复杂<br>还需要用不同的方法控制生成过程，比如音阶，乐器，旋律，流派<br>为了使音频建模更容易处理，最近的研究提出将音频信号表示为表示相同信号的多个离散token stream？不是很懂<br>大概就是，要对一条音频流进行建模，采用离散token的方法，但是有多个token序列，比如 Kharitonov对语音流的做法是每个流有不同的偏移，Agostinelli的做法是使用不同粒度的离散token表示音乐片段，使用自回归模型的层次结构对他们进行建模。</p>
<p>本工作提出了一个简单可控的音乐生成模型 MUSICGEN，能够给定文本描述，生成高质量的音乐<br>看方法，带着这几个问题去看</p>
<ul>
<li>音乐生成的大框架是怎样的</li>
<li>用什么方法来控制生成的音乐和文本旋律之间的关系<h3 id="Method-4"><a href="#Method-4" class="headerlink" title="Method"></a>Method</h3>这前面的部分完全看不懂在讲什么<h4 id="Audio-tokenization"><a href="#Audio-tokenization" class="headerlink" title="Audio tokenization"></a>Audio tokenization</h4><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/627515982">RVQ相关</a><br>使用的是 EnCodec 是个别人的工作<br>给定一个音频随机变量 $X\in \mathbb{R}^{d\cdot f_s}$ ，EnCodec将其编码成一个连续的tensor，帧率 $f_r \ll f_s$<br>然后被RVQ量化为 $Q\in \{1,\dots,M\}^{d\cdot f_r \times K}$ ，其中$K$是码本数，M是码本大小<br>然后他这个码本，是按照RVQ层数算的，有几层就有几个码本<br>因为在RVQ中，每个量化器都是对前一个量化器留下的量化误差进行编码，所以每个码本也不是独立的，第一个码本是最重要的</li>
</ul>
<h4 id="Codebook-interleaving-patterns"><a href="#Codebook-interleaving-patterns" class="headerlink" title="Codebook interleaving patterns"></a>Codebook interleaving patterns</h4><p><img src="/2024/04/08/%E7%94%9F%E6%88%90%E7%9B%B8%E5%85%B3/image-20240510144419328.png" alt=""><br><strong>Exact flattened autoregressive decomposition</strong><br>这一部分是说，对于tokenizer提取的几个码本的序列，因为有四个量化器，所以对于一条音频，就有四个token序列。在使用自回归建模这些序列的时候，有两种方式，一是把这些序列展平，这样会让序列变的很长，二是把这些序列叠起来，就是本文的做法<br><strong>Inexact autoregressive decomposition</strong><br>这两对数学公式有什么区别，我能问问嘛？<br>把这个序列叠起来预测的问题是，随着t的增加，两个分布会越离越远，但是可以加快训练和推理，特别是对于长序列<br>然后将任意码本交错模式，就是图里的</p>
<h4 id="Model-conditioning"><a href="#Model-conditioning" class="headerlink" title="Model conditioning"></a>Model conditioning</h4><p>分为两个部分，一个是文本条件，另一个是旋律条件，也没有细说，沟槽的<br>文本条件还是那几种做法，T5，FLAN-T5，CLAP<br>旋律条件：用的是色谱图（chromagram），只随便看了看，<a target="_blank" rel="noopener" href="https://www.zhihu.com/question/23631477">链接</a></p>
<p>沟槽的论文，怎么连个系统结构都不给。<br>对双声道的处理如下<br><img src="/2024/04/08/%E7%94%9F%E6%88%90%E7%9B%B8%E5%85%B3/image-20240510161643681.png" alt=""><br>就是在token的堆叠上加入了两个声道<br>吗的好气，一笔带过了没讲清楚<br>但是这篇论文和他们的AudioGEN是同一时期的东西，而且二者的结构都是类似的</p>
<h2 id="Music-ControlNet"><a href="#Music-ControlNet" class="headerlink" title="Music ControlNet"></a>Music ControlNet</h2><p>TASLP<br>2024.5.22<br>Shih-Lun Wu, Chris Donahue, Shinji watanabe<br>CMU<br>真的是快人一步<br><img src="/2024/04/08/%E7%94%9F%E6%88%90%E7%9B%B8%E5%85%B3/image-20240527154322264.png" alt=""></p>
<p>一篇在text to music中，使用controlnet加入时序，音符控制的工作<br>由于训练数据的原因，文本基本只能控制生成音乐的风格，情绪这种全局的信息<br>贡献：</p>
<ul>
<li>一个能够使用组合精准时变条件控制音频生成的框架</li>
<li>可以在推理时加入灵活的时变控制条件</li>
<li>可以控制旋律动力和节奏<h3 id="问题规约"><a href="#问题规约" class="headerlink" title="问题规约"></a>问题规约</h3>这里提到和图像上像素级别的控制相比，在频谱图上的控制不同。图像上前两维都是空间信息，而音频上一个是时间一个是频率<br>对于创作者来说，一个时变控制与时间的相关性会比频率更大。<br>所以将控制信号放宽定义为：<script type="math/tex; mode=display">
\begin{aligned}
C:=\left\{ c^{(n)} \in \mathbb{R}^{Tf_k\times B_n \times D_n} \right\}^N_{n=1}
\end{aligned}</script>$B_n$ 是特定于每种控制信号的类别数，D是通道数（单声道双声道）<br>按照这种规约，每一个 $c^{(n)}_t\in \mathbb{R}^{1\times 1}$ 都是一个帧级约束<h3 id="Method-5"><a href="#Method-5" class="headerlink" title="Method"></a>Method</h3>把Music ControlNet问题规约为<script type="math/tex; mode=display">
\tilde f^{(l)}\left( x^{(m,l-1)},m,c_{text},C \right):=Z_{out}\left( f^{(l)}(x^{(m,l-1)}+Z_{in}(M^{(n)}(c^{(n)})),m,c_{text})\right)</script>其中 $Z_{in},Z_{out}$ 是ControlNet的零卷积层，$M^{(n)}$ 是一个额外添加的单层MLP，将n种控制信号$B_n$映射到和梅尔bins$B$ 的形状相同。<br>对于多种控制信号，每种控制信号有一个单独的MLP<h4 id="Masking-Strategy-to-Enable-Partially-Specified-Controls"><a href="#Masking-Strategy-to-Enable-Partially-Specified-Controls" class="headerlink" title="Masking Strategy to Enable Partially-Specified Controls"></a>Masking Strategy to Enable Partially-Specified Controls</h4>为了使创作者可以自由使用控制类型的任何组合，使用CFG-like训练策略。令控制信号的索引集合为$I=\{1,\dots,N\}$，在每个训练step，选择一个子集$I’\in I$，其中的训练类型会被设为0或丢弃<br>$$c^{(n)}:=<br>\left\{  <pre><code>       \begin{array}{**lr**}  
      0_{Tf_k\times B_n \times D_n}&amp;\forall n\in I' \newline
      c^{(n)} &amp;\forall n \in I\setminus I'
       \end{array}  
</code></pre>\right.  </li>
</ul>
<script type="math/tex; mode=display">
这种训练策略可以让模型学习任何控制子集之间的相关性。
这个地方就和武洋说的多个事件组合的问题，如何让模型学会不同事件组合之间的相关性。
然后为了使这个control能在时域上部分指定，除了上面的全mask，还加了一个部分mask
![](生成相关/image-20240528102726846.png)
就是在启用的控制条件中，随机mask一个时间段。

#### Musical Control Signals
三种控制信号：melody, dynamics, rhythm
两种得到控制信号的方法：extracted controls, created controls
Melody：$c_{mel}\in \mathbb{R}^{Tf_k \times 12\times 1}$ 
对线性谱的每一帧计算chromagram，做argmax操作，将最突出的音阶作为该帧的的one-hot特征
还会过滤低音部分，以使生成的音阶为旋律音阶而不是低音部分的音阶

动态是音乐中声音强度随时间变化的表征
节奏曲线

#### Evaluation Metrics
Melody accuracy
Dynamics correlation
Rhythm F1
CLAP score
FAD

## FoleyCrafter: Bring Silent Videos to Life with Lifelike and Synchronized Sounds
用于自动生成与视频同步的高质量音效，包括两个关键组件：用于语义对齐的语义适配器（semantic adapter）和用于精确音视频同步的时间控制器（temporal controller）。

可以与预先训练T2A模型集成，并通过音频监督进行优化

时间控制器包括一个开始检测器（onset detector）和一个基于时间戳的适配器（timestamp-based adapter）
这个适配器的作用是，根据开始检测器预测的时间戳和音频生成对齐
重点关注这个适配器

控制LDM的生成有两种方式，一个是多层感知机MLP，扩散过程的时间戳和text embedding拼接起来作为条件信息，然后和Unet的特征图通过MLP层融合
另一个是交叉注意力机制，作用于Unet的每一块。

本文采用交叉注意力价值整合text和vision条件。
还提到了Aufusion那个工作，已经AudioLDM2，都是在Unet中间块中加入了Attension

### Approach
#### FoleyCrafter
![](生成相关/image-20240703145720202.png)
接下来详细讲每个模块
S.A.， Semantic Adapter
![](生成相关/image-20240703150424814.png)
使用预训练的visual encoder，直接提取视频特征，然后这个线性层和归一化层，算是特征压缩吧，搞成和原来的text embedding相同的形状，然后算cross attention加入unet每个模块
关于[cross attention](https://blog.csdn.net/MengYa_Dream/article/details/126688503)
无语，这哪是cross attention
下文说，是两个不同的attention，然后用一个权重参数λ组合起来
训练的优化目标还是diffusion的噪声损失
为了有效的捕获视觉线索并将它们与文本到音频生成器的条件空间对齐，以90%的概率随机丢弃文本条件

Temporal Controller
![](生成相关/image-20240703154303748.png)
这不ControlNet吗
Timestamp Detector将视频帧作为输入，预测二元时间掩码，指示目标音频中的声音效果是否存在
Temporal Adapter，将时域信息编码，注入到Unet的decoder
和我做的事情一模一样

问题，怎么把这个时间戳和对应的事件语义联系起来呢，我能问问吗
回到semantic adapter部分
这里的visual encoder干的事情</script><p>V_{emb} = MLP(AvgPooling(\tau_{vis}(v)))</p>
<p>$$<br>其中 $\tau_{vis}$ 代表CLIP image encoder，$AvgPooling$ 指对提取的CLIP特征的<strong>跨帧特征</strong>的平均池化<br>就是说，这里得到的visual embedding，已经是包含事件发生顺序的了。</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="http://yypyyds.github.io">脚踏车没有脚</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://yypyyds.github.io/2024/04/08/%E7%94%9F%E6%88%90%E7%9B%B8%E5%85%B3/">http://yypyyds.github.io/2024/04/08/%E7%94%9F%E6%88%90%E7%9B%B8%E5%85%B3/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://yypyyds.github.io" target="_blank">脚踏车的日志站</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E8%AE%BA%E6%96%87/">论文</a><a class="post-meta__tags" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></div><div class="post_share"><div class="social-share" data-image="/img/touxiang.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2024/04/19/Linux%E7%9B%B8%E5%85%B3%E5%91%BD%E4%BB%A4/" title="Linux相关命令"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">Linux相关命令</div></div></a></div><div class="next-post pull-right"><a href="/2024/04/03/VITS/" title="VITS"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">VITS</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2023/12/19/PoDA/" title="PoDA: Prompt-driven Zero-shot Domain Adaptation"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-12-19</div><div class="title">PoDA: Prompt-driven Zero-shot Domain Adaptation</div></div></a></div><div><a href="/2023/11/13/AudioLDM/" title="AudioLDM"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-11-13</div><div class="title">AudioLDM</div></div></a></div><div><a href="/2023/11/20/CLAP/" title="CLAP"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-11-20</div><div class="title">CLAP</div></div></a></div><div><a href="/2023/11/28/AudioLDM2/" title="AudioLDM2"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-11-28</div><div class="title">AudioLDM2</div></div></a></div><div><a href="/2023/12/06/Diffusion-TTA/" title="Diffusion-TTA"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-12-06</div><div class="title">Diffusion-TTA</div></div></a></div><div><a href="/2024/01/18/Few-shot%20SED%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/" title="Few-shot SED相关论文"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-01-18</div><div class="title">Few-shot SED相关论文</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/touxiang.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">脚踏车没有脚</div><div class="author-info__description">不积跬步，无以至千里</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">67</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">61</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">9</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/yypyyds"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">这是一个努力学习的笨蛋的博客</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%94%9F%E6%88%90%E7%9B%B8%E5%85%B3"><span class="toc-text">生成相关</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Audiogen"><span class="toc-text">Audiogen</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Method"><span class="toc-text">Method</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Audio-representation"><span class="toc-text">Audio representation</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Experiments"><span class="toc-text">Experiments</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Limitations"><span class="toc-text">Limitations</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Diffsound"><span class="toc-text">Diffsound</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Proposed-text-to-sound-framework"><span class="toc-text">Proposed text-to-sound framework</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Diffusion-based-decoder"><span class="toc-text">Diffusion-based decoder</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%9E%E9%AA%8C"><span class="toc-text">实验</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Make-an-Audio"><span class="toc-text">Make an Audio</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Pseudo-Prompt-Enhancement-Distill-then-Reprogram"><span class="toc-text">Pseudo Prompt Enhancement: Distill-then-Reprogram</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Textual-Representation"><span class="toc-text">Textual Representation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Audio-Representation"><span class="toc-text">Audio Representation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#X-To-Audio-No-Modality-Left-Behind"><span class="toc-text">X-To-Audio: No Modality Left Behind</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Personalized-Text-To-Audio-Generation"><span class="toc-text">Personalized Text-To-Audio Generation</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Audio-Inpainting"><span class="toc-text">Audio Inpainting</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Visual-To-Audio-Generation"><span class="toc-text">Visual-To-Audio Generation</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Training-and-Evaluation"><span class="toc-text">Training and Evaluation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%8EAudioLDM2%E7%9A%84%E5%AF%B9%E6%AF%94"><span class="toc-text">与AudioLDM2的对比</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A4%8D%E7%8E%B0"><span class="toc-text">复现</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Retrieval-Augmented-text-to-audio-generation"><span class="toc-text">Retrieval-Augmented text to audio generation</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Method-1"><span class="toc-text">Method</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Experiments-1"><span class="toc-text">Experiments</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#VoiceBox"><span class="toc-text">VoiceBox</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#TANGO"><span class="toc-text">TANGO</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Method-2"><span class="toc-text">Method</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Experiment"><span class="toc-text">Experiment</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%AD%E9%80%94%E5%B0%8F%E8%AE%B0"><span class="toc-text">中途小记</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%9F%B3%E9%A2%91%E7%94%9F%E6%88%90%E6%8C%87%E6%A0%87"><span class="toc-text">音频生成指标</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%BB%E8%A7%82%E6%8C%87%E6%A0%87"><span class="toc-text">主观指标</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Overall-quality-OVL"><span class="toc-text">Overall quality, OVL</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#relevance-to-the-text-input"><span class="toc-text">relevance to the text input</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Fidelity"><span class="toc-text">Fidelity</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Intelligibility"><span class="toc-text">Intelligibility</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#mean-opinion-score-MOS"><span class="toc-text">mean opinion score, MOS</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%A2%E8%A7%82%E6%8C%87%E6%A0%87"><span class="toc-text">客观指标</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Frechet-Audio-Distance-FAD"><span class="toc-text">Frechet Audio Distance, FAD</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Frechet-Inception-Distance-FID"><span class="toc-text">Frechet Inception Distance, FID</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#sota-%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B%E5%AF%B9%E7%94%9F%E6%88%90%E6%A0%B7%E6%9C%AC%E5%92%8C%E5%8E%9F%E5%A7%8B%E6%A0%B7%E6%9C%AC%E7%9A%84KL%E6%95%A3%E5%BA%A6"><span class="toc-text">sota 分类模型对生成样本和原始样本的KL散度</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#SPICE%EF%BC%8CCIDEr"><span class="toc-text">SPICE，CIDEr</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Frechet-Distance-FD"><span class="toc-text">Frechet Distance, FD</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Inception-Score-IS"><span class="toc-text">Inception Score, IS</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#CLAP-score"><span class="toc-text">CLAP score</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%8E%A7%E5%88%B6%E5%9B%BE%E5%83%8F%E7%94%9F%E6%88%90%E7%9A%84%E5%B7%A5%E4%BD%9C"><span class="toc-text">控制图像生成的工作</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Adding-Conditional-Control-to-Text-to-Image-Diffusion-Models"><span class="toc-text">Adding Conditional Control to Text-to-Image Diffusion Models</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Introduction"><span class="toc-text">Introduction</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Related-Work"><span class="toc-text">Related Work</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Method-3"><span class="toc-text">Method</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#ControlNet"><span class="toc-text">ControlNet</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#ControlNet-for-Text-to-Image-Diffusion"><span class="toc-text">ControlNet for Text-to-Image Diffusion</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Training"><span class="toc-text">Training</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Inference"><span class="toc-text">Inference</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Experiments-2"><span class="toc-text">Experiments</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%8E%A7%E5%88%B6%E9%9F%B3%E9%A2%91%E7%94%9F%E6%88%90%E7%9A%84%E5%B7%A5%E4%BD%9C"><span class="toc-text">控制音频生成的工作</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Simple-and-Controllable-Music-Generation"><span class="toc-text">Simple and Controllable Music Generation</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Method-4"><span class="toc-text">Method</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Audio-tokenization"><span class="toc-text">Audio tokenization</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Codebook-interleaving-patterns"><span class="toc-text">Codebook interleaving patterns</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Model-conditioning"><span class="toc-text">Model conditioning</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Music-ControlNet"><span class="toc-text">Music ControlNet</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%97%AE%E9%A2%98%E8%A7%84%E7%BA%A6"><span class="toc-text">问题规约</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Method-5"><span class="toc-text">Method</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Masking-Strategy-to-Enable-Partially-Specified-Controls"><span class="toc-text">Masking Strategy to Enable Partially-Specified Controls</span></a></li></ol></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/07/19/%E6%89%BE%E7%8F%AD%E4%B8%8A/" title="无题">无题</a><time datetime="2024-07-19T07:37:01.740Z" title="发表于 2024-07-19 15:37:01">2024-07-19</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/07/18/%E8%AF%AD%E9%9F%B3%E4%BF%A1%E5%8F%B7%E5%A4%84%E7%90%86%E7%AC%94%E8%AE%B0/" title="语音信号处理笔记">语音信号处理笔记</a><time datetime="2024-07-18T11:14:40.000Z" title="发表于 2024-07-18 19:14:40">2024-07-18</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/07/08/%E9%9F%B3%E4%B9%90%E7%94%9F%E6%88%90%E9%9A%90%E5%86%99/" title="音乐生成隐写">音乐生成隐写</a><time datetime="2024-07-08T08:20:58.000Z" title="发表于 2024-07-08 16:20:58">2024-07-08</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/06/11/Talking%20face%20detection%E7%9B%B8%E5%85%B3/" title="Talking face detection相关">Talking face detection相关</a><time datetime="2024-06-11T03:08:05.000Z" title="发表于 2024-06-11 11:08:05">2024-06-11</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/05/22/%E9%9F%B3%E9%A2%91%E6%95%B0%E6%8D%AE%E9%9B%86/" title="音频数据集">音频数据集</a><time datetime="2024-05-22T03:51:40.000Z" title="发表于 2024-05-22 11:51:40">2024-05-22</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2024 By 脚踏车没有脚</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div></div></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.min.js"></script><script src="/js/search/local-search.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        insertScript: [200, () => {
          document.querySelectorAll('mjx-container').forEach(node => {
            if (node.hasAttribute('display')) {
              btf.wrap(node, 'div', { class: 'mathjax-overflow' })
            } else {
              btf.wrap(node, 'span', { class: 'mathjax-overflow' })
            }
          });
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typeset()
}</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>